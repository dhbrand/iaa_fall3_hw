{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Dummy Vars for Cat Vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from auto_ml import Predictor\n",
    "from auto_ml.utils import get_boston_dataset\n",
    "from auto_ml.utils_models import load_ml_model\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/MLProjectData.csv\", header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need all the variables to be numeric for neural networks\n",
    "df2 =pd.get_dummies(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training and testing data\n",
    "df2_train, df2_test = train_test_split(df2, test_size=0.2, random_state=303)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_descriptions = {'target': 'output'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_predictor = Predictor(type_of_estimator='regressor', column_descriptions=column_descriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building with default regressor (GradientBoostingRegressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to auto_ml! We're about to go through and make sense of your data using machine learning, and give you a production-ready pipeline to get predictions with.\n",
      "\n",
      "If you have any issues, or new feature ideas, let us know at http://auto.ml\n",
      "You are running on version 2.9.9\n",
      "Now using the model training_params that you passed in:\n",
      "{}\n",
      "After overwriting our defaults with your values, here are the final params that will be used to initialize the model:\n",
      "{'learning_rate': 0.1, 'presort': False, 'warm_start': True}\n",
      "Running basic data cleaning\n",
      "Fitting DataFrameVectorizer\n",
      "Now using the model training_params that you passed in:\n",
      "{}\n",
      "After overwriting our defaults with your values, here are the final params that will be used to initialize the model:\n",
      "{'learning_rate': 0.1, 'presort': False, 'warm_start': True}\n",
      "\n",
      "\n",
      "********************************************************************************************\n",
      "About to fit the pipeline for the model GradientBoostingRegressor to predict target\n",
      "Started at:\n",
      "2018-11-14 14:50:48\n",
      "[1] random_holdout_set_from_training_data's score is: -1.292\n",
      "[2] random_holdout_set_from_training_data's score is: -1.293\n",
      "[3] random_holdout_set_from_training_data's score is: -1.298\n",
      "[4] random_holdout_set_from_training_data's score is: -1.301\n",
      "[5] random_holdout_set_from_training_data's score is: -1.304\n",
      "[6] random_holdout_set_from_training_data's score is: -1.307\n",
      "[7] random_holdout_set_from_training_data's score is: -1.307\n",
      "[8] random_holdout_set_from_training_data's score is: -1.309\n",
      "[9] random_holdout_set_from_training_data's score is: -1.309\n",
      "[10] random_holdout_set_from_training_data's score is: -1.307\n",
      "[11] random_holdout_set_from_training_data's score is: -1.311\n",
      "[12] random_holdout_set_from_training_data's score is: -1.315\n",
      "[13] random_holdout_set_from_training_data's score is: -1.313\n",
      "[14] random_holdout_set_from_training_data's score is: -1.314\n",
      "[15] random_holdout_set_from_training_data's score is: -1.312\n",
      "[16] random_holdout_set_from_training_data's score is: -1.314\n",
      "[17] random_holdout_set_from_training_data's score is: -1.314\n",
      "[18] random_holdout_set_from_training_data's score is: -1.317\n",
      "[19] random_holdout_set_from_training_data's score is: -1.316\n",
      "[20] random_holdout_set_from_training_data's score is: -1.316\n",
      "[21] random_holdout_set_from_training_data's score is: -1.315\n",
      "The number of estimators that were the best for this training dataset: 1\n",
      "The best score on the holdout set: -1.2922405512706898\n",
      "Finished training the pipeline!\n",
      "Total training time:\n",
      "0:00:02\n",
      "\n",
      "\n",
      "Here are the results from our GradientBoostingRegressor\n",
      "predicting target\n",
      "Calculating feature responses, for advanced analytics.\n",
      "The printed list will only contain at most the top 100 features.\n",
      "+----+----------------+--------------+----------+-------------------+-------------------+-----------+-----------+-----------+-----------+\n",
      "|    | Feature Name   |   Importance |    Delta |   FR_Decrementing |   FR_Incrementing |   FRD_abs |   FRI_abs |   FRD_MAD |   FRI_MAD |\n",
      "|----+----------------+--------------+----------+-------------------+-------------------+-----------+-----------+-----------+-----------|\n",
      "|  0 | num1           |       0.0000 | 499.5119 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 72 | cat1_D         |       0.0000 |   0.2003 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 71 | cat1_C         |       0.0000 |   0.2012 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 70 | cat1_B         |       0.0000 |   0.2030 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 69 | cat1_A         |       0.0000 |   0.1953 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 68 | cat19          |       0.0000 |   0.0899 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 67 | cat18          |       0.0000 |   0.0531 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 66 | cat17          |       0.0000 |   0.0775 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 65 | cat16          |       0.0000 |   0.0336 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 64 | cat15          |       0.0000 |   0.0396 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 63 | cat14          |       0.0000 |   0.0857 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 62 | cat13          |       0.0000 |   0.0991 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 61 | cat12          |       0.0000 |   0.1020 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 60 | cat11          |       0.0000 |   0.0876 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 59 | cat10          |       0.0000 |   0.0442 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 58 | num59          |       0.0000 | 919.8211 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 56 | num57          |       0.0000 |  76.7571 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 55 | num56          |       0.0000 |   2.2500 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 54 | num55          |       0.0000 |  74.4318 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 53 | num54          |       0.0000 |  76.7681 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 52 | num53          |       0.0000 |   2.0393 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 51 | num52          |       0.0000 |   2.2513 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 73 | cat1_E         |       0.0000 |   0.2001 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 50 | num51          |       0.0000 |   4.2226 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 74 | cat20          |       0.0000 |   0.1048 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 76 | cat22          |       0.0000 |   0.0852 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 97 | cat7           |       0.0000 |   0.0781 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 96 | cat6           |       0.0000 |   0.0860 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 95 | cat5           |       0.0000 |   0.0951 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 94 | cat4           |       0.0000 |   0.1033 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 93 | cat3           |       0.0000 |   0.0902 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 92 | cat2_L         |       0.0000 |   0.1401 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 91 | cat2_K         |       0.0000 |   0.1347 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 90 | cat2_J         |       0.0000 |   0.1398 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 89 | cat2_I         |       0.0000 |   0.1368 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 88 | cat2_H         |       0.0000 |   0.1418 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 87 | cat2_G         |       0.0000 |   0.1359 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 86 | cat2_F         |       0.0000 |   0.1403 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 85 | cat2_E         |       0.0000 |   0.1412 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 84 | cat2_D         |       0.0000 |   0.1371 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 83 | cat2_C         |       0.0000 |   0.1424 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 82 | cat2_B         |       0.0000 |   0.1340 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 81 | cat2_A         |       0.0000 |   0.1337 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 80 | cat26          |       0.0000 |   0.0508 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 79 | cat25          |       0.0000 |   0.0753 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 78 | cat24          |       0.0000 |   0.0328 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 77 | cat23          |       0.0000 |   0.0414 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 75 | cat21          |       0.0000 |   0.0977 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 98 | cat8           |       0.0000 |   0.0553 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 49 | num50          |       0.0000 |  76.7571 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 47 | num48          |       0.0000 |  74.4318 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 22 | num23          |       0.0000 |   2.2769 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 21 | num22          |       0.0000 |   0.7174 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 20 | num21          |       0.0000 |   0.7091 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 19 | num20          |       0.0000 |   0.7161 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 18 | num19          |       0.0000 |   0.7109 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 17 | num18          |       0.0000 |   0.7165 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 16 | num17          |       0.0000 |   0.7834 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 15 | num16          |       0.0000 |   0.7858 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 14 | num15          |       0.0000 |   0.7977 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 13 | num14          |       0.0000 |   0.0569 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 12 | num13          |       0.0000 |   0.0475 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 11 | num12          |       0.0000 |   0.0345 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 10 | num11          |       0.0000 |   4.0621 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "|  9 | num10          |       0.0000 |   4.0758 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "|  8 | num9           |       0.0000 |   4.0730 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "|  7 | num8           |       0.0000 |   4.0887 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "|  6 | num7           |       0.0000 |   0.0182 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "|  5 | num6           |       0.0000 |   0.0135 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "|  3 | num4           |       0.0000 |   0.0058 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "|  2 | num3           |       0.0000 |   0.6974 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "|  1 | num2           |       0.0000 |   4.0834 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 23 | num24          |       0.0000 |   4.1844 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 48 | num49          |       0.0000 |   2.2500 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 24 | num25          |       0.0000 |   7.9282 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 38 | num39          |       0.0000 |   5.5331 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 46 | num47          |       0.0000 |  76.7681 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 45 | num46          |       0.0000 |   2.0393 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 44 | num45          |       0.0000 |   2.2513 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 43 | num44          |       0.0000 |   4.2226 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 41 | num42          |       0.0000 |  18.3723 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 40 | num41          |       0.0000 |   0.5467 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 39 | num40          |       0.0000 |   6.6850 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 25 | num26          |       0.0000 |   8.6364 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 37 | num38          |       0.0000 |  17.9140 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 99 | cat9           |       0.0000 |   0.0336 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 35 | num36          |       0.0000 |   0.5135 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 33 | num34          |       0.0000 |  26.5347 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 32 | num33          |       0.0000 |  26.6477 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 30 | num31          |       0.0000 |   0.5502 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 29 | num30          |       0.0000 |  19.5508 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 28 | num29          |       0.0000 |   6.7246 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 36 | num37          |       0.0000 |  18.3793 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 42 | num43          |       0.0000 |   0.5782 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "|  4 | num5           |       0.0080 |   0.0079 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 26 | num27          |       0.0510 |   5.5294 |            0.0001 |           -0.0001 |    0.0001 |    0.0001 |    0.0000 |    0.0000 |\n",
      "| 31 | num32          |       0.1392 |   0.2455 |            0.0002 |           -0.0003 |    0.0002 |    0.0003 |    0.0000 |    0.0000 |\n",
      "| 27 | num28          |       0.1406 |   5.5592 |            0.0007 |           -0.0003 |    0.0007 |    0.0003 |    0.0000 |    0.0000 |\n",
      "| 34 | num35          |       0.3221 |   0.5467 |            0.0007 |           -0.0008 |    0.0007 |    0.0008 |    0.0000 |    0.0000 |\n",
      "| 57 | num58          |       0.3392 |   0.7091 |            0.0003 |           -0.0002 |    0.0003 |    0.0002 |    0.0000 |    0.0000 |\n",
      "+----+----------------+--------------+----------+-------------------+-------------------+-----------+-----------+-----------+-----------+\n",
      "\n",
      "\n",
      "*******\n",
      "Legend:\n",
      "Importance = Feature Importance\n",
      "     Explanation: A weighted measure of how much of the variance the model is able to explain is due to this column\n",
      "FR_delta = Feature Response Delta Amount\n",
      "     Explanation: Amount this column was incremented or decremented by to calculate the feature reponses\n",
      "FR_Decrementing = Feature Response From Decrementing Values In This Column By One FR_delta\n",
      "     Explanation: Represents how much the predicted output values respond to subtracting one FR_delta amount from every value in this column\n",
      "FR_Incrementing = Feature Response From Incrementing Values In This Column By One FR_delta\n",
      "     Explanation: Represents how much the predicted output values respond to adding one FR_delta amount to every value in this column\n",
      "FRD_MAD = Feature Response From Decrementing- Median Absolute Delta\n",
      "     Explanation: Takes the absolute value of all changes in predictions, then takes the median of those. Useful for seeing if decrementing this feature provokes strong changes that are both positive and negative\n",
      "FRI_MAD = Feature Response From Incrementing- Median Absolute Delta\n",
      "     Explanation: Takes the absolute value of all changes in predictions, then takes the median of those. Useful for seeing if incrementing this feature provokes strong changes that are both positive and negative\n",
      "FRD_abs = Feature Response From Decrementing Avg Absolute Change\n",
      "     Explanation: What is the average absolute change in predicted output values to subtracting one FR_delta amount to every value in this column. Useful for seeing if output is sensitive to a feature, but not in a uniformly positive or negative way\n",
      "FRI_abs = Feature Response From Incrementing Avg Absolute Change\n",
      "     Explanation: What is the average absolute change in predicted output values to adding one FR_delta amount to every value in this column. Useful for seeing if output is sensitive to a feature, but not in a uniformly positive or negative way\n",
      "*******\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<auto_ml.predictor.Predictor at 0x10fe9a1d0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_predictor.train(df2_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scores - 0.9699591155438507"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "\n",
      "\n",
      "***********************************************\n",
      "Advanced scoring metrics for the trained regression model on this particular dataset:\n",
      "\n",
      "Here is the overall RMSE for these predictions:\n",
      "1.412747513040082\n",
      "\n",
      "Here is the average of the predictions:\n",
      "20.02712809009115\n",
      "\n",
      "Here is the average actual value on this validation set:\n",
      "20.024082777165344\n",
      "\n",
      "Here is the median prediction:\n",
      "20.027351197746395\n",
      "\n",
      "Here is the median actual value:\n",
      "20.040008\n",
      "\n",
      "Here is the mean absolute error:\n",
      "0.9699591155438507\n",
      "\n",
      "Here is the median absolute error (robust to outliers):\n",
      "0.6173581977463947\n",
      "\n",
      "Here is the explained variance:\n",
      "0.0004276707451037476\n",
      "\n",
      "Here is the R-squared value:\n",
      "0.0004230261165047988\n",
      "Count of positive differences (prediction > actual):\n",
      "621\n",
      "Count of negative differences:\n",
      "649\n",
      "Average positive difference:\n",
      "0.994940116067793\n",
      "Average negative difference:\n",
      "-0.9460558777543762\n",
      "\n",
      "\n",
      "***********************************************\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Score the model on test data\n",
    "test_score = ml_predictor.score(df2_test, df2_test.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Tensorflow and Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to auto_ml! We're about to go through and make sense of your data using machine learning, and give you a production-ready pipeline to get predictions with.\n",
      "\n",
      "If you have any issues, or new feature ideas, let us know at http://auto.ml\n",
      "You are running on version 2.9.9\n",
      "Now using the model training_params that you passed in:\n",
      "{}\n",
      "After overwriting our defaults with your values, here are the final params that will be used to initialize the model:\n",
      "{'epochs': 1000, 'batch_size': 50, 'verbose': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running basic data cleaning\n",
      "Performing feature scaling\n",
      "Fitting DataFrameVectorizer\n",
      "Now using the model training_params that you passed in:\n",
      "{}\n",
      "After overwriting our defaults with your values, here are the final params that will be used to initialize the model:\n",
      "{'epochs': 1000, 'batch_size': 50, 'verbose': 2}\n",
      "\n",
      "\n",
      "********************************************************************************************\n",
      "About to fit the pipeline for the model DeepLearningRegressor to predict target\n",
      "Started at:\n",
      "2018-11-14 14:51:30\n",
      "\n",
      "We will stop training early if we have not seen an improvement in validation accuracy in 25 epochs\n",
      "To measure validation accuracy, we will split off a random 10 percent of your training data set\n",
      "Train on 4318 samples, validate on 762 samples\n",
      "Epoch 1/1000\n",
      " - 1s - loss: 131.3847 - mean_absolute_error: 8.2263 - mean_absolute_percentage_error: 41.1022 - val_loss: 11.2666 - val_mean_absolute_error: 2.9017 - val_mean_absolute_percentage_error: 14.3746\n",
      "Epoch 2/1000\n",
      " - 1s - loss: 8.3669 - mean_absolute_error: 2.3577 - mean_absolute_percentage_error: 11.8110 - val_loss: 4.6196 - val_mean_absolute_error: 1.5178 - val_mean_absolute_percentage_error: 8.0157\n",
      "Epoch 3/1000\n",
      " - 1s - loss: 7.0071 - mean_absolute_error: 2.1399 - mean_absolute_percentage_error: 10.7123 - val_loss: 8.3171 - val_mean_absolute_error: 2.5566 - val_mean_absolute_percentage_error: 12.6067\n",
      "Epoch 4/1000\n",
      " - 1s - loss: 6.1527 - mean_absolute_error: 2.0044 - mean_absolute_percentage_error: 10.0318 - val_loss: 3.3590 - val_mean_absolute_error: 1.2375 - val_mean_absolute_percentage_error: 6.5752\n",
      "Epoch 5/1000\n",
      " - 1s - loss: 6.1905 - mean_absolute_error: 1.9632 - mean_absolute_percentage_error: 9.7903 - val_loss: 8.7545 - val_mean_absolute_error: 2.6628 - val_mean_absolute_percentage_error: 13.0788\n",
      "Epoch 6/1000\n",
      " - 1s - loss: 5.5907 - mean_absolute_error: 1.8608 - mean_absolute_percentage_error: 9.2813 - val_loss: 2.8933 - val_mean_absolute_error: 1.1338 - val_mean_absolute_percentage_error: 6.0289\n",
      "Epoch 7/1000\n",
      " - 1s - loss: 4.9946 - mean_absolute_error: 1.7523 - mean_absolute_percentage_error: 8.7573 - val_loss: 4.2218 - val_mean_absolute_error: 1.5638 - val_mean_absolute_percentage_error: 8.2915\n",
      "Epoch 8/1000\n",
      " - 1s - loss: 4.1651 - mean_absolute_error: 1.6155 - mean_absolute_percentage_error: 8.1439 - val_loss: 5.2322 - val_mean_absolute_error: 1.9510 - val_mean_absolute_percentage_error: 9.6138\n",
      "Epoch 9/1000\n",
      " - 1s - loss: 3.9049 - mean_absolute_error: 1.5225 - mean_absolute_percentage_error: 7.6438 - val_loss: 2.7420 - val_mean_absolute_error: 1.1135 - val_mean_absolute_percentage_error: 5.9220\n",
      "Epoch 10/1000\n",
      " - 1s - loss: 3.5771 - mean_absolute_error: 1.4471 - mean_absolute_percentage_error: 7.2667 - val_loss: 4.1011 - val_mean_absolute_error: 1.6626 - val_mean_absolute_percentage_error: 8.2384\n",
      "Epoch 11/1000\n",
      " - 1s - loss: 3.4408 - mean_absolute_error: 1.4008 - mean_absolute_percentage_error: 7.0622 - val_loss: 2.6540 - val_mean_absolute_error: 1.0802 - val_mean_absolute_percentage_error: 5.7463\n",
      "Epoch 12/1000\n",
      " - 1s - loss: 3.3658 - mean_absolute_error: 1.3866 - mean_absolute_percentage_error: 6.9862 - val_loss: 3.7487 - val_mean_absolute_error: 1.5518 - val_mean_absolute_percentage_error: 7.7075\n",
      "Epoch 13/1000\n",
      " - 1s - loss: 3.3119 - mean_absolute_error: 1.3803 - mean_absolute_percentage_error: 6.9697 - val_loss: 2.7020 - val_mean_absolute_error: 1.0945 - val_mean_absolute_percentage_error: 5.8264\n",
      "Epoch 14/1000\n",
      " - 1s - loss: 3.3646 - mean_absolute_error: 1.3812 - mean_absolute_percentage_error: 6.9471 - val_loss: 4.3817 - val_mean_absolute_error: 1.7352 - val_mean_absolute_percentage_error: 8.5836\n",
      "Epoch 15/1000\n",
      " - 1s - loss: 3.2379 - mean_absolute_error: 1.3560 - mean_absolute_percentage_error: 6.8346 - val_loss: 3.9032 - val_mean_absolute_error: 1.6055 - val_mean_absolute_percentage_error: 7.9625\n",
      "Epoch 16/1000\n",
      " - 1s - loss: 3.1698 - mean_absolute_error: 1.3403 - mean_absolute_percentage_error: 6.7792 - val_loss: 2.7775 - val_mean_absolute_error: 1.1259 - val_mean_absolute_percentage_error: 5.9989\n",
      "Epoch 17/1000\n",
      " - 1s - loss: 3.2404 - mean_absolute_error: 1.3501 - mean_absolute_percentage_error: 6.8122 - val_loss: 4.0002 - val_mean_absolute_error: 1.6320 - val_mean_absolute_percentage_error: 8.0872\n",
      "Epoch 18/1000\n",
      " - 1s - loss: 3.1707 - mean_absolute_error: 1.3243 - mean_absolute_percentage_error: 6.6801 - val_loss: 2.6993 - val_mean_absolute_error: 1.0968 - val_mean_absolute_percentage_error: 5.8417\n",
      "Epoch 19/1000\n",
      " - 1s - loss: 3.1508 - mean_absolute_error: 1.3193 - mean_absolute_percentage_error: 6.6494 - val_loss: 4.0805 - val_mean_absolute_error: 1.6559 - val_mean_absolute_percentage_error: 8.2031\n",
      "Epoch 20/1000\n",
      " - 1s - loss: 3.1442 - mean_absolute_error: 1.3217 - mean_absolute_percentage_error: 6.6543 - val_loss: 2.5148 - val_mean_absolute_error: 1.0259 - val_mean_absolute_percentage_error: 5.4513\n",
      "Epoch 21/1000\n",
      " - 1s - loss: 3.0307 - mean_absolute_error: 1.2895 - mean_absolute_percentage_error: 6.5192 - val_loss: 3.3826 - val_mean_absolute_error: 1.4406 - val_mean_absolute_percentage_error: 7.1776\n",
      "Epoch 22/1000\n",
      " - 1s - loss: 3.0395 - mean_absolute_error: 1.2891 - mean_absolute_percentage_error: 6.5188 - val_loss: 2.6995 - val_mean_absolute_error: 1.0968 - val_mean_absolute_percentage_error: 5.8435\n",
      "Epoch 23/1000\n",
      " - 1s - loss: 3.0147 - mean_absolute_error: 1.2866 - mean_absolute_percentage_error: 6.4973 - val_loss: 3.6917 - val_mean_absolute_error: 1.5402 - val_mean_absolute_percentage_error: 7.6532\n",
      "Epoch 24/1000\n",
      " - 1s - loss: 2.8666 - mean_absolute_error: 1.2603 - mean_absolute_percentage_error: 6.3897 - val_loss: 2.7436 - val_mean_absolute_error: 1.1120 - val_mean_absolute_percentage_error: 5.9278\n",
      "Epoch 25/1000\n",
      " - 1s - loss: 2.9241 - mean_absolute_error: 1.2606 - mean_absolute_percentage_error: 6.3760 - val_loss: 3.4542 - val_mean_absolute_error: 1.4703 - val_mean_absolute_percentage_error: 7.3305\n",
      "Epoch 26/1000\n",
      " - 1s - loss: 2.8625 - mean_absolute_error: 1.2384 - mean_absolute_percentage_error: 6.2711 - val_loss: 2.5909 - val_mean_absolute_error: 1.0546 - val_mean_absolute_percentage_error: 5.6157\n",
      "Epoch 27/1000\n",
      " - 1s - loss: 2.8719 - mean_absolute_error: 1.2330 - mean_absolute_percentage_error: 6.2504 - val_loss: 3.1408 - val_mean_absolute_error: 1.3594 - val_mean_absolute_percentage_error: 6.7975\n",
      "Epoch 28/1000\n",
      " - 1s - loss: 2.7720 - mean_absolute_error: 1.2050 - mean_absolute_percentage_error: 6.1340 - val_loss: 2.6793 - val_mean_absolute_error: 1.0973 - val_mean_absolute_percentage_error: 5.8420\n",
      "Epoch 29/1000\n",
      " - 1s - loss: 2.6990 - mean_absolute_error: 1.1813 - mean_absolute_percentage_error: 5.9852 - val_loss: 2.9737 - val_mean_absolute_error: 1.2989 - val_mean_absolute_percentage_error: 6.5124\n",
      "Epoch 30/1000\n",
      " - 1s - loss: 2.6478 - mean_absolute_error: 1.1631 - mean_absolute_percentage_error: 5.9132 - val_loss: 2.6459 - val_mean_absolute_error: 1.0917 - val_mean_absolute_percentage_error: 5.8067\n",
      "Epoch 31/1000\n",
      " - 1s - loss: 2.6250 - mean_absolute_error: 1.1569 - mean_absolute_percentage_error: 5.8854 - val_loss: 2.7981 - val_mean_absolute_error: 1.2324 - val_mean_absolute_percentage_error: 6.2005\n",
      "Epoch 32/1000\n",
      " - 1s - loss: 2.5832 - mean_absolute_error: 1.1428 - mean_absolute_percentage_error: 5.8199 - val_loss: 2.8211 - val_mean_absolute_error: 1.1530 - val_mean_absolute_percentage_error: 6.1450\n",
      "Epoch 33/1000\n",
      " - 1s - loss: 2.5537 - mean_absolute_error: 1.1264 - mean_absolute_percentage_error: 5.7310 - val_loss: 2.3816 - val_mean_absolute_error: 0.9896 - val_mean_absolute_percentage_error: 5.2402\n",
      "Epoch 34/1000\n",
      " - 1s - loss: 2.5956 - mean_absolute_error: 1.1440 - mean_absolute_percentage_error: 5.8156 - val_loss: 2.4263 - val_mean_absolute_error: 1.0120 - val_mean_absolute_percentage_error: 5.3692\n",
      "Epoch 35/1000\n",
      " - 1s - loss: 2.4488 - mean_absolute_error: 1.1063 - mean_absolute_percentage_error: 5.6326 - val_loss: 2.8400 - val_mean_absolute_error: 1.2609 - val_mean_absolute_percentage_error: 6.3330\n",
      "Epoch 36/1000\n",
      " - 1s - loss: 2.4678 - mean_absolute_error: 1.1069 - mean_absolute_percentage_error: 5.6209 - val_loss: 2.4885 - val_mean_absolute_error: 1.1172 - val_mean_absolute_percentage_error: 5.6766\n",
      "Epoch 37/1000\n",
      " - 1s - loss: 2.4777 - mean_absolute_error: 1.1137 - mean_absolute_percentage_error: 5.6751 - val_loss: 2.6725 - val_mean_absolute_error: 1.1997 - val_mean_absolute_percentage_error: 6.0485\n",
      "Epoch 38/1000\n",
      " - 1s - loss: 2.3796 - mean_absolute_error: 1.0873 - mean_absolute_percentage_error: 5.5560 - val_loss: 2.5385 - val_mean_absolute_error: 1.0776 - val_mean_absolute_percentage_error: 5.7252\n",
      "Epoch 39/1000\n",
      " - 1s - loss: 2.4074 - mean_absolute_error: 1.0895 - mean_absolute_percentage_error: 5.5639 - val_loss: 2.4951 - val_mean_absolute_error: 1.1262 - val_mean_absolute_percentage_error: 5.7121\n",
      "Epoch 40/1000\n",
      " - 1s - loss: 2.4339 - mean_absolute_error: 1.0999 - mean_absolute_percentage_error: 5.6290 - val_loss: 2.5123 - val_mean_absolute_error: 1.0721 - val_mean_absolute_percentage_error: 5.6932\n",
      "Epoch 41/1000\n",
      " - 1s - loss: 2.4607 - mean_absolute_error: 1.1176 - mean_absolute_percentage_error: 5.6912 - val_loss: 2.8111 - val_mean_absolute_error: 1.1862 - val_mean_absolute_percentage_error: 6.3106\n",
      "Epoch 42/1000\n",
      " - 1s - loss: 2.4132 - mean_absolute_error: 1.0999 - mean_absolute_percentage_error: 5.6076 - val_loss: 2.4076 - val_mean_absolute_error: 1.0946 - val_mean_absolute_percentage_error: 5.5701\n",
      "Epoch 43/1000\n",
      " - 1s - loss: 2.4548 - mean_absolute_error: 1.1206 - mean_absolute_percentage_error: 5.7132 - val_loss: 2.5454 - val_mean_absolute_error: 1.0945 - val_mean_absolute_percentage_error: 5.8136\n",
      "Epoch 44/1000\n",
      " - 1s - loss: 2.3557 - mean_absolute_error: 1.0891 - mean_absolute_percentage_error: 5.5501 - val_loss: 2.4801 - val_mean_absolute_error: 1.1319 - val_mean_absolute_percentage_error: 5.7367\n",
      "Epoch 45/1000\n",
      " - 1s - loss: 2.3840 - mean_absolute_error: 1.0951 - mean_absolute_percentage_error: 5.6045 - val_loss: 2.2115 - val_mean_absolute_error: 0.9606 - val_mean_absolute_percentage_error: 5.0457\n",
      "Epoch 46/1000\n",
      " - 1s - loss: 2.4933 - mean_absolute_error: 1.1317 - mean_absolute_percentage_error: 5.7558 - val_loss: 2.4579 - val_mean_absolute_error: 1.1266 - val_mean_absolute_percentage_error: 5.7061\n",
      "Epoch 47/1000\n",
      " - 1s - loss: 2.4615 - mean_absolute_error: 1.1202 - mean_absolute_percentage_error: 5.7011 - val_loss: 2.7130 - val_mean_absolute_error: 1.1605 - val_mean_absolute_percentage_error: 6.1767\n",
      "Epoch 48/1000\n",
      " - 1s - loss: 2.3066 - mean_absolute_error: 1.0833 - mean_absolute_percentage_error: 5.5259 - val_loss: 2.4167 - val_mean_absolute_error: 1.0510 - val_mean_absolute_percentage_error: 5.5778\n",
      "Epoch 49/1000\n",
      " - 1s - loss: 2.3253 - mean_absolute_error: 1.0747 - mean_absolute_percentage_error: 5.4974 - val_loss: 2.3219 - val_mean_absolute_error: 1.0680 - val_mean_absolute_percentage_error: 5.4473\n",
      "Epoch 50/1000\n",
      " - 1s - loss: 2.3153 - mean_absolute_error: 1.0787 - mean_absolute_percentage_error: 5.5037 - val_loss: 2.5399 - val_mean_absolute_error: 1.0989 - val_mean_absolute_percentage_error: 5.8463\n",
      "Epoch 51/1000\n",
      " - 1s - loss: 2.3345 - mean_absolute_error: 1.0850 - mean_absolute_percentage_error: 5.5406 - val_loss: 3.7652 - val_mean_absolute_error: 1.6009 - val_mean_absolute_percentage_error: 7.9334\n",
      "Epoch 52/1000\n",
      " - 1s - loss: 2.2941 - mean_absolute_error: 1.0790 - mean_absolute_percentage_error: 5.4928 - val_loss: 2.1644 - val_mean_absolute_error: 0.9724 - val_mean_absolute_percentage_error: 5.0374\n",
      "Epoch 53/1000\n",
      " - 1s - loss: 2.2692 - mean_absolute_error: 1.0642 - mean_absolute_percentage_error: 5.4328 - val_loss: 3.0646 - val_mean_absolute_error: 1.3772 - val_mean_absolute_percentage_error: 6.8688\n",
      "Epoch 54/1000\n",
      " - 1s - loss: 2.2426 - mean_absolute_error: 1.0540 - mean_absolute_percentage_error: 5.3799 - val_loss: 2.3896 - val_mean_absolute_error: 1.1140 - val_mean_absolute_percentage_error: 5.6532\n",
      "Epoch 55/1000\n",
      " - 1s - loss: 2.2480 - mean_absolute_error: 1.0557 - mean_absolute_percentage_error: 5.3850 - val_loss: 2.5420 - val_mean_absolute_error: 1.1829 - val_mean_absolute_percentage_error: 5.9691\n",
      "Epoch 56/1000\n",
      " - 1s - loss: 2.2826 - mean_absolute_error: 1.0704 - mean_absolute_percentage_error: 5.4646 - val_loss: 2.3223 - val_mean_absolute_error: 1.0225 - val_mean_absolute_percentage_error: 5.4218\n",
      "Epoch 57/1000\n",
      " - 1s - loss: 2.2437 - mean_absolute_error: 1.0551 - mean_absolute_percentage_error: 5.3993 - val_loss: 2.4974 - val_mean_absolute_error: 1.0925 - val_mean_absolute_percentage_error: 5.8099\n",
      "Epoch 58/1000\n",
      " - 1s - loss: 2.2161 - mean_absolute_error: 1.0471 - mean_absolute_percentage_error: 5.3638 - val_loss: 2.2205 - val_mean_absolute_error: 1.0251 - val_mean_absolute_percentage_error: 5.2546\n",
      "Epoch 59/1000\n",
      " - 1s - loss: 2.2452 - mean_absolute_error: 1.0560 - mean_absolute_percentage_error: 5.3994 - val_loss: 2.1980 - val_mean_absolute_error: 0.9719 - val_mean_absolute_percentage_error: 5.1324\n",
      "Epoch 60/1000\n",
      " - 1s - loss: 2.2216 - mean_absolute_error: 1.0506 - mean_absolute_percentage_error: 5.3742 - val_loss: 3.6928 - val_mean_absolute_error: 1.5871 - val_mean_absolute_percentage_error: 7.8690\n",
      "Epoch 61/1000\n",
      " - 1s - loss: 2.2263 - mean_absolute_error: 1.0562 - mean_absolute_percentage_error: 5.3963 - val_loss: 2.5474 - val_mean_absolute_error: 1.1123 - val_mean_absolute_percentage_error: 5.9201\n",
      "Epoch 62/1000\n",
      " - 1s - loss: 2.2007 - mean_absolute_error: 1.0397 - mean_absolute_percentage_error: 5.3213 - val_loss: 2.1421 - val_mean_absolute_error: 0.9712 - val_mean_absolute_percentage_error: 5.0303\n",
      "Epoch 63/1000\n",
      " - 1s - loss: 2.1886 - mean_absolute_error: 1.0397 - mean_absolute_percentage_error: 5.3084 - val_loss: 3.4106 - val_mean_absolute_error: 1.4082 - val_mean_absolute_percentage_error: 7.4843\n",
      "Epoch 64/1000\n",
      " - 1s - loss: 2.1917 - mean_absolute_error: 1.0346 - mean_absolute_percentage_error: 5.3017 - val_loss: 2.2370 - val_mean_absolute_error: 1.0366 - val_mean_absolute_percentage_error: 5.3039\n",
      "Epoch 65/1000\n",
      " - 1s - loss: 2.1394 - mean_absolute_error: 1.0185 - mean_absolute_percentage_error: 5.2015 - val_loss: 2.2685 - val_mean_absolute_error: 1.0565 - val_mean_absolute_percentage_error: 5.3955\n",
      "Epoch 66/1000\n",
      " - 1s - loss: 2.1811 - mean_absolute_error: 1.0326 - mean_absolute_percentage_error: 5.2772 - val_loss: 2.2167 - val_mean_absolute_error: 0.9880 - val_mean_absolute_percentage_error: 5.2218\n",
      "Epoch 67/1000\n",
      " - 1s - loss: 2.0980 - mean_absolute_error: 1.0095 - mean_absolute_percentage_error: 5.1697 - val_loss: 2.2896 - val_mean_absolute_error: 1.0171 - val_mean_absolute_percentage_error: 5.3919\n",
      "Epoch 68/1000\n",
      " - 1s - loss: 2.1384 - mean_absolute_error: 1.0182 - mean_absolute_percentage_error: 5.2104 - val_loss: 2.1485 - val_mean_absolute_error: 0.9837 - val_mean_absolute_percentage_error: 5.0848\n",
      "Epoch 69/1000\n",
      " - 1s - loss: 2.1730 - mean_absolute_error: 1.0329 - mean_absolute_percentage_error: 5.2808 - val_loss: 2.2449 - val_mean_absolute_error: 0.9980 - val_mean_absolute_percentage_error: 5.2843\n",
      "Epoch 70/1000\n",
      " - 1s - loss: 2.1254 - mean_absolute_error: 1.0189 - mean_absolute_percentage_error: 5.2099 - val_loss: 2.3157 - val_mean_absolute_error: 1.0864 - val_mean_absolute_percentage_error: 5.5312\n",
      "Epoch 71/1000\n",
      " - 1s - loss: 2.1269 - mean_absolute_error: 1.0119 - mean_absolute_percentage_error: 5.1759 - val_loss: 2.1375 - val_mean_absolute_error: 0.9523 - val_mean_absolute_percentage_error: 5.0078\n",
      "Epoch 72/1000\n",
      " - 1s - loss: 2.1170 - mean_absolute_error: 1.0003 - mean_absolute_percentage_error: 5.1308 - val_loss: 2.1668 - val_mean_absolute_error: 0.9989 - val_mean_absolute_percentage_error: 5.1488\n",
      "Epoch 73/1000\n",
      " - 1s - loss: 2.1065 - mean_absolute_error: 1.0022 - mean_absolute_percentage_error: 5.1171 - val_loss: 2.1157 - val_mean_absolute_error: 0.9523 - val_mean_absolute_percentage_error: 4.9524\n",
      "Epoch 74/1000\n",
      " - 1s - loss: 2.1092 - mean_absolute_error: 1.0109 - mean_absolute_percentage_error: 5.1683 - val_loss: 2.2047 - val_mean_absolute_error: 1.0227 - val_mean_absolute_percentage_error: 5.2466\n",
      "Epoch 75/1000\n",
      " - 1s - loss: 2.0841 - mean_absolute_error: 0.9906 - mean_absolute_percentage_error: 5.0754 - val_loss: 2.1981 - val_mean_absolute_error: 1.0179 - val_mean_absolute_percentage_error: 5.2245\n",
      "Epoch 76/1000\n",
      " - 1s - loss: 2.0740 - mean_absolute_error: 0.9929 - mean_absolute_percentage_error: 5.0743 - val_loss: 2.2231 - val_mean_absolute_error: 0.9892 - val_mean_absolute_percentage_error: 5.2358\n",
      "Epoch 77/1000\n",
      " - 1s - loss: 2.0605 - mean_absolute_error: 0.9896 - mean_absolute_percentage_error: 5.0683 - val_loss: 2.1338 - val_mean_absolute_error: 0.9724 - val_mean_absolute_percentage_error: 5.0350\n",
      "Epoch 78/1000\n",
      " - 1s - loss: 2.0737 - mean_absolute_error: 0.9893 - mean_absolute_percentage_error: 5.0607 - val_loss: 2.1110 - val_mean_absolute_error: 0.9455 - val_mean_absolute_percentage_error: 4.9415\n",
      "Epoch 79/1000\n",
      " - 1s - loss: 2.0549 - mean_absolute_error: 0.9780 - mean_absolute_percentage_error: 5.0130 - val_loss: 2.3463 - val_mean_absolute_error: 1.0414 - val_mean_absolute_percentage_error: 5.5312\n",
      "Epoch 80/1000\n",
      " - 1s - loss: 2.0585 - mean_absolute_error: 0.9799 - mean_absolute_percentage_error: 5.0328 - val_loss: 2.1462 - val_mean_absolute_error: 0.9544 - val_mean_absolute_percentage_error: 5.0282\n",
      "Epoch 81/1000\n",
      " - 1s - loss: 2.0522 - mean_absolute_error: 0.9803 - mean_absolute_percentage_error: 5.0190 - val_loss: 2.1359 - val_mean_absolute_error: 0.9761 - val_mean_absolute_percentage_error: 5.0490\n",
      "Epoch 82/1000\n",
      " - 1s - loss: 2.0491 - mean_absolute_error: 0.9780 - mean_absolute_percentage_error: 5.0037 - val_loss: 2.2027 - val_mean_absolute_error: 0.9823 - val_mean_absolute_percentage_error: 5.1949\n",
      "Epoch 83/1000\n",
      " - 1s - loss: 2.0515 - mean_absolute_error: 0.9742 - mean_absolute_percentage_error: 5.0017 - val_loss: 2.1124 - val_mean_absolute_error: 0.9545 - val_mean_absolute_percentage_error: 4.9653\n",
      "Epoch 84/1000\n",
      " - 1s - loss: 2.0525 - mean_absolute_error: 0.9795 - mean_absolute_percentage_error: 5.0154 - val_loss: 2.1680 - val_mean_absolute_error: 0.9670 - val_mean_absolute_percentage_error: 5.1020\n",
      "Epoch 85/1000\n",
      " - 1s - loss: 2.0234 - mean_absolute_error: 0.9709 - mean_absolute_percentage_error: 4.9741 - val_loss: 2.1110 - val_mean_absolute_error: 0.9478 - val_mean_absolute_percentage_error: 4.9462\n",
      "Epoch 86/1000\n",
      " - 1s - loss: 2.0436 - mean_absolute_error: 0.9757 - mean_absolute_percentage_error: 5.0022 - val_loss: 2.1480 - val_mean_absolute_error: 0.9874 - val_mean_absolute_percentage_error: 5.0994\n",
      "Epoch 87/1000\n",
      " - 1s - loss: 2.0325 - mean_absolute_error: 0.9734 - mean_absolute_percentage_error: 4.9903 - val_loss: 2.1540 - val_mean_absolute_error: 0.9907 - val_mean_absolute_percentage_error: 5.1130\n",
      "Epoch 88/1000\n",
      " - 1s - loss: 2.0333 - mean_absolute_error: 0.9685 - mean_absolute_percentage_error: 4.9685 - val_loss: 2.1142 - val_mean_absolute_error: 0.9542 - val_mean_absolute_percentage_error: 4.9671\n",
      "Epoch 89/1000\n",
      " - 1s - loss: 2.0219 - mean_absolute_error: 0.9704 - mean_absolute_percentage_error: 4.9675 - val_loss: 2.1472 - val_mean_absolute_error: 0.9553 - val_mean_absolute_percentage_error: 5.0335\n",
      "Epoch 90/1000\n",
      " - 1s - loss: 2.0246 - mean_absolute_error: 0.9749 - mean_absolute_percentage_error: 4.9929 - val_loss: 2.2998 - val_mean_absolute_error: 1.0790 - val_mean_absolute_percentage_error: 5.4963\n",
      "Epoch 91/1000\n",
      " - 1s - loss: 2.0307 - mean_absolute_error: 0.9731 - mean_absolute_percentage_error: 4.9831 - val_loss: 2.1817 - val_mean_absolute_error: 0.9730 - val_mean_absolute_percentage_error: 5.1386\n",
      "Epoch 92/1000\n",
      " - 1s - loss: 2.0243 - mean_absolute_error: 0.9730 - mean_absolute_percentage_error: 4.9807 - val_loss: 2.1136 - val_mean_absolute_error: 0.9537 - val_mean_absolute_percentage_error: 4.9634\n",
      "Epoch 93/1000\n",
      " - 1s - loss: 2.0220 - mean_absolute_error: 0.9696 - mean_absolute_percentage_error: 4.9703 - val_loss: 2.1316 - val_mean_absolute_error: 0.9488 - val_mean_absolute_percentage_error: 4.9896\n",
      "Epoch 94/1000\n",
      " - 1s - loss: 2.0371 - mean_absolute_error: 0.9692 - mean_absolute_percentage_error: 4.9723 - val_loss: 2.1178 - val_mean_absolute_error: 0.9548 - val_mean_absolute_percentage_error: 4.9767\n",
      "Epoch 95/1000\n",
      " - 1s - loss: 2.0164 - mean_absolute_error: 0.9632 - mean_absolute_percentage_error: 4.9388 - val_loss: 2.1153 - val_mean_absolute_error: 0.9469 - val_mean_absolute_percentage_error: 4.9516\n",
      "Epoch 96/1000\n",
      " - 1s - loss: 2.0174 - mean_absolute_error: 0.9655 - mean_absolute_percentage_error: 4.9530 - val_loss: 2.1152 - val_mean_absolute_error: 0.9506 - val_mean_absolute_percentage_error: 4.9606\n",
      "Epoch 97/1000\n",
      " - 1s - loss: 2.0099 - mean_absolute_error: 0.9635 - mean_absolute_percentage_error: 4.9364 - val_loss: 2.1179 - val_mean_absolute_error: 0.9463 - val_mean_absolute_percentage_error: 4.9523\n",
      "Epoch 98/1000\n",
      " - 1s - loss: 2.0149 - mean_absolute_error: 0.9688 - mean_absolute_percentage_error: 4.9626 - val_loss: 2.1245 - val_mean_absolute_error: 0.9473 - val_mean_absolute_percentage_error: 4.9714\n",
      "Epoch 99/1000\n",
      " - 1s - loss: 2.0175 - mean_absolute_error: 0.9647 - mean_absolute_percentage_error: 4.9412 - val_loss: 2.1138 - val_mean_absolute_error: 0.9464 - val_mean_absolute_percentage_error: 4.9405\n",
      "Epoch 100/1000\n",
      " - 1s - loss: 2.0175 - mean_absolute_error: 0.9612 - mean_absolute_percentage_error: 4.9286 - val_loss: 2.1498 - val_mean_absolute_error: 0.9575 - val_mean_absolute_percentage_error: 5.0420\n",
      "Epoch 101/1000\n",
      " - 1s - loss: 2.0088 - mean_absolute_error: 0.9624 - mean_absolute_percentage_error: 4.9379 - val_loss: 2.1287 - val_mean_absolute_error: 0.9688 - val_mean_absolute_percentage_error: 5.0212\n",
      "Epoch 102/1000\n",
      " - 1s - loss: 2.0053 - mean_absolute_error: 0.9610 - mean_absolute_percentage_error: 4.9264 - val_loss: 2.1818 - val_mean_absolute_error: 0.9660 - val_mean_absolute_percentage_error: 5.1035\n",
      "Epoch 103/1000\n",
      " - 1s - loss: 2.0122 - mean_absolute_error: 0.9639 - mean_absolute_percentage_error: 4.9397 - val_loss: 2.1254 - val_mean_absolute_error: 0.9471 - val_mean_absolute_percentage_error: 4.9698\n",
      "Epoch 00103: early stopping\n",
      "Finished training the pipeline!\n",
      "Total training time:\n",
      "0:01:02\n",
      "Calculating feature responses, for advanced analytics.\n",
      "Here are our feature responses for the trained model\n",
      "+----+----------------+---------+-------------------+-------------------+-----------+-----------+\n",
      "|    | Feature Name   |   Delta |   FR_Decrementing |   FR_Incrementing |   FRD_MAD |   FRI_MAD |\n",
      "|----+----------------+---------+-------------------+-------------------+-----------+-----------|\n",
      "| 99 | cat25          |  0.0753 |            0.0000 |           -0.0000 |    0.0000 |    0.0000 |\n",
      "| 98 | cat24          |  0.0328 |            0.0000 |           -0.0000 |    0.0000 |    0.0000 |\n",
      "| 97 | num15          |  0.1218 |           -0.0000 |            0.0000 |    0.0001 |    0.0001 |\n",
      "| 96 | cat10          |  0.0442 |           -0.0001 |            0.0001 |    0.0001 |    0.0001 |\n",
      "| 95 | num45          |  0.1372 |            0.0001 |           -0.0001 |    0.0001 |    0.0001 |\n",
      "| 94 | num58          |  0.1256 |            0.0001 |           -0.0001 |    0.0001 |    0.0001 |\n",
      "| 93 | num10          |  0.1414 |           -0.0001 |            0.0001 |    0.0001 |    0.0001 |\n",
      "| 92 | cat3           |  0.0902 |           -0.0001 |            0.0001 |    0.0001 |    0.0001 |\n",
      "| 91 | cat9           |  0.0336 |           -0.0002 |            0.0002 |    0.0002 |    0.0002 |\n",
      "| 90 | num52          |  0.1372 |            0.0002 |           -0.0002 |    0.0002 |    0.0002 |\n",
      "| 89 | num14          |  0.1311 |            0.0003 |           -0.0003 |    0.0003 |    0.0003 |\n",
      "| 88 | num17          |  0.1220 |            0.0004 |           -0.0004 |    0.0004 |    0.0004 |\n",
      "| 87 | cat16          |  0.0336 |            0.0005 |           -0.0005 |    0.0005 |    0.0005 |\n",
      "| 86 | cat2_E         |  0.1412 |            0.0005 |           -0.0005 |    0.0005 |    0.0005 |\n",
      "| 85 | cat8           |  0.0553 |           -0.0006 |            0.0006 |    0.0006 |    0.0006 |\n",
      "| 84 | cat5           |  0.0951 |            0.0006 |           -0.0006 |    0.0006 |    0.0006 |\n",
      "| 83 | cat18          |  0.0531 |           -0.0006 |            0.0006 |    0.0006 |    0.0006 |\n",
      "| 82 | num49          |  0.1371 |           -0.0006 |            0.0007 |    0.0006 |    0.0006 |\n",
      "| 81 | num32          |  0.1331 |           -0.0006 |            0.0007 |    0.0006 |    0.0006 |\n",
      "| 80 | num11          |  0.1420 |            0.0007 |           -0.0007 |    0.0007 |    0.0007 |\n",
      "| 79 | cat14          |  0.0857 |           -0.0007 |            0.0007 |    0.0007 |    0.0007 |\n",
      "| 78 | cat23          |  0.0414 |           -0.0007 |            0.0007 |    0.0007 |    0.0007 |\n",
      "| 77 | num36          |  0.1321 |            0.0009 |           -0.0009 |    0.0009 |    0.0009 |\n",
      "| 76 | num13          |  0.1319 |            0.0009 |           -0.0009 |    0.0009 |    0.0009 |\n",
      "| 75 | num7           |  0.1310 |           -0.0009 |            0.0009 |    0.0010 |    0.0010 |\n",
      "| 74 | num37          |  0.1322 |           -0.0010 |            0.0010 |    0.0010 |    0.0010 |\n",
      "| 73 | num56          |  0.1371 |           -0.0011 |            0.0011 |    0.0011 |    0.0011 |\n",
      "| 72 | num53          |  0.1359 |           -0.0011 |            0.0012 |    0.0011 |    0.0011 |\n",
      "| 71 | cat26          |  0.0508 |            0.0012 |           -0.0012 |    0.0012 |    0.0012 |\n",
      "| 70 | num41          |  0.1333 |            0.0013 |           -0.0013 |    0.0013 |    0.0013 |\n",
      "| 69 | cat15          |  0.0396 |           -0.0013 |            0.0013 |    0.0013 |    0.0013 |\n",
      "| 68 | num46          |  0.1359 |           -0.0013 |            0.0013 |    0.0013 |    0.0013 |\n",
      "| 67 | num8           |  0.1420 |           -0.0014 |            0.0014 |    0.0014 |    0.0014 |\n",
      "| 66 | num43          |  0.1224 |           -0.0016 |            0.0016 |    0.0016 |    0.0016 |\n",
      "| 65 | num42          |  0.1317 |           -0.0016 |            0.0016 |    0.0016 |    0.0016 |\n",
      "| 64 | cat2_B         |  0.1340 |            0.0016 |           -0.0016 |    0.0016 |    0.0016 |\n",
      "| 63 | num24          |  0.1375 |            0.0017 |           -0.0017 |    0.0017 |    0.0017 |\n",
      "| 62 | cat7           |  0.0781 |           -0.0018 |            0.0018 |    0.0018 |    0.0018 |\n",
      "| 61 | cat20          |  0.1048 |           -0.0018 |            0.0018 |    0.0018 |    0.0018 |\n",
      "| 60 | num16          |  0.1223 |            0.0018 |           -0.0018 |    0.0018 |    0.0018 |\n",
      "| 59 | num29          |  0.1398 |            0.0019 |           -0.0018 |    0.0018 |    0.0018 |\n",
      "| 58 | cat1_C         |  0.2012 |           -0.0019 |            0.0019 |    0.0018 |    0.0019 |\n",
      "| 57 | num35          |  0.1335 |            0.0019 |           -0.0019 |    0.0019 |    0.0019 |\n",
      "| 56 | num38          |  0.1315 |            0.0020 |           -0.0020 |    0.0020 |    0.0020 |\n",
      "| 55 | num23          |  0.1301 |            0.0020 |           -0.0020 |    0.0020 |    0.0020 |\n",
      "| 54 | cat17          |  0.0775 |           -0.0020 |            0.0021 |    0.0020 |    0.0021 |\n",
      "| 53 | num34          |  0.1333 |           -0.0021 |            0.0021 |    0.0021 |    0.0021 |\n",
      "| 52 | num51          |  0.1403 |           -0.0021 |            0.0021 |    0.0021 |    0.0021 |\n",
      "| 51 | num44          |  0.1403 |           -0.0021 |            0.0021 |    0.0021 |    0.0021 |\n",
      "| 50 | num12          |  0.1294 |            0.0022 |           -0.0022 |    0.0022 |    0.0022 |\n",
      "| 49 | num47          |  0.1336 |            0.0023 |           -0.0023 |    0.0023 |    0.0023 |\n",
      "| 48 | num19          |  0.1231 |            0.0024 |           -0.0024 |    0.0024 |    0.0024 |\n",
      "| 47 | num33          |  0.1332 |           -0.0024 |            0.0024 |    0.0024 |    0.0024 |\n",
      "| 46 | cat11          |  0.0876 |            0.0025 |           -0.0025 |    0.0025 |    0.0025 |\n",
      "| 45 | num2           |  0.1418 |           -0.0025 |            0.0025 |    0.0025 |    0.0025 |\n",
      "| 44 | cat2_F         |  0.1403 |            0.0025 |           -0.0025 |    0.0025 |    0.0025 |\n",
      "| 43 | num9           |  0.1420 |           -0.0025 |            0.0025 |    0.0025 |    0.0025 |\n",
      "| 42 | num54          |  0.1336 |            0.0026 |           -0.0026 |    0.0026 |    0.0026 |\n",
      "| 41 | num20          |  0.1235 |            0.0031 |           -0.0031 |    0.0031 |    0.0031 |\n",
      "| 40 | num50          |  0.1343 |            0.0031 |           -0.0031 |    0.0031 |    0.0031 |\n",
      "| 39 | num57          |  0.1343 |            0.0032 |           -0.0032 |    0.0032 |    0.0032 |\n",
      "| 38 | cat12          |  0.1020 |            0.0032 |           -0.0032 |    0.0032 |    0.0032 |\n",
      "| 37 | cat2_L         |  0.1401 |           -0.0032 |            0.0032 |    0.0032 |    0.0032 |\n",
      "| 36 | num1           |  0.1491 |            0.0032 |           -0.0032 |    0.0032 |    0.0032 |\n",
      "| 35 | num48          |  0.1337 |            0.0033 |           -0.0033 |    0.0033 |    0.0033 |\n",
      "| 34 | num39          |  0.1448 |           -0.0033 |            0.0033 |    0.0033 |    0.0033 |\n",
      "| 33 | cat6           |  0.0860 |            0.0034 |           -0.0034 |    0.0034 |    0.0034 |\n",
      "| 32 | num55          |  0.1337 |            0.0035 |           -0.0035 |    0.0035 |    0.0035 |\n",
      "| 31 | num27          |  0.1446 |           -0.0036 |            0.0036 |    0.0035 |    0.0036 |\n",
      "| 30 | cat2_A         |  0.1337 |            0.0037 |           -0.0037 |    0.0037 |    0.0037 |\n",
      "| 29 | num3           |  0.1235 |           -0.0039 |            0.0039 |    0.0039 |    0.0039 |\n",
      "| 28 | num25          |  0.1401 |           -0.0039 |            0.0039 |    0.0039 |    0.0039 |\n",
      "| 27 | cat19          |  0.0899 |            0.0040 |           -0.0040 |    0.0040 |    0.0040 |\n",
      "| 26 | num26          |  0.1479 |           -0.0042 |            0.0042 |    0.0042 |    0.0042 |\n",
      "| 25 | cat2_I         |  0.1368 |            0.0043 |           -0.0043 |    0.0043 |    0.0043 |\n",
      "| 24 | cat2_C         |  0.1424 |           -0.0043 |            0.0044 |    0.0043 |    0.0044 |\n",
      "| 23 | cat22          |  0.0852 |            0.0044 |           -0.0044 |    0.0044 |    0.0044 |\n",
      "| 22 | cat2_G         |  0.1359 |            0.0045 |           -0.0045 |    0.0045 |    0.0045 |\n",
      "| 21 | num5           |  0.1268 |            0.0047 |           -0.0047 |    0.0047 |    0.0047 |\n",
      "| 20 | num21          |  0.1233 |            0.0047 |           -0.0047 |    0.0047 |    0.0047 |\n",
      "| 19 | cat2_H         |  0.1418 |            0.0049 |           -0.0049 |    0.0049 |    0.0049 |\n",
      "| 18 | num30          |  0.1397 |            0.0050 |           -0.0050 |    0.0050 |    0.0049 |\n",
      "| 17 | cat4           |  0.1033 |           -0.0050 |            0.0050 |    0.0049 |    0.0050 |\n",
      "| 16 | cat1_B         |  0.2030 |           -0.0051 |            0.0051 |    0.0051 |    0.0051 |\n",
      "| 15 | num4           |  0.1251 |            0.0052 |           -0.0052 |    0.0052 |    0.0052 |\n",
      "| 14 | num40          |  0.1403 |            0.0057 |           -0.0057 |    0.0057 |    0.0056 |\n",
      "| 13 | cat21          |  0.0977 |            0.0058 |           -0.0058 |    0.0058 |    0.0058 |\n",
      "| 12 | num59          |  0.1587 |           -0.0059 |            0.0059 |    0.0059 |    0.0059 |\n",
      "| 11 | num18          |  0.1239 |            0.0061 |           -0.0061 |    0.0061 |    0.0061 |\n",
      "| 10 | cat1_A         |  0.1953 |           -0.0061 |            0.0062 |    0.0061 |    0.0062 |\n",
      "|  9 | cat13          |  0.0991 |           -0.0070 |            0.0070 |    0.0069 |    0.0070 |\n",
      "|  8 | cat2_K         |  0.1347 |           -0.0073 |            0.0074 |    0.0073 |    0.0074 |\n",
      "|  7 | num28          |  0.1417 |            0.0076 |           -0.0075 |    0.0076 |    0.0075 |\n",
      "|  6 | cat2_J         |  0.1398 |           -0.0082 |            0.0083 |    0.0082 |    0.0083 |\n",
      "|  5 | cat2_D         |  0.1371 |           -0.0090 |            0.0091 |    0.0090 |    0.0091 |\n",
      "|  4 | cat1_D         |  0.2003 |            0.0097 |           -0.0096 |    0.0097 |    0.0096 |\n",
      "|  3 | cat1_E         |  0.2001 |           -0.0115 |            0.0117 |    0.0115 |    0.0117 |\n",
      "|  2 | num6           |  0.1294 |            0.0120 |           -0.0119 |    0.0120 |    0.0118 |\n",
      "|  1 | num22          |  0.1231 |            0.0155 |           -0.0152 |    0.0155 |    0.0152 |\n",
      "|  0 | num31          |  0.5502 |            0.0250 |           -0.0247 |    0.0251 |    0.0247 |\n",
      "+----+----------------+---------+-------------------+-------------------+-----------+-----------+\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<auto_ml.predictor.Predictor at 0x10fe9a1d0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_predictor.train(d2f_train, model_names='DeepLearningRegressor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scores - 0.968573689967262"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "\n",
      "\n",
      "***********************************************\n",
      "Advanced scoring metrics for the trained regression model on this particular dataset:\n",
      "\n",
      "Here is the overall RMSE for these predictions:\n",
      "1.4127158903457109\n",
      "\n",
      "Here is the average of the predictions:\n",
      "20.06877771963285\n",
      "\n",
      "Here is the average actual value on this validation set:\n",
      "20.024082777165344\n",
      "\n",
      "Here is the median prediction:\n",
      "20.05689239501953\n",
      "\n",
      "Here is the median actual value:\n",
      "20.040008\n",
      "\n",
      "Here is the mean absolute error:\n",
      "0.968573689967262\n",
      "\n",
      "Here is the median absolute error (robust to outliers):\n",
      "0.6194534805908205\n",
      "\n",
      "Here is the explained variance:\n",
      "0.0014682439495429245\n",
      "\n",
      "Here is the R-squared value:\n",
      "0.00046777432959321796\n",
      "Count of positive differences (prediction > actual):\n",
      "638\n",
      "Count of negative differences:\n",
      "632\n",
      "Average positive difference:\n",
      "1.0085040463888229\n",
      "Average negative difference:\n",
      "-0.9282642478834695\n",
      "\n",
      "\n",
      "***********************************************\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Score the model on test data\n",
    "test_score = ml_predictor.score(df2_test, df2_test.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing with XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to auto_ml! We're about to go through and make sense of your data using machine learning, and give you a production-ready pipeline to get predictions with.\n",
      "\n",
      "If you have any issues, or new feature ideas, let us know at http://auto.ml\n",
      "You are running on version 2.9.9\n",
      "Now using the model training_params that you passed in:\n",
      "{}\n",
      "After overwriting our defaults with your values, here are the final params that will be used to initialize the model:\n",
      "{}\n",
      "Running basic data cleaning\n",
      "Fitting DataFrameVectorizer\n",
      "Now using the model training_params that you passed in:\n",
      "{}\n",
      "After overwriting our defaults with your values, here are the final params that will be used to initialize the model:\n",
      "{}\n",
      "\n",
      "\n",
      "********************************************************************************************\n",
      "About to fit the pipeline for the model XGBRegressor to predict target\n",
      "Started at:\n",
      "2018-11-14 20:31:46\n",
      "Finished training the pipeline!\n",
      "Total training time:\n",
      "0:00:04\n",
      "\n",
      "\n",
      "Here are the results from our XGBRegressor\n",
      "predicting target\n",
      "Calculating feature responses, for advanced analytics.\n",
      "The printed list will only contain at most the top 100 features.\n",
      "+----+----------------+--------------+----------+-------------------+-------------------+-----------+-----------+-----------+-----------+\n",
      "|    | Feature Name   |   Importance |    Delta |   FR_Decrementing |   FR_Incrementing |   FRD_abs |   FRI_abs |   FRD_MAD |   FRI_MAD |\n",
      "|----+----------------+--------------+----------+-------------------+-------------------+-----------+-----------+-----------+-----------|\n",
      "| 99 | cat9=True      |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 75 | cat15=True     |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 60 | cat1=C         |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 74 | cat15=False    |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 73 | cat14=True     |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 72 | cat14=False    |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 71 | cat13=True     |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 69 | cat12=True     |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 68 | cat12=False    |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 67 | cat11=True     |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 66 | cat11=False    |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 76 | cat16=False    |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 65 | cat10=True     |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 63 | cat1=B         |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 62 | cat1=A         |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 98 | cat8=True      |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 50 | num51          |       0.0000 |   4.2226 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 51 | num52          |       0.0000 |   2.2513 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 52 | num53          |       0.0000 |   2.0393 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 53 | num54          |       0.0000 |  76.7681 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 54 | num55          |       0.0000 |  74.4318 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 55 | num56          |       0.0000 |   2.2500 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 56 | num57          |       0.0000 |  76.7571 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 64 | cat10=False    |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 77 | cat16=True     |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 59 | cat1=D         |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 79 | cat17=True     |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 93 | cat6=False     |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 86 | cat2=B         |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 94 | cat6=True      |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 95 | cat7=False     |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 78 | cat17=False    |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 96 | cat7=True      |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 85 | cat2=G         |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 83 | cat19=True     |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 82 | cat19=False    |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 81 | cat18=True     |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 97 | cat8=False     |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 80 | cat18=False    |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 84 | cat2=E         |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 92 | cat4=False     |       0.0017 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 91 | cat20=False    |       0.0017 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "|  8 | num9           |       0.0017 |   4.0730 |           -0.0064 |            0.0025 |    0.0064 |    0.0025 |    0.0000 |    0.0000 |\n",
      "| 90 | cat2=H         |       0.0017 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 88 | cat2=K         |       0.0017 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 87 | cat2=D         |       0.0017 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 11 | num12          |       0.0034 |   0.0345 |           -0.0000 |            0.0001 |    0.0001 |    0.0002 |    0.0000 |    0.0000 |\n",
      "| 10 | num11          |       0.0034 |   4.0621 |           -0.0005 |            0.0003 |    0.0005 |    0.0003 |    0.0000 |    0.0000 |\n",
      "| 41 | num42          |       0.0034 |  18.3723 |           -0.0009 |            0.0022 |    0.0009 |    0.0022 |    0.0000 |    0.0000 |\n",
      "| 70 | cat13=False    |       0.0034 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 89 | cat2=I         |       0.0034 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 43 | num44          |       0.0051 |   4.2226 |           -0.0001 |            0.0001 |    0.0011 |    0.0018 |    0.0000 |    0.0000 |\n",
      "| 61 | cat1=E         |       0.0051 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 40 | num41          |       0.0051 |   0.5467 |           -0.0013 |           -0.0004 |    0.0029 |    0.0017 |    0.0000 |    0.0000 |\n",
      "| 14 | num15          |       0.0051 |   0.7977 |            0.0040 |           -0.0031 |    0.0041 |    0.0033 |    0.0000 |    0.0000 |\n",
      "| 35 | num36          |       0.0051 |   0.5135 |            0.0002 |           -0.0025 |    0.0008 |    0.0031 |    0.0000 |    0.0000 |\n",
      "| 33 | num34          |       0.0051 |  26.5347 |            0.0004 |           -0.0032 |    0.0018 |    0.0130 |    0.0000 |    0.0000 |\n",
      "| 28 | num29          |       0.0051 |   6.7246 |            0.0022 |           -0.0007 |    0.0025 |    0.0011 |    0.0000 |    0.0000 |\n",
      "| 29 | num30          |       0.0051 |  19.5508 |            0.0003 |           -0.0006 |    0.0006 |    0.0006 |    0.0000 |    0.0000 |\n",
      "| 49 | num50          |       0.0068 |  76.7571 |            0.0014 |           -0.0024 |    0.0014 |    0.0025 |    0.0000 |    0.0000 |\n",
      "| 45 | num46          |       0.0068 |   2.0393 |            0.0013 |           -0.0022 |    0.0013 |    0.0022 |    0.0000 |    0.0000 |\n",
      "| 48 | num49          |       0.0085 |   2.2500 |            0.0005 |           -0.0015 |    0.0005 |    0.0015 |    0.0000 |    0.0000 |\n",
      "| 46 | num47          |       0.0085 |  76.7681 |            0.0003 |           -0.0004 |    0.0018 |    0.0030 |    0.0000 |    0.0000 |\n",
      "|  9 | num10          |       0.0085 |   4.0758 |            0.0097 |            0.0004 |    0.0116 |    0.0011 |    0.0000 |    0.0000 |\n",
      "| 58 | num59          |       0.0102 | 919.8211 |           -0.0065 |            0.0346 |    0.0080 |    0.0368 |    0.0000 |    0.0000 |\n",
      "| 36 | num37          |       0.0102 |  18.3793 |           -0.0034 |            0.0023 |    0.0044 |    0.0035 |    0.0000 |    0.0000 |\n",
      "| 25 | num26          |       0.0102 |   8.6364 |            0.0165 |           -0.0097 |    0.0187 |    0.0111 |    0.0000 |    0.0000 |\n",
      "|  7 | num8           |       0.0119 |   4.0887 |           -0.0006 |            0.0008 |    0.0014 |    0.0029 |    0.0000 |    0.0000 |\n",
      "| 42 | num43          |       0.0119 |   0.5782 |            0.0022 |           -0.0026 |    0.0028 |    0.0040 |    0.0000 |    0.0000 |\n",
      "| 22 | num23          |       0.0119 |   2.2769 |            0.0027 |           -0.0012 |    0.0027 |    0.0012 |    0.0000 |    0.0000 |\n",
      "| 24 | num25          |       0.0136 |   7.9282 |           -0.0023 |           -0.0001 |    0.0037 |    0.0021 |    0.0000 |    0.0000 |\n",
      "| 12 | num13          |       0.0153 |   0.0475 |           -0.0034 |            0.0042 |    0.0034 |    0.0042 |    0.0000 |    0.0000 |\n",
      "| 44 | num45          |       0.0153 |   2.2513 |           -0.0021 |            0.0058 |    0.0021 |    0.0058 |    0.0000 |    0.0000 |\n",
      "| 13 | num14          |       0.0153 |   0.0569 |            0.0006 |           -0.0003 |    0.0028 |    0.0027 |    0.0000 |    0.0000 |\n",
      "| 27 | num28          |       0.0170 |   5.5592 |            0.0114 |           -0.0055 |    0.0131 |    0.0071 |    0.0000 |    0.0000 |\n",
      "| 32 | num33          |       0.0170 |  26.6477 |            0.0116 |           -0.0056 |    0.0125 |    0.0070 |    0.0000 |    0.0000 |\n",
      "| 37 | num38          |       0.0170 |  17.9140 |           -0.0016 |            0.0007 |    0.0115 |    0.0067 |    0.0000 |    0.0000 |\n",
      "| 23 | num24          |       0.0187 |   4.1844 |            0.0023 |           -0.0039 |    0.0026 |    0.0039 |    0.0000 |    0.0000 |\n",
      "| 17 | num18          |       0.0187 |   0.7165 |            0.0064 |           -0.0042 |    0.0081 |    0.0052 |    0.0000 |    0.0000 |\n",
      "| 30 | num31          |       0.0187 |   0.5502 |           -0.0005 |            0.0019 |    0.0016 |    0.0028 |    0.0000 |    0.0000 |\n",
      "| 16 | num17          |       0.0221 |   0.7834 |           -0.0034 |            0.0012 |    0.0098 |    0.0063 |    0.0000 |    0.0000 |\n",
      "| 31 | num32          |       0.0221 |   0.2455 |            0.0026 |           -0.0087 |    0.0050 |    0.0110 |    0.0000 |    0.0000 |\n",
      "| 34 | num35          |       0.0221 |   0.5467 |            0.0010 |           -0.0047 |    0.0058 |    0.0085 |    0.0000 |    0.0000 |\n",
      "| 39 | num40          |       0.0238 |   6.6850 |            0.0161 |           -0.0084 |    0.0186 |    0.0107 |    0.0000 |    0.0000 |\n",
      "| 19 | num20          |       0.0238 |   0.7161 |           -0.0001 |            0.0017 |    0.0045 |    0.0047 |    0.0000 |    0.0000 |\n",
      "| 38 | num39          |       0.0255 |   5.5331 |           -0.0313 |            0.0015 |    0.0334 |    0.0053 |    0.0000 |    0.0000 |\n",
      "| 47 | num48          |       0.0255 |  74.4318 |           -0.0028 |            0.0056 |    0.0049 |    0.0066 |    0.0000 |    0.0000 |\n",
      "|  5 | num6           |       0.0272 |   0.0135 |            0.0043 |           -0.0036 |    0.0069 |    0.0061 |    0.0000 |    0.0000 |\n",
      "|  6 | num7           |       0.0272 |   0.0182 |            0.0103 |           -0.0054 |    0.0109 |    0.0057 |    0.0000 |    0.0000 |\n",
      "| 18 | num19          |       0.0289 |   0.7109 |           -0.0037 |            0.0012 |    0.0088 |    0.0041 |    0.0000 |    0.0000 |\n",
      "| 26 | num27          |       0.0289 |   5.5294 |           -0.1345 |            0.0135 |    0.1362 |    0.0152 |    0.0000 |    0.0000 |\n",
      "|  4 | num5           |       0.0289 |   0.0079 |            0.0000 |           -0.0012 |    0.0071 |    0.0034 |    0.0000 |    0.0000 |\n",
      "| 57 | num58          |       0.0306 |   0.7091 |           -0.0005 |           -0.0013 |    0.0149 |    0.0081 |    0.0000 |    0.0000 |\n",
      "| 20 | num21          |       0.0306 |   0.7091 |            0.0020 |           -0.0002 |    0.0041 |    0.0027 |    0.0000 |    0.0000 |\n",
      "|  3 | num4           |       0.0323 |   0.0058 |            0.0005 |           -0.0003 |    0.0015 |    0.0012 |    0.0000 |    0.0000 |\n",
      "|  1 | num2           |       0.0357 |   4.0834 |           -0.0025 |           -0.0104 |    0.0073 |    0.0190 |    0.0000 |    0.0000 |\n",
      "| 21 | num22          |       0.0374 |   0.7174 |            0.0077 |           -0.0051 |    0.0128 |    0.0117 |    0.0051 |    0.0051 |\n",
      "| 15 | num16          |       0.0459 |   0.7858 |           -0.0073 |            0.0062 |    0.0095 |    0.0076 |    0.0000 |    0.0000 |\n",
      "|  2 | num3           |       0.0544 |   0.6974 |            0.0042 |           -0.0060 |    0.0178 |    0.0128 |    0.0000 |    0.0000 |\n",
      "|  0 | num1           |       0.0867 | 499.5119 |            0.0024 |           -0.0056 |    0.0046 |    0.0092 |    0.0000 |    0.0000 |\n",
      "+----+----------------+--------------+----------+-------------------+-------------------+-----------+-----------+-----------+-----------+\n",
      "\n",
      "\n",
      "*******\n",
      "Legend:\n",
      "Importance = Feature Importance\n",
      "     Explanation: A weighted measure of how much of the variance the model is able to explain is due to this column\n",
      "FR_delta = Feature Response Delta Amount\n",
      "     Explanation: Amount this column was incremented or decremented by to calculate the feature reponses\n",
      "FR_Decrementing = Feature Response From Decrementing Values In This Column By One FR_delta\n",
      "     Explanation: Represents how much the predicted output values respond to subtracting one FR_delta amount from every value in this column\n",
      "FR_Incrementing = Feature Response From Incrementing Values In This Column By One FR_delta\n",
      "     Explanation: Represents how much the predicted output values respond to adding one FR_delta amount to every value in this column\n",
      "FRD_MAD = Feature Response From Decrementing- Median Absolute Delta\n",
      "     Explanation: Takes the absolute value of all changes in predictions, then takes the median of those. Useful for seeing if decrementing this feature provokes strong changes that are both positive and negative\n",
      "FRI_MAD = Feature Response From Incrementing- Median Absolute Delta\n",
      "     Explanation: Takes the absolute value of all changes in predictions, then takes the median of those. Useful for seeing if incrementing this feature provokes strong changes that are both positive and negative\n",
      "FRD_abs = Feature Response From Decrementing Avg Absolute Change\n",
      "     Explanation: What is the average absolute change in predicted output values to subtracting one FR_delta amount to every value in this column. Useful for seeing if output is sensitive to a feature, but not in a uniformly positive or negative way\n",
      "FRI_abs = Feature Response From Incrementing Avg Absolute Change\n",
      "     Explanation: What is the average absolute change in predicted output values to adding one FR_delta amount to every value in this column. Useful for seeing if output is sensitive to a feature, but not in a uniformly positive or negative way\n",
      "*******\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<auto_ml.predictor.Predictor at 0x1a240c6668>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_predictor.train(df2_train, model_names='XGBRegressor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scores - 0.9689582737977173\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "\n",
      "\n",
      "***********************************************\n",
      "Advanced scoring metrics for the trained regression model on this particular dataset:\n",
      "\n",
      "Here is the overall RMSE for these predictions:\n",
      "1.4158195683394312\n",
      "\n",
      "Here is the average of the predictions:\n",
      "20.03953146154638\n",
      "\n",
      "Here is the average actual value on this validation set:\n",
      "20.024082777165344\n",
      "\n",
      "Here is the median prediction:\n",
      "20.03954369952779\n",
      "\n",
      "Here is the median actual value:\n",
      "20.040008\n",
      "\n",
      "Here is the mean absolute error:\n",
      "0.9689582737977173\n",
      "\n",
      "Here is the median absolute error (robust to outliers):\n",
      "0.61670680047221\n",
      "\n",
      "Here is the explained variance:\n",
      "-0.0038093832742340794\n",
      "\n",
      "Here is the R-squared value:\n",
      "-0.0039289114087390775\n",
      "Count of positive differences (prediction > actual):\n",
      "632\n",
      "Count of negative differences:\n",
      "638\n",
      "Average positive difference:\n",
      "0.9890797760182745\n",
      "Average negative difference:\n",
      "-0.9490260020055667\n",
      "\n",
      "\n",
      "***********************************************\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Score the model on test data\n",
    "test_score = ml_predictor.score(df2_test, df2_test.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using original categorical variables instead of dummy vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training and testing data\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=303)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = df.filter(regex=(\"cat*\")).columns\n",
    "column_descriptions = {'target': 'output'}\n",
    "for col in cat_cols:\n",
    "    column_descriptions[col] = 'categorical'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_predictor = Predictor(type_of_estimator='regressor', column_descriptions=column_descriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to auto_ml! We're about to go through and make sense of your data using machine learning, and give you a production-ready pipeline to get predictions with.\n",
      "\n",
      "If you have any issues, or new feature ideas, let us know at http://auto.ml\n",
      "You are running on version 2.9.9\n",
      "Now using the model training_params that you passed in:\n",
      "{}\n",
      "After overwriting our defaults with your values, here are the final params that will be used to initialize the model:\n",
      "{'learning_rate': 0.1, 'presort': False, 'warm_start': True}\n",
      "Running basic data cleaning\n",
      "Fitting DataFrameVectorizer\n",
      "Now using the model training_params that you passed in:\n",
      "{}\n",
      "After overwriting our defaults with your values, here are the final params that will be used to initialize the model:\n",
      "{'learning_rate': 0.1, 'presort': False, 'warm_start': True}\n",
      "\n",
      "\n",
      "********************************************************************************************\n",
      "About to fit the pipeline for the model GradientBoostingRegressor to predict target\n",
      "Started at:\n",
      "2018-11-14 20:29:02\n",
      "[1] random_holdout_set_from_training_data's score is: -1.438\n",
      "[2] random_holdout_set_from_training_data's score is: -1.438\n",
      "[3] random_holdout_set_from_training_data's score is: -1.438\n",
      "[4] random_holdout_set_from_training_data's score is: -1.438\n",
      "[5] random_holdout_set_from_training_data's score is: -1.439\n",
      "[6] random_holdout_set_from_training_data's score is: -1.439\n",
      "[7] random_holdout_set_from_training_data's score is: -1.434\n",
      "[8] random_holdout_set_from_training_data's score is: -1.433\n",
      "[9] random_holdout_set_from_training_data's score is: -1.433\n",
      "[10] random_holdout_set_from_training_data's score is: -1.431\n",
      "[11] random_holdout_set_from_training_data's score is: -1.434\n",
      "[12] random_holdout_set_from_training_data's score is: -1.434\n",
      "[13] random_holdout_set_from_training_data's score is: -1.435\n",
      "[14] random_holdout_set_from_training_data's score is: -1.435\n",
      "[15] random_holdout_set_from_training_data's score is: -1.433\n",
      "[16] random_holdout_set_from_training_data's score is: -1.432\n",
      "[17] random_holdout_set_from_training_data's score is: -1.434\n",
      "[18] random_holdout_set_from_training_data's score is: -1.434\n",
      "[19] random_holdout_set_from_training_data's score is: -1.434\n",
      "[20] random_holdout_set_from_training_data's score is: -1.431\n",
      "[21] random_holdout_set_from_training_data's score is: -1.432\n",
      "[22] random_holdout_set_from_training_data's score is: -1.432\n",
      "[23] random_holdout_set_from_training_data's score is: -1.435\n",
      "[24] random_holdout_set_from_training_data's score is: -1.435\n",
      "[25] random_holdout_set_from_training_data's score is: -1.435\n",
      "[26] random_holdout_set_from_training_data's score is: -1.435\n",
      "[27] random_holdout_set_from_training_data's score is: -1.433\n",
      "[28] random_holdout_set_from_training_data's score is: -1.434\n",
      "[29] random_holdout_set_from_training_data's score is: -1.435\n",
      "[30] random_holdout_set_from_training_data's score is: -1.434\n",
      "The number of estimators that were the best for this training dataset: 10\n",
      "The best score on the holdout set: -1.4314094680267997\n",
      "Finished training the pipeline!\n",
      "Total training time:\n",
      "0:00:03\n",
      "\n",
      "\n",
      "Here are the results from our GradientBoostingRegressor\n",
      "predicting target\n",
      "Calculating feature responses, for advanced analytics.\n",
      "The printed list will only contain at most the top 100 features.\n",
      "+----+----------------+--------------+----------+-------------------+-------------------+-----------+-----------+-----------+-----------+\n",
      "|    | Feature Name   |   Importance |    Delta |   FR_Decrementing |   FR_Incrementing |   FRD_abs |   FRI_abs |   FRD_MAD |   FRI_MAD |\n",
      "|----+----------------+--------------+----------+-------------------+-------------------+-----------+-----------+-----------+-----------|\n",
      "| 49 | num50          |       0.0000 |  76.7571 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 74 | cat21=False    |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 73 | cat20=True     |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 72 | cat20=False    |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 71 | cat2=H         |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 70 | cat2=L         |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 69 | cat2=I         |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 68 | cat2=A         |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 67 | cat2=K         |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 66 | cat2=C         |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 65 | cat2=F         |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 62 | cat1=A         |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 61 | cat1=E         |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 60 | cat1=C         |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 59 | cat1=D         |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 58 | num59          |       0.0000 | 919.8211 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 56 | num57          |       0.0000 |  76.7571 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 54 | num55          |       0.0000 |  74.4318 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 53 | num54          |       0.0000 |  76.7681 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 98 | cat9=False     |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 75 | cat21=True     |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 48 | num49          |       0.0000 |   2.2500 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 76 | cat22=False    |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 78 | cat23=False    |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 97 | cat8=True      |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 96 | cat8=False     |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 95 | cat7=True      |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 94 | cat7=False     |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 93 | cat6=True      |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 92 | cat6=False     |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 91 | cat5=True      |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 90 | cat5=False     |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 89 | cat4=True      |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 88 | cat4=False     |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 87 | cat3=True      |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 86 | cat3=False     |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 85 | cat26=True     |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 84 | cat26=False    |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 83 | cat25=True     |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 82 | cat25=False    |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 81 | cat24=True     |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 80 | cat24=False    |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 79 | cat23=True     |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 77 | cat22=True     |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 46 | num47          |       0.0000 |  76.7681 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 99 | cat9=True      |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 26 | num27          |       0.0000 |   5.5294 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "|  7 | num8           |       0.0000 |   4.0887 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 10 | num11          |       0.0000 |   4.0621 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 13 | num14          |       0.0000 |   0.0569 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 14 | num15          |       0.0000 |   0.7977 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 17 | num18          |       0.0000 |   0.7165 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 18 | num19          |       0.0000 |   0.7109 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 21 | num22          |       0.0000 |   0.7174 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 24 | num25          |       0.0000 |   7.9282 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 25 | num26          |       0.0000 |   8.6364 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 45 | num46          |       0.0000 |   2.0393 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 28 | num29          |       0.0000 |   6.7246 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 31 | num32          |       0.0000 |   0.2455 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 32 | num33          |       0.0000 |  26.6477 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 33 | num34          |       0.0000 |  26.5347 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 29 | num30          |       0.0000 |  19.5508 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 42 | num43          |       0.0000 |   0.5782 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 35 | num36          |       0.0000 |   0.5135 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 40 | num41          |       0.0000 |   0.5467 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 38 | num39          |       0.0000 |   5.5331 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "|  2 | num3           |       0.0000 |   0.6974 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 39 | num40          |       0.0000 |   6.6850 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 30 | num31          |       0.0000 |   0.5502 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 23 | num24          |       0.0000 |   4.1844 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "|  1 | num2           |       0.0000 |   4.0834 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 11 | num12          |       0.0005 |   0.0345 |           -0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 41 | num42          |       0.0007 |  18.3723 |            0.0000 |           -0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 36 | num37          |       0.0008 |  18.3793 |            0.0001 |           -0.0001 |    0.0001 |    0.0001 |    0.0000 |    0.0000 |\n",
      "| 37 | num38          |       0.0009 |  17.9140 |           -0.0001 |            0.0001 |    0.0001 |    0.0001 |    0.0000 |    0.0000 |\n",
      "| 19 | num20          |       0.0021 |   0.7161 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 55 | num56          |       0.0044 |   2.2500 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "|  6 | num7           |       0.0046 |   0.0182 |           -0.0002 |            0.0001 |    0.0002 |    0.0001 |    0.0000 |    0.0000 |\n",
      "|  8 | num9           |       0.0049 |   4.0730 |            0.0001 |           -0.0001 |    0.0001 |    0.0001 |    0.0000 |    0.0000 |\n",
      "| 52 | num53          |       0.0059 |   2.0393 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 63 | cat11=False    |       0.0128 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 43 | num44          |       0.0132 |   4.2226 |            0.0002 |           -0.0004 |    0.0002 |    0.0004 |    0.0000 |    0.0000 |\n",
      "| 16 | num17          |       0.0134 |   0.7834 |           -0.0001 |            0.0001 |    0.0001 |    0.0001 |    0.0000 |    0.0000 |\n",
      "| 64 | cat11=True     |       0.0135 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 22 | num23          |       0.0173 |   2.2769 |            0.0002 |           -0.0002 |    0.0002 |    0.0002 |    0.0000 |    0.0000 |\n",
      "|  5 | num6           |       0.0174 |   0.0135 |            0.0003 |           -0.0002 |    0.0003 |    0.0002 |    0.0000 |    0.0000 |\n",
      "| 20 | num21          |       0.0186 |   0.7091 |           -0.0001 |            0.0003 |    0.0001 |    0.0003 |    0.0000 |    0.0000 |\n",
      "| 12 | num13          |       0.0198 |   0.0475 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "|  3 | num4           |       0.0200 |   0.0058 |           -0.0001 |            0.0001 |    0.0001 |    0.0001 |    0.0000 |    0.0000 |\n",
      "|  9 | num10          |       0.0221 |   4.0758 |           -0.0003 |            0.0002 |    0.0004 |    0.0004 |    0.0000 |    0.0000 |\n",
      "|  4 | num5           |       0.0222 |   0.0079 |            0.0007 |           -0.0005 |    0.0007 |    0.0005 |    0.0000 |    0.0000 |\n",
      "| 50 | num51          |       0.0232 |   4.2226 |            0.0003 |           -0.0007 |    0.0003 |    0.0007 |    0.0000 |    0.0000 |\n",
      "| 27 | num28          |       0.0383 |   5.5592 |            0.0005 |           -0.0005 |    0.0006 |    0.0006 |    0.0000 |    0.0000 |\n",
      "| 47 | num48          |       0.0547 |  74.4318 |            0.0000 |            0.0007 |    0.0006 |    0.0013 |    0.0000 |    0.0000 |\n",
      "| 44 | num45          |       0.0607 |   2.2513 |           -0.0004 |            0.0011 |    0.0007 |    0.0016 |    0.0000 |    0.0000 |\n",
      "| 15 | num16          |       0.0675 |   0.7858 |           -0.0011 |            0.0012 |    0.0011 |    0.0012 |    0.0000 |    0.0000 |\n",
      "| 34 | num35          |       0.0799 |   0.5467 |            0.0041 |           -0.0010 |    0.0041 |    0.0010 |    0.0000 |    0.0000 |\n",
      "| 51 | num52          |       0.0833 |   2.2513 |           -0.0010 |            0.0026 |    0.0013 |    0.0030 |    0.0000 |    0.0000 |\n",
      "|  0 | num1           |       0.1761 | 499.5119 |           -0.0000 |            0.0022 |    0.0015 |    0.0035 |    0.0000 |    0.0000 |\n",
      "| 57 | num58          |       0.2013 |   0.7091 |            0.0012 |           -0.0006 |    0.0012 |    0.0006 |    0.0000 |    0.0000 |\n",
      "+----+----------------+--------------+----------+-------------------+-------------------+-----------+-----------+-----------+-----------+\n",
      "\n",
      "\n",
      "*******\n",
      "Legend:\n",
      "Importance = Feature Importance\n",
      "     Explanation: A weighted measure of how much of the variance the model is able to explain is due to this column\n",
      "FR_delta = Feature Response Delta Amount\n",
      "     Explanation: Amount this column was incremented or decremented by to calculate the feature reponses\n",
      "FR_Decrementing = Feature Response From Decrementing Values In This Column By One FR_delta\n",
      "     Explanation: Represents how much the predicted output values respond to subtracting one FR_delta amount from every value in this column\n",
      "FR_Incrementing = Feature Response From Incrementing Values In This Column By One FR_delta\n",
      "     Explanation: Represents how much the predicted output values respond to adding one FR_delta amount to every value in this column\n",
      "FRD_MAD = Feature Response From Decrementing- Median Absolute Delta\n",
      "     Explanation: Takes the absolute value of all changes in predictions, then takes the median of those. Useful for seeing if decrementing this feature provokes strong changes that are both positive and negative\n",
      "FRI_MAD = Feature Response From Incrementing- Median Absolute Delta\n",
      "     Explanation: Takes the absolute value of all changes in predictions, then takes the median of those. Useful for seeing if incrementing this feature provokes strong changes that are both positive and negative\n",
      "FRD_abs = Feature Response From Decrementing Avg Absolute Change\n",
      "     Explanation: What is the average absolute change in predicted output values to subtracting one FR_delta amount to every value in this column. Useful for seeing if output is sensitive to a feature, but not in a uniformly positive or negative way\n",
      "FRI_abs = Feature Response From Incrementing Avg Absolute Change\n",
      "     Explanation: What is the average absolute change in predicted output values to adding one FR_delta amount to every value in this column. Useful for seeing if output is sensitive to a feature, but not in a uniformly positive or negative way\n",
      "*******\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<auto_ml.predictor.Predictor at 0x1a240c6668>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_predictor.train(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scores - 0.9689582737977173"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "\n",
      "\n",
      "***********************************************\n",
      "Advanced scoring metrics for the trained regression model on this particular dataset:\n",
      "\n",
      "Here is the overall RMSE for these predictions:\n",
      "1.4158195683394312\n",
      "\n",
      "Here is the average of the predictions:\n",
      "20.03953146154638\n",
      "\n",
      "Here is the average actual value on this validation set:\n",
      "20.024082777165344\n",
      "\n",
      "Here is the median prediction:\n",
      "20.03954369952779\n",
      "\n",
      "Here is the median actual value:\n",
      "20.040008\n",
      "\n",
      "Here is the mean absolute error:\n",
      "0.9689582737977173\n",
      "\n",
      "Here is the median absolute error (robust to outliers):\n",
      "0.61670680047221\n",
      "\n",
      "Here is the explained variance:\n",
      "-0.0038093832742340794\n",
      "\n",
      "Here is the R-squared value:\n",
      "-0.0039289114087390775\n",
      "Count of positive differences (prediction > actual):\n",
      "632\n",
      "Count of negative differences:\n",
      "638\n",
      "Average positive difference:\n",
      "0.9890797760182745\n",
      "Average negative difference:\n",
      "-0.9490260020055667\n",
      "\n",
      "\n",
      "***********************************************\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Score the model on test data\n",
    "test_score = ml_predictor.score(df_test, df_test.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARDRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to auto_ml! We're about to go through and make sense of your data using machine learning, and give you a production-ready pipeline to get predictions with.\n",
      "\n",
      "If you have any issues, or new feature ideas, let us know at http://auto.ml\n",
      "You are running on version 2.9.9\n",
      "Now using the model training_params that you passed in:\n",
      "{}\n",
      "After overwriting our defaults with your values, here are the final params that will be used to initialize the model:\n",
      "{}\n",
      "Running basic data cleaning\n",
      "Performing feature scaling\n",
      "Fitting DataFrameVectorizer\n",
      "Now using the model training_params that you passed in:\n",
      "{}\n",
      "After overwriting our defaults with your values, here are the final params that will be used to initialize the model:\n",
      "{}\n",
      "\n",
      "\n",
      "********************************************************************************************\n",
      "About to fit the pipeline for the model ARDRegression to predict target\n",
      "Started at:\n",
      "2018-11-14 20:46:44\n",
      "Finished training the pipeline!\n",
      "Total training time:\n",
      "0:13:22\n",
      "Calculating feature responses, for advanced analytics.\n",
      "Here are our feature responses for the trained model\n",
      "+----+----------------+----------+-------------------+-------------------+-----------+-----------+\n",
      "|    | Feature Name   |    Delta |   FR_Decrementing |   FR_Incrementing |   FRD_MAD |   FRI_MAD |\n",
      "|----+----------------+----------+-------------------+-------------------+-----------+-----------|\n",
      "| 58 | num51          |   0.1556 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |\n",
      "| 57 | num4           |   0.1656 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |\n",
      "| 56 | num10          |   0.1758 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |\n",
      "| 55 | num20          |   0.1581 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |\n",
      "| 54 | num29          |   0.1511 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |\n",
      "| 53 | num21          |   0.1583 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |\n",
      "| 52 | num19          |   0.1573 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |\n",
      "| 51 | num31          |   0.5502 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |\n",
      "| 50 | num17          |   0.1809 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |\n",
      "| 49 | num33          |   0.1402 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |\n",
      "| 48 | num37          |   0.1657 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |\n",
      "| 47 | num44          |   0.1556 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |\n",
      "| 46 | num48          |   0.1637 |           -0.0000 |            0.0000 |    0.0000 |    0.0000 |\n",
      "| 45 | num55          |   0.1637 |           -0.0000 |            0.0000 |    0.0000 |    0.0000 |\n",
      "| 44 | num43          |   0.1580 |           -0.0000 |            0.0000 |    0.0000 |    0.0000 |\n",
      "| 43 | num34          |   0.1402 |            0.0000 |           -0.0000 |    0.0000 |    0.0000 |\n",
      "| 42 | num25          |   0.1601 |            0.0000 |           -0.0000 |    0.0000 |    0.0000 |\n",
      "| 41 | num5           |   0.1617 |            0.0000 |           -0.0000 |    0.0000 |    0.0000 |\n",
      "| 40 | num26          |   0.1557 |           -0.0001 |            0.0001 |    0.0001 |    0.0001 |\n",
      "| 39 | num28          |   0.1526 |            0.0001 |           -0.0001 |    0.0001 |    0.0001 |\n",
      "| 38 | num11          |   0.1769 |            0.0001 |           -0.0001 |    0.0001 |    0.0001 |\n",
      "| 37 | num36          |   0.1661 |            0.0001 |           -0.0001 |    0.0001 |    0.0001 |\n",
      "| 36 | num41          |   0.1685 |            0.0001 |           -0.0001 |    0.0001 |    0.0001 |\n",
      "| 35 | num57          |   0.1648 |            0.0001 |           -0.0001 |    0.0001 |    0.0001 |\n",
      "| 34 | num50          |   0.1648 |            0.0001 |           -0.0001 |    0.0001 |    0.0001 |\n",
      "| 33 | num56          |   0.1677 |           -0.0002 |            0.0002 |    0.0002 |    0.0002 |\n",
      "| 32 | num49          |   0.1677 |           -0.0002 |            0.0002 |    0.0002 |    0.0002 |\n",
      "| 31 | num42          |   0.1646 |            0.0002 |           -0.0002 |    0.0002 |    0.0002 |\n",
      "| 30 | num30          |   0.1661 |           -0.0002 |            0.0002 |    0.0002 |    0.0002 |\n",
      "| 29 | num35          |   0.1693 |            0.0002 |           -0.0002 |    0.0002 |    0.0002 |\n",
      "| 28 | num45          |   0.1679 |           -0.0002 |            0.0002 |    0.0002 |    0.0002 |\n",
      "| 27 | num52          |   0.1679 |           -0.0002 |            0.0002 |    0.0002 |    0.0002 |\n",
      "| 26 | num27          |   0.1566 |           -0.0003 |            0.0003 |    0.0003 |    0.0003 |\n",
      "| 25 | num39          |   0.1569 |           -0.0003 |            0.0003 |    0.0003 |    0.0003 |\n",
      "| 24 | num3           |   0.1684 |            0.0003 |           -0.0003 |    0.0003 |    0.0003 |\n",
      "| 23 | num9           |   0.1776 |           -0.0003 |            0.0003 |    0.0003 |    0.0003 |\n",
      "| 22 | num38          |   0.1641 |            0.0003 |           -0.0003 |    0.0003 |    0.0003 |\n",
      "| 21 | num18          |   0.1599 |            0.0003 |           -0.0003 |    0.0003 |    0.0003 |\n",
      "| 20 | num24          |   0.1504 |           -0.0003 |            0.0003 |    0.0003 |    0.0003 |\n",
      "| 19 | num40          |   0.1519 |            0.0003 |           -0.0003 |    0.0003 |    0.0003 |\n",
      "| 18 | num8           |   0.1787 |           -0.0004 |            0.0004 |    0.0004 |    0.0004 |\n",
      "| 17 | num6           |   0.1632 |            0.0068 |           -0.0068 |    0.0068 |    0.0068 |\n",
      "| 16 | num14          |   0.1694 |            0.0208 |           -0.0208 |    0.0208 |    0.0208 |\n",
      "| 15 | num22          |   0.1587 |            0.0279 |           -0.0279 |    0.0279 |    0.0279 |\n",
      "| 14 | num7           |   0.1624 |           -0.0326 |            0.0326 |    0.0326 |    0.0326 |\n",
      "| 13 | num58          |   0.1638 |            0.0327 |           -0.0327 |    0.0327 |    0.0327 |\n",
      "| 12 | num32          |   0.1497 |            0.0354 |           -0.0354 |    0.0354 |    0.0354 |\n",
      "| 11 | num12          |   0.1673 |           -0.0379 |            0.0379 |    0.0379 |    0.0379 |\n",
      "| 10 | num1           |   0.1799 |            0.0389 |           -0.0389 |    0.0389 |    0.0389 |\n",
      "|  9 | num2           |   0.1778 |           -0.0392 |            0.0392 |    0.0392 |    0.0392 |\n",
      "|  8 | num23          |   0.1598 |            0.0458 |           -0.0458 |    0.0458 |    0.0458 |\n",
      "|  7 | num59          |   0.1609 |           -0.0501 |            0.0501 |    0.0501 |    0.0501 |\n",
      "|  6 | num15          |   0.1817 |           -0.0558 |            0.0558 |    0.0558 |    0.0558 |\n",
      "|  5 | num13          |   0.1689 |            0.0788 |           -0.0788 |    0.0788 |    0.0788 |\n",
      "|  4 | num16          |   0.1815 |           -0.0820 |            0.0820 |    0.0820 |    0.0820 |\n",
      "|  3 | num46          |   0.1641 |           -0.0869 |            0.0869 |    0.0869 |    0.0869 |\n",
      "|  2 | num53          |   0.1641 |           -0.0869 |            0.0869 |    0.0869 |    0.0869 |\n",
      "|  1 | num47          |   0.1636 |            0.0928 |           -0.0928 |    0.0928 |    0.0928 |\n",
      "|  0 | num54          |   0.1636 |            0.0928 |           -0.0928 |    0.0928 |    0.0928 |\n",
      "| 59 | cat1=D         | nan      |          nan      |          nan      |  nan      |  nan      |\n",
      "| 60 | cat1=C         | nan      |          nan      |          nan      |  nan      |  nan      |\n",
      "| 61 | cat1=E         | nan      |          nan      |          nan      |  nan      |  nan      |\n",
      "| 62 | cat1=A         | nan      |          nan      |          nan      |  nan      |  nan      |\n",
      "| 63 | cat1=B         | nan      |          nan      |          nan      |  nan      |  nan      |\n",
      "| 64 | cat10=False    | nan      |          nan      |          nan      |  nan      |  nan      |\n",
      "| 65 | cat10=True     | nan      |          nan      |          nan      |  nan      |  nan      |\n",
      "| 66 | cat11=False    | nan      |          nan      |          nan      |  nan      |  nan      |\n",
      "| 67 | cat11=True     | nan      |          nan      |          nan      |  nan      |  nan      |\n",
      "| 68 | cat12=False    | nan      |          nan      |          nan      |  nan      |  nan      |\n",
      "| 69 | cat12=True     | nan      |          nan      |          nan      |  nan      |  nan      |\n",
      "| 70 | cat13=False    | nan      |          nan      |          nan      |  nan      |  nan      |\n",
      "| 71 | cat13=True     | nan      |          nan      |          nan      |  nan      |  nan      |\n",
      "| 72 | cat14=False    | nan      |          nan      |          nan      |  nan      |  nan      |\n",
      "| 73 | cat14=True     | nan      |          nan      |          nan      |  nan      |  nan      |\n",
      "| 74 | cat15=False    | nan      |          nan      |          nan      |  nan      |  nan      |\n",
      "| 75 | cat15=True     | nan      |          nan      |          nan      |  nan      |  nan      |\n",
      "| 76 | cat16=False    | nan      |          nan      |          nan      |  nan      |  nan      |\n",
      "| 77 | cat16=True     | nan      |          nan      |          nan      |  nan      |  nan      |\n",
      "| 78 | cat17=False    | nan      |          nan      |          nan      |  nan      |  nan      |\n",
      "| 79 | cat17=True     | nan      |          nan      |          nan      |  nan      |  nan      |\n",
      "| 80 | cat18=False    | nan      |          nan      |          nan      |  nan      |  nan      |\n",
      "| 81 | cat18=True     | nan      |          nan      |          nan      |  nan      |  nan      |\n",
      "| 82 | cat19=False    | nan      |          nan      |          nan      |  nan      |  nan      |\n",
      "| 83 | cat19=True     | nan      |          nan      |          nan      |  nan      |  nan      |\n",
      "| 84 | cat2=E         | nan      |          nan      |          nan      |  nan      |  nan      |\n",
      "| 85 | cat2=G         | nan      |          nan      |          nan      |  nan      |  nan      |\n",
      "| 86 | cat2=B         | nan      |          nan      |          nan      |  nan      |  nan      |\n",
      "| 87 | cat2=D         | nan      |          nan      |          nan      |  nan      |  nan      |\n",
      "| 88 | cat2=F         | nan      |          nan      |          nan      |  nan      |  nan      |\n",
      "| 89 | cat2=C         | nan      |          nan      |          nan      |  nan      |  nan      |\n",
      "| 90 | cat2=K         | nan      |          nan      |          nan      |  nan      |  nan      |\n",
      "| 91 | cat2=A         | nan      |          nan      |          nan      |  nan      |  nan      |\n",
      "| 92 | cat2=I         | nan      |          nan      |          nan      |  nan      |  nan      |\n",
      "| 93 | cat2=L         | nan      |          nan      |          nan      |  nan      |  nan      |\n",
      "| 94 | cat2=J         | nan      |          nan      |          nan      |  nan      |  nan      |\n",
      "| 95 | cat2=H         | nan      |          nan      |          nan      |  nan      |  nan      |\n",
      "| 96 | cat20=False    | nan      |          nan      |          nan      |  nan      |  nan      |\n",
      "| 97 | cat20=True     | nan      |          nan      |          nan      |  nan      |  nan      |\n",
      "| 98 | cat21=False    | nan      |          nan      |          nan      |  nan      |  nan      |\n",
      "| 99 | cat21=True     | nan      |          nan      |          nan      |  nan      |  nan      |\n",
      "+----+----------------+----------+-------------------+-------------------+-----------+-----------+\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<auto_ml.predictor.Predictor at 0x1a240c6668>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_predictor.train(df_train, model_names='ARDRegression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scores - 0.9698217994619233"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "\n",
      "\n",
      "***********************************************\n",
      "Advanced scoring metrics for the trained regression model on this particular dataset:\n",
      "\n",
      "Here is the overall RMSE for these predictions:\n",
      "1.41120580457569\n",
      "\n",
      "Here is the average of the predictions:\n",
      "20.036850714239204\n",
      "\n",
      "Here is the average actual value on this validation set:\n",
      "20.024082777165344\n",
      "\n",
      "Here is the median prediction:\n",
      "20.037035500946345\n",
      "\n",
      "Here is the median actual value:\n",
      "20.040008\n",
      "\n",
      "Here is the mean absolute error:\n",
      "0.9698217994619233\n",
      "\n",
      "Here is the median absolute error (robust to outliers):\n",
      "0.6385089492367442\n",
      "\n",
      "Here is the explained variance:\n",
      "0.0026851248372815606\n",
      "\n",
      "Here is the R-squared value:\n",
      "0.0026034800226117927\n",
      "Count of positive differences (prediction > actual):\n",
      "630\n",
      "Count of negative differences:\n",
      "640\n",
      "Average positive difference:\n",
      "0.9903880677780889\n",
      "Average negative difference:\n",
      "-0.9495768790881967\n",
      "\n",
      "\n",
      "***********************************************\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Score the model on test data\n",
    "test_score = ml_predictor.score(df_test, df_test.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomForestRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to auto_ml! We're about to go through and make sense of your data using machine learning, and give you a production-ready pipeline to get predictions with.\n",
      "\n",
      "If you have any issues, or new feature ideas, let us know at http://auto.ml\n",
      "You are running on version 2.9.9\n",
      "Now using the model training_params that you passed in:\n",
      "{}\n",
      "After overwriting our defaults with your values, here are the final params that will be used to initialize the model:\n",
      "{'n_estimators': 30, 'n_jobs': -2}\n",
      "Running basic data cleaning\n",
      "Fitting DataFrameVectorizer\n",
      "Now using the model training_params that you passed in:\n",
      "{}\n",
      "After overwriting our defaults with your values, here are the final params that will be used to initialize the model:\n",
      "{'n_estimators': 30, 'n_jobs': -2}\n",
      "\n",
      "\n",
      "********************************************************************************************\n",
      "About to fit the pipeline for the model RandomForestRegressor to predict target\n",
      "Started at:\n",
      "2018-11-14 21:07:38\n",
      "Finished training the pipeline!\n",
      "Total training time:\n",
      "0:00:11\n",
      "\n",
      "\n",
      "Here are the results from our RandomForestRegressor\n",
      "predicting target\n",
      "Calculating feature responses, for advanced analytics.\n",
      "The printed list will only contain at most the top 100 features.\n",
      "+----+----------------+--------------+----------+-------------------+-------------------+-----------+-----------+-----------+-----------+\n",
      "|    | Feature Name   |   Importance |    Delta |   FR_Decrementing |   FR_Incrementing |   FRD_abs |   FRI_abs |   FRD_MAD |   FRI_MAD |\n",
      "|----+----------------+--------------+----------+-------------------+-------------------+-----------+-----------+-----------+-----------|\n",
      "| 95 | cat6=False     |       0.0003 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 91 | cat25=True     |       0.0003 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 66 | cat13=False    |       0.0003 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 90 | cat25=False    |       0.0003 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 93 | cat5=False     |       0.0004 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 92 | cat4=False     |       0.0004 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 69 | cat18=True     |       0.0004 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 96 | cat7=False     |       0.0004 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 64 | cat11=True     |       0.0004 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 88 | cat22=False    |       0.0004 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 85 | cat20=True     |       0.0004 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 89 | cat22=True     |       0.0005 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 65 | cat12=True     |       0.0005 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 71 | cat19=True     |       0.0005 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 84 | cat20=False    |       0.0005 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 68 | cat17=False    |       0.0006 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 94 | cat5=True      |       0.0006 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 70 | cat19=False    |       0.0007 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 73 | cat2=G         |       0.0007 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 67 | cat13=True     |       0.0007 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 99 | cat9=False     |       0.0007 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 75 | cat2=D         |       0.0008 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 97 | cat8=False     |       0.0008 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 81 | cat2=L         |       0.0009 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 83 | cat2=H         |       0.0010 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 72 | cat2=E         |       0.0010 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 98 | cat8=True      |       0.0011 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 74 | cat2=B         |       0.0011 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 86 | cat21=False    |       0.0012 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 62 | cat1=A         |       0.0012 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 87 | cat21=True     |       0.0012 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 82 | cat2=J         |       0.0013 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 78 | cat2=K         |       0.0014 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 61 | cat1=E         |       0.0014 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 76 | cat2=F         |       0.0015 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 77 | cat2=C         |       0.0016 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 60 | cat1=C         |       0.0018 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 80 | cat2=I         |       0.0020 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 79 | cat2=A         |       0.0020 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 59 | cat1=D         |       0.0021 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 63 | cat1=B         |       0.0022 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 52 | num53          |       0.0046 |   2.0393 |           -0.0010 |            0.0003 |    0.0140 |    0.0149 |    0.0000 |    0.0000 |\n",
      "| 56 | num57          |       0.0049 |  76.7571 |           -0.0051 |            0.0006 |    0.0203 |    0.0155 |    0.0000 |    0.0000 |\n",
      "| 46 | num47          |       0.0051 |  76.7681 |            0.0034 |           -0.0026 |    0.0148 |    0.0149 |    0.0000 |    0.0000 |\n",
      "| 55 | num56          |       0.0056 |   2.2500 |            0.0015 |           -0.0021 |    0.0162 |    0.0166 |    0.0000 |    0.0003 |\n",
      "| 49 | num50          |       0.0058 |  76.7571 |            0.0010 |           -0.0015 |    0.0201 |    0.0190 |    0.0001 |    0.0000 |\n",
      "| 45 | num46          |       0.0064 |   2.0393 |           -0.0037 |           -0.0011 |    0.0184 |    0.0199 |    0.0000 |    0.0000 |\n",
      "| 53 | num54          |       0.0065 |  76.7681 |            0.0006 |           -0.0038 |    0.0181 |    0.0149 |    0.0000 |    0.0000 |\n",
      "| 48 | num49          |       0.0070 |   2.2500 |            0.0009 |           -0.0002 |    0.0177 |    0.0192 |    0.0000 |    0.0000 |\n",
      "| 44 | num45          |       0.0077 |   2.2513 |           -0.0069 |            0.0056 |    0.0184 |    0.0244 |    0.0000 |    0.0027 |\n",
      "| 41 | num42          |       0.0080 |  18.3723 |           -0.0025 |           -0.0047 |    0.0331 |    0.0286 |    0.0093 |    0.0057 |\n",
      "| 54 | num55          |       0.0086 |  74.4318 |           -0.0026 |            0.0024 |    0.0257 |    0.0200 |    0.0042 |    0.0009 |\n",
      "| 36 | num37          |       0.0087 |  18.3793 |           -0.0000 |           -0.0027 |    0.0273 |    0.0285 |    0.0060 |    0.0047 |\n",
      "| 50 | num51          |       0.0089 |   4.2226 |           -0.0082 |            0.0012 |    0.0275 |    0.0294 |    0.0077 |    0.0120 |\n",
      "| 35 | num36          |       0.0090 |   0.5135 |           -0.0080 |           -0.0020 |    0.0427 |    0.0259 |    0.0180 |    0.0060 |\n",
      "| 47 | num48          |       0.0091 |  74.4318 |           -0.0034 |           -0.0036 |    0.0268 |    0.0219 |    0.0057 |    0.0021 |\n",
      "| 51 | num52          |       0.0093 |   2.2513 |            0.0000 |           -0.0020 |    0.0196 |    0.0270 |    0.0006 |    0.0057 |\n",
      "| 43 | num44          |       0.0094 |   4.2226 |           -0.0087 |            0.0031 |    0.0286 |    0.0278 |    0.0087 |    0.0060 |\n",
      "| 40 | num41          |       0.0098 |   0.5467 |           -0.0044 |           -0.0015 |    0.0350 |    0.0345 |    0.0121 |    0.0110 |\n",
      "|  8 | num9           |       0.0104 |   4.0730 |           -0.0114 |            0.0118 |    0.0462 |    0.0494 |    0.0230 |    0.0263 |\n",
      "|  9 | num10          |       0.0108 |   4.0758 |            0.0172 |            0.0075 |    0.0490 |    0.0516 |    0.0288 |    0.0280 |\n",
      "| 33 | num34          |       0.0116 |  26.5347 |            0.0063 |           -0.0156 |    0.0327 |    0.0468 |    0.0120 |    0.0190 |\n",
      "| 32 | num33          |       0.0118 |  26.6477 |            0.0061 |           -0.0083 |    0.0378 |    0.0495 |    0.0157 |    0.0240 |\n",
      "|  6 | num7           |       0.0127 |   0.0182 |           -0.0050 |           -0.0050 |    0.0558 |    0.0462 |    0.0339 |    0.0279 |\n",
      "|  7 | num8           |       0.0131 |   4.0887 |           -0.0022 |            0.0036 |    0.0375 |    0.0604 |    0.0123 |    0.0287 |\n",
      "|  3 | num4           |       0.0139 |   0.0058 |           -0.0069 |           -0.0027 |    0.0441 |    0.0306 |    0.0214 |    0.0104 |\n",
      "| 26 | num27          |       0.0141 |   5.5294 |           -0.0327 |            0.0057 |    0.0709 |    0.0297 |    0.0250 |    0.0045 |\n",
      "| 11 | num12          |       0.0145 |   0.0345 |           -0.0008 |           -0.0032 |    0.0367 |    0.0360 |    0.0177 |    0.0151 |\n",
      "| 58 | num59          |       0.0152 | 919.8211 |           -0.0163 |            0.0147 |    0.0438 |    0.0640 |    0.0156 |    0.0361 |\n",
      "| 10 | num11          |       0.0158 |   4.0621 |           -0.0112 |           -0.0126 |    0.0682 |    0.0621 |    0.0391 |    0.0336 |\n",
      "| 38 | num39          |       0.0159 |   5.5331 |           -0.0169 |            0.0055 |    0.0491 |    0.0368 |    0.0201 |    0.0092 |\n",
      "| 37 | num38          |       0.0162 |  17.9140 |            0.0015 |           -0.0071 |    0.0555 |    0.0534 |    0.0338 |    0.0327 |\n",
      "| 29 | num30          |       0.0165 |  19.5508 |            0.0027 |           -0.0075 |    0.0587 |    0.0494 |    0.0367 |    0.0270 |\n",
      "| 13 | num14          |       0.0168 |   0.0569 |           -0.0015 |           -0.0010 |    0.0368 |    0.0414 |    0.0183 |    0.0224 |\n",
      "| 28 | num29          |       0.0170 |   6.7246 |           -0.0005 |           -0.0035 |    0.0475 |    0.0362 |    0.0229 |    0.0160 |\n",
      "| 30 | num31          |       0.0171 |   0.5502 |            0.0001 |           -0.0026 |    0.0365 |    0.0511 |    0.0170 |    0.0333 |\n",
      "| 39 | num40          |       0.0176 |   6.6850 |            0.0090 |           -0.0087 |    0.0536 |    0.0438 |    0.0179 |    0.0130 |\n",
      "| 25 | num26          |       0.0180 |   8.6364 |           -0.0083 |           -0.0061 |    0.0605 |    0.0398 |    0.0255 |    0.0137 |\n",
      "| 12 | num13          |       0.0181 |   0.0475 |           -0.0031 |            0.0019 |    0.0447 |    0.0511 |    0.0225 |    0.0270 |\n",
      "| 34 | num35          |       0.0182 |   0.5467 |            0.0089 |           -0.0159 |    0.0416 |    0.0420 |    0.0147 |    0.0153 |\n",
      "| 14 | num15          |       0.0185 |   0.7977 |            0.0174 |           -0.0057 |    0.0743 |    0.0555 |    0.0373 |    0.0302 |\n",
      "|  1 | num2           |       0.0186 |   4.0834 |           -0.0169 |            0.0317 |    0.0620 |    0.0869 |    0.0285 |    0.0388 |\n",
      "| 24 | num25          |       0.0194 |   7.9282 |           -0.0137 |           -0.0057 |    0.0591 |    0.0610 |    0.0345 |    0.0350 |\n",
      "| 23 | num24          |       0.0194 |   4.1844 |           -0.0040 |           -0.0007 |    0.0436 |    0.0387 |    0.0223 |    0.0173 |\n",
      "| 27 | num28          |       0.0200 |   5.5592 |            0.0136 |           -0.0093 |    0.0598 |    0.0416 |    0.0280 |    0.0173 |\n",
      "|  4 | num5           |       0.0207 |   0.0079 |            0.0052 |           -0.0090 |    0.0587 |    0.0476 |    0.0337 |    0.0208 |\n",
      "| 16 | num17          |       0.0221 |   0.7834 |            0.0017 |           -0.0032 |    0.0528 |    0.0570 |    0.0322 |    0.0332 |\n",
      "| 42 | num43          |       0.0238 |   0.5782 |           -0.0135 |            0.0023 |    0.0799 |    0.0538 |    0.0487 |    0.0271 |\n",
      "| 21 | num22          |       0.0244 |   0.7174 |            0.0112 |           -0.0107 |    0.0623 |    0.0667 |    0.0426 |    0.0446 |\n",
      "|  5 | num6           |       0.0247 |   0.0135 |            0.0209 |           -0.0173 |    0.1087 |    0.0769 |    0.0640 |    0.0376 |\n",
      "|  2 | num3           |       0.0254 |   0.6974 |           -0.0132 |           -0.0143 |    0.0915 |    0.0675 |    0.0570 |    0.0420 |\n",
      "| 17 | num18          |       0.0256 |   0.7165 |           -0.0068 |           -0.0072 |    0.0826 |    0.0522 |    0.0468 |    0.0247 |\n",
      "| 15 | num16          |       0.0257 |   0.7858 |           -0.0136 |           -0.0048 |    0.0746 |    0.0760 |    0.0478 |    0.0482 |\n",
      "| 22 | num23          |       0.0276 |   2.2769 |           -0.0024 |           -0.0108 |    0.1125 |    0.0630 |    0.0590 |    0.0346 |\n",
      "| 31 | num32          |       0.0284 |   0.2455 |           -0.0056 |           -0.0069 |    0.0620 |    0.0831 |    0.0313 |    0.0488 |\n",
      "| 19 | num20          |       0.0285 |   0.7161 |           -0.0023 |           -0.0082 |    0.0657 |    0.0617 |    0.0417 |    0.0395 |\n",
      "| 18 | num19          |       0.0291 |   0.7109 |           -0.0340 |            0.0008 |    0.1099 |    0.0861 |    0.0677 |    0.0556 |\n",
      "|  0 | num1           |       0.0342 | 499.5119 |           -0.0097 |            0.0010 |    0.0827 |    0.1098 |    0.0457 |    0.0673 |\n",
      "| 20 | num21          |       0.0363 |   0.7091 |            0.0034 |           -0.0067 |    0.0704 |    0.0726 |    0.0422 |    0.0434 |\n",
      "| 57 | num58          |       0.0368 |   0.7091 |           -0.0134 |           -0.0051 |    0.0825 |    0.0606 |    0.0486 |    0.0350 |\n",
      "+----+----------------+--------------+----------+-------------------+-------------------+-----------+-----------+-----------+-----------+\n",
      "\n",
      "\n",
      "*******\n",
      "Legend:\n",
      "Importance = Feature Importance\n",
      "     Explanation: A weighted measure of how much of the variance the model is able to explain is due to this column\n",
      "FR_delta = Feature Response Delta Amount\n",
      "     Explanation: Amount this column was incremented or decremented by to calculate the feature reponses\n",
      "FR_Decrementing = Feature Response From Decrementing Values In This Column By One FR_delta\n",
      "     Explanation: Represents how much the predicted output values respond to subtracting one FR_delta amount from every value in this column\n",
      "FR_Incrementing = Feature Response From Incrementing Values In This Column By One FR_delta\n",
      "     Explanation: Represents how much the predicted output values respond to adding one FR_delta amount to every value in this column\n",
      "FRD_MAD = Feature Response From Decrementing- Median Absolute Delta\n",
      "     Explanation: Takes the absolute value of all changes in predictions, then takes the median of those. Useful for seeing if decrementing this feature provokes strong changes that are both positive and negative\n",
      "FRI_MAD = Feature Response From Incrementing- Median Absolute Delta\n",
      "     Explanation: Takes the absolute value of all changes in predictions, then takes the median of those. Useful for seeing if incrementing this feature provokes strong changes that are both positive and negative\n",
      "FRD_abs = Feature Response From Decrementing Avg Absolute Change\n",
      "     Explanation: What is the average absolute change in predicted output values to subtracting one FR_delta amount to every value in this column. Useful for seeing if output is sensitive to a feature, but not in a uniformly positive or negative way\n",
      "FRI_abs = Feature Response From Incrementing Avg Absolute Change\n",
      "     Explanation: What is the average absolute change in predicted output values to adding one FR_delta amount to every value in this column. Useful for seeing if output is sensitive to a feature, but not in a uniformly positive or negative way\n",
      "*******\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<auto_ml.predictor.Predictor at 0x1a240c6668>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_predictor.train(df_train, model_names='RandomForestRegressor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scores - 0.9963557431118195"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "\n",
      "\n",
      "***********************************************\n",
      "Advanced scoring metrics for the trained regression model on this particular dataset:\n",
      "\n",
      "Here is the overall RMSE for these predictions:\n",
      "1.4551856572729736\n",
      "\n",
      "Here is the average of the predictions:\n",
      "20.02292025698398\n",
      "\n",
      "Here is the average actual value on this validation set:\n",
      "20.024082777165344\n",
      "\n",
      "Here is the median prediction:\n",
      "20.02202373888889\n",
      "\n",
      "Here is the median actual value:\n",
      "20.040008\n",
      "\n",
      "Here is the mean absolute error:\n",
      "0.9963557431118195\n",
      "\n",
      "Here is the median absolute error (robust to outliers):\n",
      "0.6850194708333319\n",
      "\n",
      "Here is the explained variance:\n",
      "-0.06053174845147269\n",
      "\n",
      "Here is the R-squared value:\n",
      "-0.06053242529478542\n",
      "Count of positive differences (prediction > actual):\n",
      "607\n",
      "Count of negative differences:\n",
      "663\n",
      "Average positive difference:\n",
      "1.0410999943341572\n",
      "Average negative difference:\n",
      "-0.9553907951601461\n",
      "\n",
      "\n",
      "***********************************************\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Score the model on test data\n",
    "test_score = ml_predictor.score(df_test, df_test.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to auto_ml! We're about to go through and make sense of your data using machine learning, and give you a production-ready pipeline to get predictions with.\n",
      "\n",
      "If you have any issues, or new feature ideas, let us know at http://auto.ml\n",
      "You are running on version 2.9.9\n",
      "Now using the model training_params that you passed in:\n",
      "{}\n",
      "After overwriting our defaults with your values, here are the final params that will be used to initialize the model:\n",
      "{}\n",
      "Running basic data cleaning\n",
      "Fitting DataFrameVectorizer\n",
      "Now using the model training_params that you passed in:\n",
      "{}\n",
      "After overwriting our defaults with your values, here are the final params that will be used to initialize the model:\n",
      "{}\n",
      "\n",
      "\n",
      "********************************************************************************************\n",
      "About to fit the pipeline for the model XGBRegressor to predict target\n",
      "Started at:\n",
      "2018-11-14 21:11:20\n",
      "Finished training the pipeline!\n",
      "Total training time:\n",
      "0:00:03\n",
      "\n",
      "\n",
      "Here are the results from our XGBRegressor\n",
      "predicting target\n",
      "Calculating feature responses, for advanced analytics.\n",
      "The printed list will only contain at most the top 100 features.\n",
      "+----+----------------+--------------+----------+-------------------+-------------------+-----------+-----------+-----------+-----------+\n",
      "|    | Feature Name   |   Importance |    Delta |   FR_Decrementing |   FR_Incrementing |   FRD_abs |   FRI_abs |   FRD_MAD |   FRI_MAD |\n",
      "|----+----------------+--------------+----------+-------------------+-------------------+-----------+-----------+-----------+-----------|\n",
      "| 99 | cat9=True      |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 75 | cat15=True     |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 60 | cat1=C         |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 74 | cat15=False    |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 73 | cat14=True     |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 72 | cat14=False    |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 71 | cat13=True     |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 69 | cat12=True     |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 68 | cat12=False    |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 67 | cat11=True     |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 66 | cat11=False    |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 76 | cat16=False    |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 65 | cat10=True     |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 63 | cat1=B         |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 62 | cat1=A         |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 98 | cat8=True      |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 50 | num51          |       0.0000 |   4.2226 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 51 | num52          |       0.0000 |   2.2513 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 52 | num53          |       0.0000 |   2.0393 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 53 | num54          |       0.0000 |  76.7681 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 54 | num55          |       0.0000 |  74.4318 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 55 | num56          |       0.0000 |   2.2500 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 56 | num57          |       0.0000 |  76.7571 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 64 | cat10=False    |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 77 | cat16=True     |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 59 | cat1=D         |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 79 | cat17=True     |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 93 | cat6=False     |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 86 | cat2=B         |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 94 | cat6=True      |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 95 | cat7=False     |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 78 | cat17=False    |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 96 | cat7=True      |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 85 | cat2=G         |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 83 | cat19=True     |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 82 | cat19=False    |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 81 | cat18=True     |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 97 | cat8=False     |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 80 | cat18=False    |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 84 | cat2=E         |       0.0000 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 92 | cat4=False     |       0.0017 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 91 | cat20=False    |       0.0017 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "|  8 | num9           |       0.0017 |   4.0730 |           -0.0064 |            0.0025 |    0.0064 |    0.0025 |    0.0000 |    0.0000 |\n",
      "| 90 | cat2=H         |       0.0017 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 88 | cat2=K         |       0.0017 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 87 | cat2=D         |       0.0017 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 11 | num12          |       0.0034 |   0.0345 |           -0.0000 |            0.0001 |    0.0001 |    0.0002 |    0.0000 |    0.0000 |\n",
      "| 10 | num11          |       0.0034 |   4.0621 |           -0.0005 |            0.0003 |    0.0005 |    0.0003 |    0.0000 |    0.0000 |\n",
      "| 41 | num42          |       0.0034 |  18.3723 |           -0.0009 |            0.0022 |    0.0009 |    0.0022 |    0.0000 |    0.0000 |\n",
      "| 70 | cat13=False    |       0.0034 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 89 | cat2=I         |       0.0034 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 43 | num44          |       0.0051 |   4.2226 |           -0.0001 |            0.0001 |    0.0011 |    0.0018 |    0.0000 |    0.0000 |\n",
      "| 61 | cat1=E         |       0.0051 | nan      |          nan      |          nan      |  nan      |  nan      |  nan      |  nan      |\n",
      "| 40 | num41          |       0.0051 |   0.5467 |           -0.0013 |           -0.0004 |    0.0029 |    0.0017 |    0.0000 |    0.0000 |\n",
      "| 14 | num15          |       0.0051 |   0.7977 |            0.0040 |           -0.0031 |    0.0041 |    0.0033 |    0.0000 |    0.0000 |\n",
      "| 35 | num36          |       0.0051 |   0.5135 |            0.0002 |           -0.0025 |    0.0008 |    0.0031 |    0.0000 |    0.0000 |\n",
      "| 33 | num34          |       0.0051 |  26.5347 |            0.0004 |           -0.0032 |    0.0018 |    0.0130 |    0.0000 |    0.0000 |\n",
      "| 28 | num29          |       0.0051 |   6.7246 |            0.0022 |           -0.0007 |    0.0025 |    0.0011 |    0.0000 |    0.0000 |\n",
      "| 29 | num30          |       0.0051 |  19.5508 |            0.0003 |           -0.0006 |    0.0006 |    0.0006 |    0.0000 |    0.0000 |\n",
      "| 49 | num50          |       0.0068 |  76.7571 |            0.0014 |           -0.0024 |    0.0014 |    0.0025 |    0.0000 |    0.0000 |\n",
      "| 45 | num46          |       0.0068 |   2.0393 |            0.0013 |           -0.0022 |    0.0013 |    0.0022 |    0.0000 |    0.0000 |\n",
      "| 48 | num49          |       0.0085 |   2.2500 |            0.0005 |           -0.0015 |    0.0005 |    0.0015 |    0.0000 |    0.0000 |\n",
      "| 46 | num47          |       0.0085 |  76.7681 |            0.0003 |           -0.0004 |    0.0018 |    0.0030 |    0.0000 |    0.0000 |\n",
      "|  9 | num10          |       0.0085 |   4.0758 |            0.0097 |            0.0004 |    0.0116 |    0.0011 |    0.0000 |    0.0000 |\n",
      "| 58 | num59          |       0.0102 | 919.8211 |           -0.0065 |            0.0346 |    0.0080 |    0.0368 |    0.0000 |    0.0000 |\n",
      "| 36 | num37          |       0.0102 |  18.3793 |           -0.0034 |            0.0023 |    0.0044 |    0.0035 |    0.0000 |    0.0000 |\n",
      "| 25 | num26          |       0.0102 |   8.6364 |            0.0165 |           -0.0097 |    0.0187 |    0.0111 |    0.0000 |    0.0000 |\n",
      "|  7 | num8           |       0.0119 |   4.0887 |           -0.0006 |            0.0008 |    0.0014 |    0.0029 |    0.0000 |    0.0000 |\n",
      "| 42 | num43          |       0.0119 |   0.5782 |            0.0022 |           -0.0026 |    0.0028 |    0.0040 |    0.0000 |    0.0000 |\n",
      "| 22 | num23          |       0.0119 |   2.2769 |            0.0027 |           -0.0012 |    0.0027 |    0.0012 |    0.0000 |    0.0000 |\n",
      "| 24 | num25          |       0.0136 |   7.9282 |           -0.0023 |           -0.0001 |    0.0037 |    0.0021 |    0.0000 |    0.0000 |\n",
      "| 12 | num13          |       0.0153 |   0.0475 |           -0.0034 |            0.0042 |    0.0034 |    0.0042 |    0.0000 |    0.0000 |\n",
      "| 44 | num45          |       0.0153 |   2.2513 |           -0.0021 |            0.0058 |    0.0021 |    0.0058 |    0.0000 |    0.0000 |\n",
      "| 13 | num14          |       0.0153 |   0.0569 |            0.0006 |           -0.0003 |    0.0028 |    0.0027 |    0.0000 |    0.0000 |\n",
      "| 27 | num28          |       0.0170 |   5.5592 |            0.0114 |           -0.0055 |    0.0131 |    0.0071 |    0.0000 |    0.0000 |\n",
      "| 32 | num33          |       0.0170 |  26.6477 |            0.0116 |           -0.0056 |    0.0125 |    0.0070 |    0.0000 |    0.0000 |\n",
      "| 37 | num38          |       0.0170 |  17.9140 |           -0.0016 |            0.0007 |    0.0115 |    0.0067 |    0.0000 |    0.0000 |\n",
      "| 23 | num24          |       0.0187 |   4.1844 |            0.0023 |           -0.0039 |    0.0026 |    0.0039 |    0.0000 |    0.0000 |\n",
      "| 17 | num18          |       0.0187 |   0.7165 |            0.0064 |           -0.0042 |    0.0081 |    0.0052 |    0.0000 |    0.0000 |\n",
      "| 30 | num31          |       0.0187 |   0.5502 |           -0.0005 |            0.0019 |    0.0016 |    0.0028 |    0.0000 |    0.0000 |\n",
      "| 16 | num17          |       0.0221 |   0.7834 |           -0.0034 |            0.0012 |    0.0098 |    0.0063 |    0.0000 |    0.0000 |\n",
      "| 31 | num32          |       0.0221 |   0.2455 |            0.0026 |           -0.0087 |    0.0050 |    0.0110 |    0.0000 |    0.0000 |\n",
      "| 34 | num35          |       0.0221 |   0.5467 |            0.0010 |           -0.0047 |    0.0058 |    0.0085 |    0.0000 |    0.0000 |\n",
      "| 39 | num40          |       0.0238 |   6.6850 |            0.0161 |           -0.0084 |    0.0186 |    0.0107 |    0.0000 |    0.0000 |\n",
      "| 19 | num20          |       0.0238 |   0.7161 |           -0.0001 |            0.0017 |    0.0045 |    0.0047 |    0.0000 |    0.0000 |\n",
      "| 38 | num39          |       0.0255 |   5.5331 |           -0.0313 |            0.0015 |    0.0334 |    0.0053 |    0.0000 |    0.0000 |\n",
      "| 47 | num48          |       0.0255 |  74.4318 |           -0.0028 |            0.0056 |    0.0049 |    0.0066 |    0.0000 |    0.0000 |\n",
      "|  5 | num6           |       0.0272 |   0.0135 |            0.0043 |           -0.0036 |    0.0069 |    0.0061 |    0.0000 |    0.0000 |\n",
      "|  6 | num7           |       0.0272 |   0.0182 |            0.0103 |           -0.0054 |    0.0109 |    0.0057 |    0.0000 |    0.0000 |\n",
      "| 18 | num19          |       0.0289 |   0.7109 |           -0.0037 |            0.0012 |    0.0088 |    0.0041 |    0.0000 |    0.0000 |\n",
      "| 26 | num27          |       0.0289 |   5.5294 |           -0.1345 |            0.0135 |    0.1362 |    0.0152 |    0.0000 |    0.0000 |\n",
      "|  4 | num5           |       0.0289 |   0.0079 |            0.0000 |           -0.0012 |    0.0071 |    0.0034 |    0.0000 |    0.0000 |\n",
      "| 57 | num58          |       0.0306 |   0.7091 |           -0.0005 |           -0.0013 |    0.0149 |    0.0081 |    0.0000 |    0.0000 |\n",
      "| 20 | num21          |       0.0306 |   0.7091 |            0.0020 |           -0.0002 |    0.0041 |    0.0027 |    0.0000 |    0.0000 |\n",
      "|  3 | num4           |       0.0323 |   0.0058 |            0.0005 |           -0.0003 |    0.0015 |    0.0012 |    0.0000 |    0.0000 |\n",
      "|  1 | num2           |       0.0357 |   4.0834 |           -0.0025 |           -0.0104 |    0.0073 |    0.0190 |    0.0000 |    0.0000 |\n",
      "| 21 | num22          |       0.0374 |   0.7174 |            0.0077 |           -0.0051 |    0.0128 |    0.0117 |    0.0051 |    0.0051 |\n",
      "| 15 | num16          |       0.0459 |   0.7858 |           -0.0073 |            0.0062 |    0.0095 |    0.0076 |    0.0000 |    0.0000 |\n",
      "|  2 | num3           |       0.0544 |   0.6974 |            0.0042 |           -0.0060 |    0.0178 |    0.0128 |    0.0000 |    0.0000 |\n",
      "|  0 | num1           |       0.0867 | 499.5119 |            0.0024 |           -0.0056 |    0.0046 |    0.0092 |    0.0000 |    0.0000 |\n",
      "+----+----------------+--------------+----------+-------------------+-------------------+-----------+-----------+-----------+-----------+\n",
      "\n",
      "\n",
      "*******\n",
      "Legend:\n",
      "Importance = Feature Importance\n",
      "     Explanation: A weighted measure of how much of the variance the model is able to explain is due to this column\n",
      "FR_delta = Feature Response Delta Amount\n",
      "     Explanation: Amount this column was incremented or decremented by to calculate the feature reponses\n",
      "FR_Decrementing = Feature Response From Decrementing Values In This Column By One FR_delta\n",
      "     Explanation: Represents how much the predicted output values respond to subtracting one FR_delta amount from every value in this column\n",
      "FR_Incrementing = Feature Response From Incrementing Values In This Column By One FR_delta\n",
      "     Explanation: Represents how much the predicted output values respond to adding one FR_delta amount to every value in this column\n",
      "FRD_MAD = Feature Response From Decrementing- Median Absolute Delta\n",
      "     Explanation: Takes the absolute value of all changes in predictions, then takes the median of those. Useful for seeing if decrementing this feature provokes strong changes that are both positive and negative\n",
      "FRI_MAD = Feature Response From Incrementing- Median Absolute Delta\n",
      "     Explanation: Takes the absolute value of all changes in predictions, then takes the median of those. Useful for seeing if incrementing this feature provokes strong changes that are both positive and negative\n",
      "FRD_abs = Feature Response From Decrementing Avg Absolute Change\n",
      "     Explanation: What is the average absolute change in predicted output values to subtracting one FR_delta amount to every value in this column. Useful for seeing if output is sensitive to a feature, but not in a uniformly positive or negative way\n",
      "FRI_abs = Feature Response From Incrementing Avg Absolute Change\n",
      "     Explanation: What is the average absolute change in predicted output values to adding one FR_delta amount to every value in this column. Useful for seeing if output is sensitive to a feature, but not in a uniformly positive or negative way\n",
      "*******\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<auto_ml.predictor.Predictor at 0x1a240c6668>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_predictor.train(df_train, model_names='XGBRegressor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scores - 0.9690882888647844"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "\n",
      "\n",
      "***********************************************\n",
      "Advanced scoring metrics for the trained regression model on this particular dataset:\n",
      "\n",
      "Here is the overall RMSE for these predictions:\n",
      "1.4166582566117893\n",
      "\n",
      "Here is the average of the predictions:\n",
      "20.03303821143203\n",
      "\n",
      "Here is the average actual value on this validation set:\n",
      "20.024082777165344\n",
      "\n",
      "Here is the median prediction:\n",
      "20.01677131652832\n",
      "\n",
      "Here is the median actual value:\n",
      "20.040008\n",
      "\n",
      "Here is the mean absolute error:\n",
      "0.9690882888647844\n",
      "\n",
      "Here is the median absolute error (robust to outliers):\n",
      "0.6240696771240248\n",
      "\n",
      "Here is the explained variance:\n",
      "-0.005078491170324995\n",
      "\n",
      "Here is the R-squared value:\n",
      "-0.005118657342106037\n",
      "Count of positive differences (prediction > actual):\n",
      "626\n",
      "Count of negative differences:\n",
      "644\n",
      "Average positive difference:\n",
      "0.99210505461418\n",
      "Average negative difference:\n",
      "-0.9467148488661478\n",
      "\n",
      "\n",
      "***********************************************\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Score the model on test data\n",
    "test_score = ml_predictor.score(df_test, df_test.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepLearningRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to auto_ml! We're about to go through and make sense of your data using machine learning, and give you a production-ready pipeline to get predictions with.\n",
      "\n",
      "If you have any issues, or new feature ideas, let us know at http://auto.ml\n",
      "You are running on version 2.9.9\n",
      "Now using the model training_params that you passed in:\n",
      "{}\n",
      "After overwriting our defaults with your values, here are the final params that will be used to initialize the model:\n",
      "{'epochs': 1000, 'batch_size': 50, 'verbose': 2}\n",
      "Running basic data cleaning\n",
      "Performing feature scaling\n",
      "Fitting DataFrameVectorizer\n",
      "Now using the model training_params that you passed in:\n",
      "{}\n",
      "After overwriting our defaults with your values, here are the final params that will be used to initialize the model:\n",
      "{'epochs': 1000, 'batch_size': 50, 'verbose': 2}\n",
      "\n",
      "\n",
      "********************************************************************************************\n",
      "About to fit the pipeline for the model DeepLearningRegressor to predict target\n",
      "Started at:\n",
      "2018-11-14 21:16:24\n",
      "\n",
      "We will stop training early if we have not seen an improvement in validation accuracy in 25 epochs\n",
      "To measure validation accuracy, we will split off a random 10 percent of your training data set\n",
      "Train on 4318 samples, validate on 762 samples\n",
      "Epoch 1/1000\n",
      " - 1s - loss: 134.2673 - mean_absolute_error: 8.2395 - mean_absolute_percentage_error: 41.1576 - val_loss: 8.3853 - val_mean_absolute_error: 2.5275 - val_mean_absolute_percentage_error: 12.3952\n",
      "Epoch 2/1000\n",
      " - 1s - loss: 5.8933 - mean_absolute_error: 1.9472 - mean_absolute_percentage_error: 9.7763 - val_loss: 7.4537 - val_mean_absolute_error: 2.3992 - val_mean_absolute_percentage_error: 11.7494\n",
      "Epoch 3/1000\n",
      " - 1s - loss: 5.4736 - mean_absolute_error: 1.8931 - mean_absolute_percentage_error: 9.5636 - val_loss: 4.3748 - val_mean_absolute_error: 1.7326 - val_mean_absolute_percentage_error: 8.5226\n",
      "Epoch 4/1000\n",
      " - 1s - loss: 4.9776 - mean_absolute_error: 1.8204 - mean_absolute_percentage_error: 9.2095 - val_loss: 3.2293 - val_mean_absolute_error: 1.3069 - val_mean_absolute_percentage_error: 6.8847\n",
      "Epoch 5/1000\n",
      " - 1s - loss: 5.4123 - mean_absolute_error: 1.8406 - mean_absolute_percentage_error: 9.2104 - val_loss: 9.0619 - val_mean_absolute_error: 2.7070 - val_mean_absolute_percentage_error: 13.2282\n",
      "Epoch 6/1000\n",
      " - 1s - loss: 5.0151 - mean_absolute_error: 1.7878 - mean_absolute_percentage_error: 8.9943 - val_loss: 5.6875 - val_mean_absolute_error: 2.0477 - val_mean_absolute_percentage_error: 10.0132\n",
      "Epoch 7/1000\n",
      " - 1s - loss: 4.7698 - mean_absolute_error: 1.7398 - mean_absolute_percentage_error: 8.7398 - val_loss: 2.6498 - val_mean_absolute_error: 1.1414 - val_mean_absolute_percentage_error: 5.9914\n",
      "Epoch 8/1000\n",
      " - 1s - loss: 5.1117 - mean_absolute_error: 1.7752 - mean_absolute_percentage_error: 8.8685 - val_loss: 7.6082 - val_mean_absolute_error: 2.4505 - val_mean_absolute_percentage_error: 11.9755\n",
      "Epoch 9/1000\n",
      " - 1s - loss: 4.5712 - mean_absolute_error: 1.6832 - mean_absolute_percentage_error: 8.4471 - val_loss: 2.4884 - val_mean_absolute_error: 1.0790 - val_mean_absolute_percentage_error: 5.6586\n",
      "Epoch 10/1000\n",
      " - 1s - loss: 4.5262 - mean_absolute_error: 1.6952 - mean_absolute_percentage_error: 8.5652 - val_loss: 4.4353 - val_mean_absolute_error: 1.7585 - val_mean_absolute_percentage_error: 8.6207\n",
      "Epoch 11/1000\n",
      " - 1s - loss: 4.1604 - mean_absolute_error: 1.6403 - mean_absolute_percentage_error: 8.2790 - val_loss: 2.8364 - val_mean_absolute_error: 1.2010 - val_mean_absolute_percentage_error: 6.3261\n",
      "Epoch 12/1000\n",
      " - 1s - loss: 4.3280 - mean_absolute_error: 1.6273 - mean_absolute_percentage_error: 8.1660 - val_loss: 6.0256 - val_mean_absolute_error: 2.1304 - val_mean_absolute_percentage_error: 10.4143\n",
      "Epoch 13/1000\n",
      " - 1s - loss: 4.4674 - mean_absolute_error: 1.6427 - mean_absolute_percentage_error: 8.2052 - val_loss: 2.3012 - val_mean_absolute_error: 1.0061 - val_mean_absolute_percentage_error: 5.2557\n",
      "Epoch 14/1000\n",
      " - 1s - loss: 4.3328 - mean_absolute_error: 1.6443 - mean_absolute_percentage_error: 8.3710 - val_loss: 4.2558 - val_mean_absolute_error: 1.6436 - val_mean_absolute_percentage_error: 8.6333\n",
      "Epoch 15/1000\n",
      " - 1s - loss: 3.9100 - mean_absolute_error: 1.5618 - mean_absolute_percentage_error: 7.9358 - val_loss: 4.7361 - val_mean_absolute_error: 1.8254 - val_mean_absolute_percentage_error: 8.9325\n",
      "Epoch 16/1000\n",
      " - 1s - loss: 3.9569 - mean_absolute_error: 1.5424 - mean_absolute_percentage_error: 7.7455 - val_loss: 7.6021 - val_mean_absolute_error: 2.4520 - val_mean_absolute_percentage_error: 11.9843\n",
      "Epoch 17/1000\n",
      " - 1s - loss: 3.7934 - mean_absolute_error: 1.5050 - mean_absolute_percentage_error: 7.5771 - val_loss: 2.4384 - val_mean_absolute_error: 1.0548 - val_mean_absolute_percentage_error: 5.5377\n",
      "Epoch 18/1000\n",
      " - 1s - loss: 3.6536 - mean_absolute_error: 1.4649 - mean_absolute_percentage_error: 7.3721 - val_loss: 5.2258 - val_mean_absolute_error: 1.9496 - val_mean_absolute_percentage_error: 9.5416\n",
      "Epoch 19/1000\n",
      " - 1s - loss: 3.7810 - mean_absolute_error: 1.4850 - mean_absolute_percentage_error: 7.6096 - val_loss: 3.7851 - val_mean_absolute_error: 1.5051 - val_mean_absolute_percentage_error: 7.9213\n",
      "Epoch 20/1000\n",
      " - 1s - loss: 3.2518 - mean_absolute_error: 1.3855 - mean_absolute_percentage_error: 7.0524 - val_loss: 3.9970 - val_mean_absolute_error: 1.6312 - val_mean_absolute_percentage_error: 8.0055\n",
      "Epoch 21/1000\n",
      " - 1s - loss: 3.3845 - mean_absolute_error: 1.3787 - mean_absolute_percentage_error: 6.9429 - val_loss: 2.8570 - val_mean_absolute_error: 1.2852 - val_mean_absolute_percentage_error: 6.3795\n",
      "Epoch 22/1000\n",
      " - 1s - loss: 3.0781 - mean_absolute_error: 1.3081 - mean_absolute_percentage_error: 6.6582 - val_loss: 2.4993 - val_mean_absolute_error: 1.0801 - val_mean_absolute_percentage_error: 5.6693\n",
      "Epoch 23/1000\n",
      " - 1s - loss: 3.0670 - mean_absolute_error: 1.2986 - mean_absolute_percentage_error: 6.5701 - val_loss: 2.9431 - val_mean_absolute_error: 1.3105 - val_mean_absolute_percentage_error: 6.4939\n",
      "Epoch 24/1000\n",
      " - 1s - loss: 2.9308 - mean_absolute_error: 1.2594 - mean_absolute_percentage_error: 6.4018 - val_loss: 2.4255 - val_mean_absolute_error: 1.0510 - val_mean_absolute_percentage_error: 5.5100\n",
      "Epoch 25/1000\n",
      " - 1s - loss: 3.0758 - mean_absolute_error: 1.2865 - mean_absolute_percentage_error: 6.5085 - val_loss: 3.8899 - val_mean_absolute_error: 1.5993 - val_mean_absolute_percentage_error: 7.8522\n",
      "Epoch 26/1000\n",
      " - 1s - loss: 3.2394 - mean_absolute_error: 1.3108 - mean_absolute_percentage_error: 6.6875 - val_loss: 2.9158 - val_mean_absolute_error: 1.2915 - val_mean_absolute_percentage_error: 6.3900\n",
      "Epoch 27/1000\n",
      " - 1s - loss: 2.8108 - mean_absolute_error: 1.2202 - mean_absolute_percentage_error: 6.2075 - val_loss: 2.5590 - val_mean_absolute_error: 1.1183 - val_mean_absolute_percentage_error: 5.8664\n",
      "Epoch 28/1000\n",
      " - 1s - loss: 2.7528 - mean_absolute_error: 1.2171 - mean_absolute_percentage_error: 6.1693 - val_loss: 3.2227 - val_mean_absolute_error: 1.3959 - val_mean_absolute_percentage_error: 6.8812\n",
      "Epoch 29/1000\n",
      " - 1s - loss: 2.8443 - mean_absolute_error: 1.2253 - mean_absolute_percentage_error: 6.2131 - val_loss: 2.8505 - val_mean_absolute_error: 1.2723 - val_mean_absolute_percentage_error: 6.3048\n",
      "Epoch 30/1000\n",
      " - 1s - loss: 2.7640 - mean_absolute_error: 1.2218 - mean_absolute_percentage_error: 6.2205 - val_loss: 2.4929 - val_mean_absolute_error: 1.0907 - val_mean_absolute_percentage_error: 5.7195\n",
      "Epoch 31/1000\n",
      " - 1s - loss: 2.7457 - mean_absolute_error: 1.1997 - mean_absolute_percentage_error: 6.0776 - val_loss: 5.9599 - val_mean_absolute_error: 2.0787 - val_mean_absolute_percentage_error: 10.8581\n",
      "Epoch 32/1000\n",
      " - 1s - loss: 2.8561 - mean_absolute_error: 1.2315 - mean_absolute_percentage_error: 6.2186 - val_loss: 3.5081 - val_mean_absolute_error: 1.4954 - val_mean_absolute_percentage_error: 7.3577\n",
      "Epoch 33/1000\n",
      " - 1s - loss: 3.0055 - mean_absolute_error: 1.2678 - mean_absolute_percentage_error: 6.4335 - val_loss: 2.9930 - val_mean_absolute_error: 1.3317 - val_mean_absolute_percentage_error: 6.5852\n",
      "Epoch 34/1000\n",
      " - 1s - loss: 2.6128 - mean_absolute_error: 1.1678 - mean_absolute_percentage_error: 5.9558 - val_loss: 2.5282 - val_mean_absolute_error: 1.1089 - val_mean_absolute_percentage_error: 5.8207\n",
      "Epoch 35/1000\n",
      " - 1s - loss: 2.7305 - mean_absolute_error: 1.2002 - mean_absolute_percentage_error: 6.1186 - val_loss: 2.8442 - val_mean_absolute_error: 1.2767 - val_mean_absolute_percentage_error: 6.3271\n",
      "Epoch 36/1000\n",
      " - 1s - loss: 2.6689 - mean_absolute_error: 1.1756 - mean_absolute_percentage_error: 6.0086 - val_loss: 2.3436 - val_mean_absolute_error: 1.0729 - val_mean_absolute_percentage_error: 5.3922\n",
      "Epoch 37/1000\n",
      " - 1s - loss: 2.6798 - mean_absolute_error: 1.1681 - mean_absolute_percentage_error: 6.0084 - val_loss: 2.6015 - val_mean_absolute_error: 1.1419 - val_mean_absolute_percentage_error: 5.9949\n",
      "Epoch 38/1000\n",
      " - 1s - loss: 2.6067 - mean_absolute_error: 1.1666 - mean_absolute_percentage_error: 5.9577 - val_loss: 2.6733 - val_mean_absolute_error: 1.2122 - val_mean_absolute_percentage_error: 6.0251\n",
      "Epoch 00038: early stopping\n",
      "Finished training the pipeline!\n",
      "Total training time:\n",
      "0:00:26\n",
      "Calculating feature responses, for advanced analytics.\n",
      "Here are our feature responses for the trained model\n",
      "+----+----------------+----------+-------------------+-------------------+-----------+-----------+\n",
      "|    | Feature Name   |    Delta |   FR_Decrementing |   FR_Incrementing |   FRD_MAD |   FRI_MAD |\n",
      "|----+----------------+----------+-------------------+-------------------+-----------+-----------|\n",
      "| 58 | num55          |   0.1337 |           -0.0001 |            0.0001 |    0.0002 |    0.0002 |\n",
      "| 57 | num56          |   0.1371 |           -0.0003 |            0.0003 |    0.0003 |    0.0002 |\n",
      "| 56 | num18          |   0.1239 |           -0.0005 |            0.0005 |    0.0005 |    0.0005 |\n",
      "| 55 | num54          |   0.1336 |           -0.0006 |            0.0006 |    0.0006 |    0.0005 |\n",
      "| 54 | num32          |   0.1331 |           -0.0006 |            0.0006 |    0.0006 |    0.0006 |\n",
      "| 53 | num24          |   0.1375 |           -0.0012 |            0.0012 |    0.0012 |    0.0012 |\n",
      "| 52 | num45          |   0.1372 |            0.0013 |           -0.0013 |    0.0013 |    0.0013 |\n",
      "| 51 | num52          |   0.1372 |           -0.0018 |            0.0018 |    0.0018 |    0.0018 |\n",
      "| 50 | num50          |   0.1343 |           -0.0020 |            0.0020 |    0.0020 |    0.0020 |\n",
      "| 49 | num47          |   0.1336 |           -0.0022 |            0.0022 |    0.0022 |    0.0022 |\n",
      "| 48 | num5           |   0.1268 |           -0.0023 |            0.0023 |    0.0023 |    0.0023 |\n",
      "| 47 | num6           |   0.1294 |            0.0026 |           -0.0026 |    0.0026 |    0.0026 |\n",
      "| 46 | num43          |   0.1224 |            0.0026 |           -0.0026 |    0.0026 |    0.0027 |\n",
      "| 45 | num1           |   0.1491 |            0.0027 |           -0.0027 |    0.0028 |    0.0028 |\n",
      "| 44 | num46          |   0.1359 |           -0.0028 |            0.0028 |    0.0027 |    0.0027 |\n",
      "| 43 | num23          |   0.1301 |            0.0029 |           -0.0029 |    0.0029 |    0.0029 |\n",
      "| 42 | num13          |   0.1319 |           -0.0032 |            0.0032 |    0.0031 |    0.0031 |\n",
      "| 41 | num44          |   0.1403 |            0.0044 |           -0.0044 |    0.0044 |    0.0044 |\n",
      "| 40 | num51          |   0.1403 |           -0.0045 |            0.0045 |    0.0045 |    0.0045 |\n",
      "| 39 | num27          |   0.1446 |            0.0046 |           -0.0046 |    0.0046 |    0.0046 |\n",
      "| 38 | num10          |   0.1414 |           -0.0051 |            0.0051 |    0.0051 |    0.0051 |\n",
      "| 37 | num41          |   0.1333 |            0.0060 |           -0.0060 |    0.0060 |    0.0060 |\n",
      "| 36 | num39          |   0.1448 |            0.0060 |           -0.0060 |    0.0060 |    0.0060 |\n",
      "| 35 | num53          |   0.1359 |           -0.0061 |            0.0061 |    0.0061 |    0.0061 |\n",
      "| 34 | num14          |   0.1311 |           -0.0062 |            0.0062 |    0.0062 |    0.0062 |\n",
      "| 33 | num42          |   0.1317 |           -0.0065 |            0.0065 |    0.0064 |    0.0064 |\n",
      "| 32 | num7           |   0.1310 |           -0.0071 |            0.0071 |    0.0071 |    0.0071 |\n",
      "| 31 | num28          |   0.1417 |            0.0072 |           -0.0072 |    0.0072 |    0.0072 |\n",
      "| 30 | num26          |   0.1479 |           -0.0079 |            0.0079 |    0.0079 |    0.0079 |\n",
      "| 29 | num34          |   0.1333 |           -0.0081 |            0.0081 |    0.0081 |    0.0081 |\n",
      "| 28 | num4           |   0.1251 |           -0.0083 |            0.0082 |    0.0082 |    0.0082 |\n",
      "| 27 | num58          |   0.1256 |           -0.0084 |            0.0084 |    0.0084 |    0.0083 |\n",
      "| 26 | num2           |   0.1418 |           -0.0084 |            0.0084 |    0.0084 |    0.0084 |\n",
      "| 25 | num49          |   0.1371 |           -0.0086 |            0.0086 |    0.0085 |    0.0085 |\n",
      "| 24 | num57          |   0.1343 |            0.0090 |           -0.0090 |    0.0090 |    0.0090 |\n",
      "| 23 | num48          |   0.1337 |            0.0091 |           -0.0091 |    0.0091 |    0.0091 |\n",
      "| 22 | num35          |   0.1335 |           -0.0092 |            0.0092 |    0.0092 |    0.0092 |\n",
      "| 21 | num40          |   0.1403 |            0.0094 |           -0.0094 |    0.0094 |    0.0094 |\n",
      "| 20 | num9           |   0.1420 |           -0.0096 |            0.0096 |    0.0096 |    0.0096 |\n",
      "| 19 | num8           |   0.1420 |           -0.0107 |            0.0107 |    0.0107 |    0.0107 |\n",
      "| 18 | num19          |   0.1231 |           -0.0114 |            0.0114 |    0.0114 |    0.0114 |\n",
      "| 17 | num21          |   0.1233 |           -0.0121 |            0.0121 |    0.0121 |    0.0121 |\n",
      "| 16 | num37          |   0.1322 |           -0.0124 |            0.0124 |    0.0123 |    0.0123 |\n",
      "| 15 | num11          |   0.1420 |           -0.0134 |            0.0134 |    0.0134 |    0.0134 |\n",
      "| 14 | num36          |   0.1321 |           -0.0141 |            0.0140 |    0.0140 |    0.0140 |\n",
      "| 13 | num22          |   0.1231 |            0.0143 |           -0.0143 |    0.0143 |    0.0143 |\n",
      "| 12 | num59          |   0.1587 |           -0.0159 |            0.0159 |    0.0159 |    0.0159 |\n",
      "| 11 | num29          |   0.1398 |           -0.0161 |            0.0161 |    0.0161 |    0.0161 |\n",
      "| 10 | num38          |   0.1315 |           -0.0163 |            0.0163 |    0.0162 |    0.0162 |\n",
      "|  9 | num12          |   0.1294 |           -0.0165 |            0.0165 |    0.0165 |    0.0165 |\n",
      "|  8 | num3           |   0.1235 |           -0.0169 |            0.0169 |    0.0169 |    0.0169 |\n",
      "|  7 | num33          |   0.1332 |           -0.0198 |            0.0198 |    0.0198 |    0.0198 |\n",
      "|  6 | num17          |   0.1220 |           -0.0199 |            0.0199 |    0.0199 |    0.0198 |\n",
      "|  5 | num30          |   0.1397 |           -0.0202 |            0.0202 |    0.0202 |    0.0202 |\n",
      "|  4 | num15          |   0.1218 |           -0.0216 |            0.0216 |    0.0216 |    0.0216 |\n",
      "|  3 | num16          |   0.1223 |           -0.0236 |            0.0236 |    0.0236 |    0.0236 |\n",
      "|  2 | num20          |   0.1235 |           -0.0236 |            0.0236 |    0.0236 |    0.0236 |\n",
      "|  1 | num31          |   0.5502 |            0.0431 |           -0.0432 |    0.0432 |    0.0432 |\n",
      "|  0 | num25          |   0.1401 |           -0.0728 |            0.0728 |    0.0728 |    0.0728 |\n",
      "| 59 | cat1=D         | nan      |          nan      |          nan      |  nan      |  nan      |\n",
      "| 60 | cat1=C         | nan      |          nan      |          nan      |  nan      |  nan      |\n",
      "| 61 | cat1=E         | nan      |          nan      |          nan      |  nan      |  nan      |\n",
      "| 62 | cat1=A         | nan      |          nan      |          nan      |  nan      |  nan      |\n",
      "| 63 | cat1=B         | nan      |          nan      |          nan      |  nan      |  nan      |\n",
      "| 64 | cat10=False    | nan      |          nan      |          nan      |  nan      |  nan      |\n",
      "| 65 | cat10=True     | nan      |          nan      |          nan      |  nan      |  nan      |\n",
      "| 66 | cat11=False    | nan      |          nan      |          nan      |  nan      |  nan      |\n",
      "| 67 | cat11=True     | nan      |          nan      |          nan      |  nan      |  nan      |\n",
      "| 68 | cat12=False    | nan      |          nan      |          nan      |  nan      |  nan      |\n",
      "| 69 | cat12=True     | nan      |          nan      |          nan      |  nan      |  nan      |\n",
      "| 70 | cat13=False    | nan      |          nan      |          nan      |  nan      |  nan      |\n",
      "| 71 | cat13=True     | nan      |          nan      |          nan      |  nan      |  nan      |\n",
      "| 72 | cat14=False    | nan      |          nan      |          nan      |  nan      |  nan      |\n",
      "| 73 | cat14=True     | nan      |          nan      |          nan      |  nan      |  nan      |\n",
      "| 74 | cat15=False    | nan      |          nan      |          nan      |  nan      |  nan      |\n",
      "| 75 | cat15=True     | nan      |          nan      |          nan      |  nan      |  nan      |\n",
      "| 76 | cat16=False    | nan      |          nan      |          nan      |  nan      |  nan      |\n",
      "| 77 | cat16=True     | nan      |          nan      |          nan      |  nan      |  nan      |\n",
      "| 78 | cat17=False    | nan      |          nan      |          nan      |  nan      |  nan      |\n",
      "| 79 | cat17=True     | nan      |          nan      |          nan      |  nan      |  nan      |\n",
      "| 80 | cat18=False    | nan      |          nan      |          nan      |  nan      |  nan      |\n",
      "| 81 | cat18=True     | nan      |          nan      |          nan      |  nan      |  nan      |\n",
      "| 82 | cat19=False    | nan      |          nan      |          nan      |  nan      |  nan      |\n",
      "| 83 | cat19=True     | nan      |          nan      |          nan      |  nan      |  nan      |\n",
      "| 84 | cat2=E         | nan      |          nan      |          nan      |  nan      |  nan      |\n",
      "| 85 | cat2=G         | nan      |          nan      |          nan      |  nan      |  nan      |\n",
      "| 86 | cat2=B         | nan      |          nan      |          nan      |  nan      |  nan      |\n",
      "| 87 | cat2=D         | nan      |          nan      |          nan      |  nan      |  nan      |\n",
      "| 88 | cat2=F         | nan      |          nan      |          nan      |  nan      |  nan      |\n",
      "| 89 | cat2=C         | nan      |          nan      |          nan      |  nan      |  nan      |\n",
      "| 90 | cat2=K         | nan      |          nan      |          nan      |  nan      |  nan      |\n",
      "| 91 | cat2=A         | nan      |          nan      |          nan      |  nan      |  nan      |\n",
      "| 92 | cat2=I         | nan      |          nan      |          nan      |  nan      |  nan      |\n",
      "| 93 | cat2=L         | nan      |          nan      |          nan      |  nan      |  nan      |\n",
      "| 94 | cat2=J         | nan      |          nan      |          nan      |  nan      |  nan      |\n",
      "| 95 | cat2=H         | nan      |          nan      |          nan      |  nan      |  nan      |\n",
      "| 96 | cat20=False    | nan      |          nan      |          nan      |  nan      |  nan      |\n",
      "| 97 | cat20=True     | nan      |          nan      |          nan      |  nan      |  nan      |\n",
      "| 98 | cat21=False    | nan      |          nan      |          nan      |  nan      |  nan      |\n",
      "| 99 | cat21=True     | nan      |          nan      |          nan      |  nan      |  nan      |\n",
      "+----+----------------+----------+-------------------+-------------------+-----------+-----------+\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<auto_ml.predictor.Predictor at 0x1a240c6668>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_predictor.train(df_train, model_names='DeepLearningRegressor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scores - 1.0260875669148122"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "\n",
      "\n",
      "***********************************************\n",
      "Advanced scoring metrics for the trained regression model on this particular dataset:\n",
      "\n",
      "Here is the overall RMSE for these predictions:\n",
      "1.4760234383877187\n",
      "\n",
      "Here is the average of the predictions:\n",
      "20.41556608094944\n",
      "\n",
      "Here is the average actual value on this validation set:\n",
      "20.024082777165344\n",
      "\n",
      "Here is the median prediction:\n",
      "20.421393394470215\n",
      "\n",
      "Here is the median actual value:\n",
      "20.040008\n",
      "\n",
      "Here is the mean absolute error:\n",
      "1.0260875669148122\n",
      "\n",
      "Here is the median absolute error (robust to outliers):\n",
      "0.7125936766967769\n",
      "\n",
      "Here is the explained variance:\n",
      "-0.014366614334592187\n",
      "\n",
      "Here is the R-squared value:\n",
      "-0.09112284331739318\n",
      "Count of positive differences (prediction > actual):\n",
      "814\n",
      "Count of negative differences:\n",
      "456\n",
      "Average positive difference:\n",
      "1.1058445981496294\n",
      "Average negative difference:\n",
      "-0.8837142699298528\n",
      "\n",
      "\n",
      "***********************************************\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Score the model on test data\n",
    "test_score = ml_predictor.score(df_test, df_test.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num1</th>\n",
       "      <th>num2</th>\n",
       "      <th>num3</th>\n",
       "      <th>num4</th>\n",
       "      <th>num5</th>\n",
       "      <th>num6</th>\n",
       "      <th>num7</th>\n",
       "      <th>num8</th>\n",
       "      <th>num9</th>\n",
       "      <th>num10</th>\n",
       "      <th>...</th>\n",
       "      <th>num51</th>\n",
       "      <th>num52</th>\n",
       "      <th>num53</th>\n",
       "      <th>num54</th>\n",
       "      <th>num55</th>\n",
       "      <th>num56</th>\n",
       "      <th>num57</th>\n",
       "      <th>num58</th>\n",
       "      <th>num59</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6350.000000</td>\n",
       "      <td>6350.000000</td>\n",
       "      <td>6350.000000</td>\n",
       "      <td>6350.000000</td>\n",
       "      <td>6350.000000</td>\n",
       "      <td>6350.000000</td>\n",
       "      <td>6350.000000</td>\n",
       "      <td>6350.000000</td>\n",
       "      <td>6350.000000</td>\n",
       "      <td>6350.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>6350.000000</td>\n",
       "      <td>6350.000000</td>\n",
       "      <td>6350.000000</td>\n",
       "      <td>6350.000000</td>\n",
       "      <td>6350.000000</td>\n",
       "      <td>6350.000000</td>\n",
       "      <td>6350.000000</td>\n",
       "      <td>6350.000000</td>\n",
       "      <td>6350.000000</td>\n",
       "      <td>6350.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>849.859489</td>\n",
       "      <td>19.424687</td>\n",
       "      <td>0.035514</td>\n",
       "      <td>0.000350</td>\n",
       "      <td>0.000692</td>\n",
       "      <td>0.002359</td>\n",
       "      <td>0.004674</td>\n",
       "      <td>19.423997</td>\n",
       "      <td>19.423383</td>\n",
       "      <td>19.422728</td>\n",
       "      <td>...</td>\n",
       "      <td>49.205268</td>\n",
       "      <td>-0.174183</td>\n",
       "      <td>-0.176646</td>\n",
       "      <td>-0.035296</td>\n",
       "      <td>-0.108280</td>\n",
       "      <td>-0.175369</td>\n",
       "      <td>-0.066943</td>\n",
       "      <td>0.035880</td>\n",
       "      <td>3227.500000</td>\n",
       "      <td>20.035933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1000.217417</td>\n",
       "      <td>8.141230</td>\n",
       "      <td>1.378046</td>\n",
       "      <td>0.011589</td>\n",
       "      <td>0.015842</td>\n",
       "      <td>0.026975</td>\n",
       "      <td>0.036410</td>\n",
       "      <td>8.141693</td>\n",
       "      <td>8.142129</td>\n",
       "      <td>8.142565</td>\n",
       "      <td>...</td>\n",
       "      <td>8.409250</td>\n",
       "      <td>4.465990</td>\n",
       "      <td>4.039964</td>\n",
       "      <td>152.012519</td>\n",
       "      <td>147.239668</td>\n",
       "      <td>4.465680</td>\n",
       "      <td>152.009651</td>\n",
       "      <td>1.408658</td>\n",
       "      <td>1833.231437</td>\n",
       "      <td>1.419549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.052000</td>\n",
       "      <td>9.140000</td>\n",
       "      <td>-15.120010</td>\n",
       "      <td>-0.098448</td>\n",
       "      <td>-0.133555</td>\n",
       "      <td>-0.237463</td>\n",
       "      <td>-0.270464</td>\n",
       "      <td>9.140000</td>\n",
       "      <td>9.140000</td>\n",
       "      <td>9.140000</td>\n",
       "      <td>...</td>\n",
       "      <td>24.368050</td>\n",
       "      <td>-11.187660</td>\n",
       "      <td>-9.896388</td>\n",
       "      <td>-419.550500</td>\n",
       "      <td>-404.172500</td>\n",
       "      <td>-11.187663</td>\n",
       "      <td>-419.550500</td>\n",
       "      <td>-13.510010</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>8.479981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>68.644250</td>\n",
       "      <td>13.420000</td>\n",
       "      <td>-0.529998</td>\n",
       "      <td>-0.004574</td>\n",
       "      <td>-0.006676</td>\n",
       "      <td>-0.010854</td>\n",
       "      <td>-0.012989</td>\n",
       "      <td>13.420000</td>\n",
       "      <td>13.420000</td>\n",
       "      <td>13.420000</td>\n",
       "      <td>...</td>\n",
       "      <td>43.056097</td>\n",
       "      <td>-3.158996</td>\n",
       "      <td>-2.855053</td>\n",
       "      <td>-97.696500</td>\n",
       "      <td>-95.130599</td>\n",
       "      <td>-3.158996</td>\n",
       "      <td>-97.696500</td>\n",
       "      <td>-0.576153</td>\n",
       "      <td>1640.250000</td>\n",
       "      <td>19.453096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>558.720000</td>\n",
       "      <td>17.540000</td>\n",
       "      <td>0.079990</td>\n",
       "      <td>0.000598</td>\n",
       "      <td>0.001327</td>\n",
       "      <td>0.004094</td>\n",
       "      <td>0.007847</td>\n",
       "      <td>17.540000</td>\n",
       "      <td>17.540000</td>\n",
       "      <td>17.540000</td>\n",
       "      <td>...</td>\n",
       "      <td>48.242387</td>\n",
       "      <td>-0.764160</td>\n",
       "      <td>-0.602180</td>\n",
       "      <td>-15.435551</td>\n",
       "      <td>-14.287910</td>\n",
       "      <td>-0.765040</td>\n",
       "      <td>-15.443575</td>\n",
       "      <td>0.080002</td>\n",
       "      <td>3227.500000</td>\n",
       "      <td>20.062500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1250.553750</td>\n",
       "      <td>23.010000</td>\n",
       "      <td>0.639999</td>\n",
       "      <td>0.005726</td>\n",
       "      <td>0.008740</td>\n",
       "      <td>0.017024</td>\n",
       "      <td>0.025125</td>\n",
       "      <td>23.010000</td>\n",
       "      <td>23.010000</td>\n",
       "      <td>23.010000</td>\n",
       "      <td>...</td>\n",
       "      <td>54.518745</td>\n",
       "      <td>2.178925</td>\n",
       "      <td>1.971575</td>\n",
       "      <td>77.249947</td>\n",
       "      <td>74.291276</td>\n",
       "      <td>2.177838</td>\n",
       "      <td>77.174036</td>\n",
       "      <td>0.727497</td>\n",
       "      <td>4814.750000</td>\n",
       "      <td>20.680000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>8710.263000</td>\n",
       "      <td>80.860000</td>\n",
       "      <td>9.529999</td>\n",
       "      <td>0.145198</td>\n",
       "      <td>0.128249</td>\n",
       "      <td>0.173543</td>\n",
       "      <td>0.205891</td>\n",
       "      <td>80.860000</td>\n",
       "      <td>80.860000</td>\n",
       "      <td>80.860000</td>\n",
       "      <td>...</td>\n",
       "      <td>91.037513</td>\n",
       "      <td>28.970420</td>\n",
       "      <td>22.056720</td>\n",
       "      <td>814.407602</td>\n",
       "      <td>748.015035</td>\n",
       "      <td>28.970424</td>\n",
       "      <td>814.407600</td>\n",
       "      <td>10.829990</td>\n",
       "      <td>6402.000000</td>\n",
       "      <td>32.849998</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows  60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              num1         num2         num3         num4         num5  \\\n",
       "count  6350.000000  6350.000000  6350.000000  6350.000000  6350.000000   \n",
       "mean    849.859489    19.424687     0.035514     0.000350     0.000692   \n",
       "std    1000.217417     8.141230     1.378046     0.011589     0.015842   \n",
       "min       0.052000     9.140000   -15.120010    -0.098448    -0.133555   \n",
       "25%      68.644250    13.420000    -0.529998    -0.004574    -0.006676   \n",
       "50%     558.720000    17.540000     0.079990     0.000598     0.001327   \n",
       "75%    1250.553750    23.010000     0.639999     0.005726     0.008740   \n",
       "max    8710.263000    80.860000     9.529999     0.145198     0.128249   \n",
       "\n",
       "              num6         num7         num8         num9        num10  \\\n",
       "count  6350.000000  6350.000000  6350.000000  6350.000000  6350.000000   \n",
       "mean      0.002359     0.004674    19.423997    19.423383    19.422728   \n",
       "std       0.026975     0.036410     8.141693     8.142129     8.142565   \n",
       "min      -0.237463    -0.270464     9.140000     9.140000     9.140000   \n",
       "25%      -0.010854    -0.012989    13.420000    13.420000    13.420000   \n",
       "50%       0.004094     0.007847    17.540000    17.540000    17.540000   \n",
       "75%       0.017024     0.025125    23.010000    23.010000    23.010000   \n",
       "max       0.173543     0.205891    80.860000    80.860000    80.860000   \n",
       "\n",
       "          ...             num51        num52        num53        num54  \\\n",
       "count     ...       6350.000000  6350.000000  6350.000000  6350.000000   \n",
       "mean      ...         49.205268    -0.174183    -0.176646    -0.035296   \n",
       "std       ...          8.409250     4.465990     4.039964   152.012519   \n",
       "min       ...         24.368050   -11.187660    -9.896388  -419.550500   \n",
       "25%       ...         43.056097    -3.158996    -2.855053   -97.696500   \n",
       "50%       ...         48.242387    -0.764160    -0.602180   -15.435551   \n",
       "75%       ...         54.518745     2.178925     1.971575    77.249947   \n",
       "max       ...         91.037513    28.970420    22.056720   814.407602   \n",
       "\n",
       "             num55        num56        num57        num58        num59  \\\n",
       "count  6350.000000  6350.000000  6350.000000  6350.000000  6350.000000   \n",
       "mean     -0.108280    -0.175369    -0.066943     0.035880  3227.500000   \n",
       "std     147.239668     4.465680   152.009651     1.408658  1833.231437   \n",
       "min    -404.172500   -11.187663  -419.550500   -13.510010    53.000000   \n",
       "25%     -95.130599    -3.158996   -97.696500    -0.576153  1640.250000   \n",
       "50%     -14.287910    -0.765040   -15.443575     0.080002  3227.500000   \n",
       "75%      74.291276     2.177838    77.174036     0.727497  4814.750000   \n",
       "max     748.015035    28.970424   814.407600    10.829990  6402.000000   \n",
       "\n",
       "            target  \n",
       "count  6350.000000  \n",
       "mean     20.035933  \n",
       "std       1.419549  \n",
       "min       8.479981  \n",
       "25%      19.453096  \n",
       "50%      20.062500  \n",
       "75%      20.680000  \n",
       "max      32.849998  \n",
       "\n",
       "[8 rows x 60 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only the ones that are within +3 to -3 standard deviations in the column 'num1'.\n",
    "df3 = df[np.abs(df.num1-df.num1.mean()) <= (3*df.num1.std())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num1</th>\n",
       "      <th>num2</th>\n",
       "      <th>num3</th>\n",
       "      <th>num4</th>\n",
       "      <th>num5</th>\n",
       "      <th>num6</th>\n",
       "      <th>num7</th>\n",
       "      <th>num8</th>\n",
       "      <th>num9</th>\n",
       "      <th>num10</th>\n",
       "      <th>...</th>\n",
       "      <th>num51</th>\n",
       "      <th>num52</th>\n",
       "      <th>num53</th>\n",
       "      <th>num54</th>\n",
       "      <th>num55</th>\n",
       "      <th>num56</th>\n",
       "      <th>num57</th>\n",
       "      <th>num58</th>\n",
       "      <th>num59</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6239.000000</td>\n",
       "      <td>6239.000000</td>\n",
       "      <td>6239.000000</td>\n",
       "      <td>6239.000000</td>\n",
       "      <td>6239.000000</td>\n",
       "      <td>6239.000000</td>\n",
       "      <td>6239.000000</td>\n",
       "      <td>6239.000000</td>\n",
       "      <td>6239.000000</td>\n",
       "      <td>6239.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>6239.000000</td>\n",
       "      <td>6239.000000</td>\n",
       "      <td>6239.000000</td>\n",
       "      <td>6239.000000</td>\n",
       "      <td>6239.000000</td>\n",
       "      <td>6239.000000</td>\n",
       "      <td>6239.000000</td>\n",
       "      <td>6239.000000</td>\n",
       "      <td>6239.000000</td>\n",
       "      <td>6239.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>776.448560</td>\n",
       "      <td>18.930019</td>\n",
       "      <td>0.058332</td>\n",
       "      <td>0.000449</td>\n",
       "      <td>0.000968</td>\n",
       "      <td>0.003170</td>\n",
       "      <td>0.005954</td>\n",
       "      <td>18.951314</td>\n",
       "      <td>18.976213</td>\n",
       "      <td>18.987831</td>\n",
       "      <td>...</td>\n",
       "      <td>49.021532</td>\n",
       "      <td>-0.310647</td>\n",
       "      <td>-0.289436</td>\n",
       "      <td>-4.270822</td>\n",
       "      <td>-3.766860</td>\n",
       "      <td>-0.300875</td>\n",
       "      <td>-4.111663</td>\n",
       "      <td>0.056922</td>\n",
       "      <td>3212.233852</td>\n",
       "      <td>20.038236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>831.684485</td>\n",
       "      <td>7.037145</td>\n",
       "      <td>1.287471</td>\n",
       "      <td>0.010442</td>\n",
       "      <td>0.014430</td>\n",
       "      <td>0.024912</td>\n",
       "      <td>0.033387</td>\n",
       "      <td>7.100360</td>\n",
       "      <td>7.167988</td>\n",
       "      <td>7.206140</td>\n",
       "      <td>...</td>\n",
       "      <td>8.250047</td>\n",
       "      <td>4.261222</td>\n",
       "      <td>3.872749</td>\n",
       "      <td>145.396055</td>\n",
       "      <td>141.306682</td>\n",
       "      <td>4.287757</td>\n",
       "      <td>145.671955</td>\n",
       "      <td>1.326969</td>\n",
       "      <td>1845.417693</td>\n",
       "      <td>1.356749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.052000</td>\n",
       "      <td>9.140000</td>\n",
       "      <td>-12.100006</td>\n",
       "      <td>-0.088578</td>\n",
       "      <td>-0.081633</td>\n",
       "      <td>-0.131838</td>\n",
       "      <td>-0.177753</td>\n",
       "      <td>9.140000</td>\n",
       "      <td>9.140000</td>\n",
       "      <td>9.140000</td>\n",
       "      <td>...</td>\n",
       "      <td>24.368050</td>\n",
       "      <td>-11.187660</td>\n",
       "      <td>-9.896388</td>\n",
       "      <td>-419.550500</td>\n",
       "      <td>-404.172500</td>\n",
       "      <td>-11.187663</td>\n",
       "      <td>-419.550500</td>\n",
       "      <td>-13.510010</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>8.479981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>65.569000</td>\n",
       "      <td>13.380000</td>\n",
       "      <td>-0.500000</td>\n",
       "      <td>-0.004320</td>\n",
       "      <td>-0.006370</td>\n",
       "      <td>-0.010173</td>\n",
       "      <td>-0.012272</td>\n",
       "      <td>13.375000</td>\n",
       "      <td>13.370000</td>\n",
       "      <td>13.370000</td>\n",
       "      <td>...</td>\n",
       "      <td>42.952762</td>\n",
       "      <td>-3.191202</td>\n",
       "      <td>-2.896049</td>\n",
       "      <td>-98.862874</td>\n",
       "      <td>-96.182435</td>\n",
       "      <td>-3.190494</td>\n",
       "      <td>-98.862877</td>\n",
       "      <td>-0.549996</td>\n",
       "      <td>1612.500000</td>\n",
       "      <td>19.469400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>542.938000</td>\n",
       "      <td>17.330000</td>\n",
       "      <td>0.089996</td>\n",
       "      <td>0.000628</td>\n",
       "      <td>0.001373</td>\n",
       "      <td>0.004355</td>\n",
       "      <td>0.008179</td>\n",
       "      <td>17.330000</td>\n",
       "      <td>17.330000</td>\n",
       "      <td>17.330000</td>\n",
       "      <td>...</td>\n",
       "      <td>48.085733</td>\n",
       "      <td>-0.828591</td>\n",
       "      <td>-0.660092</td>\n",
       "      <td>-17.163470</td>\n",
       "      <td>-15.852694</td>\n",
       "      <td>-0.837911</td>\n",
       "      <td>-17.380570</td>\n",
       "      <td>0.090004</td>\n",
       "      <td>3172.000000</td>\n",
       "      <td>20.062500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1199.525000</td>\n",
       "      <td>22.655000</td>\n",
       "      <td>0.639999</td>\n",
       "      <td>0.005648</td>\n",
       "      <td>0.008734</td>\n",
       "      <td>0.017152</td>\n",
       "      <td>0.025250</td>\n",
       "      <td>22.660000</td>\n",
       "      <td>22.685000</td>\n",
       "      <td>22.685000</td>\n",
       "      <td>...</td>\n",
       "      <td>54.301545</td>\n",
       "      <td>2.086521</td>\n",
       "      <td>1.865139</td>\n",
       "      <td>72.973573</td>\n",
       "      <td>71.029982</td>\n",
       "      <td>2.086521</td>\n",
       "      <td>72.994556</td>\n",
       "      <td>0.724998</td>\n",
       "      <td>4841.500000</td>\n",
       "      <td>20.670000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3842.763000</td>\n",
       "      <td>68.510000</td>\n",
       "      <td>8.879975</td>\n",
       "      <td>0.060079</td>\n",
       "      <td>0.077593</td>\n",
       "      <td>0.153608</td>\n",
       "      <td>0.198209</td>\n",
       "      <td>70.330000</td>\n",
       "      <td>70.330000</td>\n",
       "      <td>72.670000</td>\n",
       "      <td>...</td>\n",
       "      <td>91.037513</td>\n",
       "      <td>28.970420</td>\n",
       "      <td>22.056720</td>\n",
       "      <td>814.407602</td>\n",
       "      <td>748.015035</td>\n",
       "      <td>28.970424</td>\n",
       "      <td>814.407600</td>\n",
       "      <td>8.799988</td>\n",
       "      <td>6402.000000</td>\n",
       "      <td>28.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows  60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              num1         num2         num3         num4         num5  \\\n",
       "count  6239.000000  6239.000000  6239.000000  6239.000000  6239.000000   \n",
       "mean    776.448560    18.930019     0.058332     0.000449     0.000968   \n",
       "std     831.684485     7.037145     1.287471     0.010442     0.014430   \n",
       "min       0.052000     9.140000   -12.100006    -0.088578    -0.081633   \n",
       "25%      65.569000    13.380000    -0.500000    -0.004320    -0.006370   \n",
       "50%     542.938000    17.330000     0.089996     0.000628     0.001373   \n",
       "75%    1199.525000    22.655000     0.639999     0.005648     0.008734   \n",
       "max    3842.763000    68.510000     8.879975     0.060079     0.077593   \n",
       "\n",
       "              num6         num7         num8         num9        num10  \\\n",
       "count  6239.000000  6239.000000  6239.000000  6239.000000  6239.000000   \n",
       "mean      0.003170     0.005954    18.951314    18.976213    18.987831   \n",
       "std       0.024912     0.033387     7.100360     7.167988     7.206140   \n",
       "min      -0.131838    -0.177753     9.140000     9.140000     9.140000   \n",
       "25%      -0.010173    -0.012272    13.375000    13.370000    13.370000   \n",
       "50%       0.004355     0.008179    17.330000    17.330000    17.330000   \n",
       "75%       0.017152     0.025250    22.660000    22.685000    22.685000   \n",
       "max       0.153608     0.198209    70.330000    70.330000    72.670000   \n",
       "\n",
       "          ...             num51        num52        num53        num54  \\\n",
       "count     ...       6239.000000  6239.000000  6239.000000  6239.000000   \n",
       "mean      ...         49.021532    -0.310647    -0.289436    -4.270822   \n",
       "std       ...          8.250047     4.261222     3.872749   145.396055   \n",
       "min       ...         24.368050   -11.187660    -9.896388  -419.550500   \n",
       "25%       ...         42.952762    -3.191202    -2.896049   -98.862874   \n",
       "50%       ...         48.085733    -0.828591    -0.660092   -17.163470   \n",
       "75%       ...         54.301545     2.086521     1.865139    72.973573   \n",
       "max       ...         91.037513    28.970420    22.056720   814.407602   \n",
       "\n",
       "             num55        num56        num57        num58        num59  \\\n",
       "count  6239.000000  6239.000000  6239.000000  6239.000000  6239.000000   \n",
       "mean     -3.766860    -0.300875    -4.111663     0.056922  3212.233852   \n",
       "std     141.306682     4.287757   145.671955     1.326969  1845.417693   \n",
       "min    -404.172500   -11.187663  -419.550500   -13.510010    53.000000   \n",
       "25%     -96.182435    -3.190494   -98.862877    -0.549996  1612.500000   \n",
       "50%     -15.852694    -0.837911   -17.380570     0.090004  3172.000000   \n",
       "75%      71.029982     2.086521    72.994556     0.724998  4841.500000   \n",
       "max     748.015035    28.970424   814.407600     8.799988  6402.000000   \n",
       "\n",
       "            target  \n",
       "count  6239.000000  \n",
       "mean     20.038236  \n",
       "std       1.356749  \n",
       "min       8.479981  \n",
       "25%      19.469400  \n",
       "50%      20.062500  \n",
       "75%      20.670000  \n",
       "max      28.000000  \n",
       "\n",
       "[8 rows x 60 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need all the variables to be numeric for neural networks\n",
    "df3 =pd.get_dummies(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training and testing data\n",
    "df3_train, df3_test = train_test_split(df3, test_size=0.2, random_state=303)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_descriptions = {'target': 'output'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_predictor = Predictor(type_of_estimator='regressor', column_descriptions=column_descriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Tensorflow and Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to auto_ml! We're about to go through and make sense of your data using machine learning, and give you a production-ready pipeline to get predictions with.\n",
      "\n",
      "If you have any issues, or new feature ideas, let us know at http://auto.ml\n",
      "You are running on version 2.9.9\n",
      "Now using the model training_params that you passed in:\n",
      "{}\n",
      "After overwriting our defaults with your values, here are the final params that will be used to initialize the model:\n",
      "{'epochs': 1000, 'batch_size': 50, 'verbose': 2}\n",
      "Running basic data cleaning\n",
      "Performing feature scaling\n",
      "Fitting DataFrameVectorizer\n",
      "Now using the model training_params that you passed in:\n",
      "{}\n",
      "After overwriting our defaults with your values, here are the final params that will be used to initialize the model:\n",
      "{'epochs': 1000, 'batch_size': 50, 'verbose': 2}\n",
      "\n",
      "\n",
      "********************************************************************************************\n",
      "About to fit the pipeline for the model DeepLearningRegressor to predict target\n",
      "Started at:\n",
      "2018-11-14 21:26:14\n",
      "\n",
      "We will stop training early if we have not seen an improvement in validation accuracy in 25 epochs\n",
      "To measure validation accuracy, we will split off a random 10 percent of your training data set\n",
      "Train on 4242 samples, validate on 749 samples\n",
      "Epoch 1/1000\n",
      " - 2s - loss: 136.2446 - mean_absolute_error: 8.4042 - mean_absolute_percentage_error: 42.0585 - val_loss: 4.9666 - val_mean_absolute_error: 1.7053 - val_mean_absolute_percentage_error: 8.7633\n",
      "Epoch 2/1000\n",
      " - 1s - loss: 6.1550 - mean_absolute_error: 1.9962 - mean_absolute_percentage_error: 10.0273 - val_loss: 3.9202 - val_mean_absolute_error: 1.5354 - val_mean_absolute_percentage_error: 7.9641\n",
      "Epoch 3/1000\n",
      " - 1s - loss: 5.1979 - mean_absolute_error: 1.7995 - mean_absolute_percentage_error: 9.0396 - val_loss: 7.8669 - val_mean_absolute_error: 2.4451 - val_mean_absolute_percentage_error: 12.6621\n",
      "Epoch 4/1000\n",
      " - 1s - loss: 4.1135 - mean_absolute_error: 1.6101 - mean_absolute_percentage_error: 8.1963 - val_loss: 4.9516 - val_mean_absolute_error: 1.8920 - val_mean_absolute_percentage_error: 9.2305\n",
      "Epoch 5/1000\n",
      " - 1s - loss: 3.9758 - mean_absolute_error: 1.5410 - mean_absolute_percentage_error: 7.7303 - val_loss: 2.1350 - val_mean_absolute_error: 0.9803 - val_mean_absolute_percentage_error: 5.0399\n",
      "Epoch 6/1000\n",
      " - 1s - loss: 3.6633 - mean_absolute_error: 1.4644 - mean_absolute_percentage_error: 7.5184 - val_loss: 3.0954 - val_mean_absolute_error: 1.3195 - val_mean_absolute_percentage_error: 6.8925\n",
      "Epoch 7/1000\n",
      " - 1s - loss: 3.0835 - mean_absolute_error: 1.3297 - mean_absolute_percentage_error: 6.7233 - val_loss: 4.1305 - val_mean_absolute_error: 1.6880 - val_mean_absolute_percentage_error: 8.2466\n",
      "Epoch 8/1000\n",
      " - 1s - loss: 3.0237 - mean_absolute_error: 1.3086 - mean_absolute_percentage_error: 6.6557 - val_loss: 2.6081 - val_mean_absolute_error: 1.1504 - val_mean_absolute_percentage_error: 5.9978\n",
      "Epoch 9/1000\n",
      " - 1s - loss: 2.8817 - mean_absolute_error: 1.2600 - mean_absolute_percentage_error: 6.3628 - val_loss: 4.0692 - val_mean_absolute_error: 1.6667 - val_mean_absolute_percentage_error: 8.1405\n",
      "Epoch 10/1000\n",
      " - 1s - loss: 2.7622 - mean_absolute_error: 1.2290 - mean_absolute_percentage_error: 6.2611 - val_loss: 2.3790 - val_mean_absolute_error: 1.0583 - val_mean_absolute_percentage_error: 5.5048\n",
      "Epoch 11/1000\n",
      " - 1s - loss: 2.8294 - mean_absolute_error: 1.2410 - mean_absolute_percentage_error: 6.2680 - val_loss: 2.1875 - val_mean_absolute_error: 0.9820 - val_mean_absolute_percentage_error: 5.0821\n",
      "Epoch 12/1000\n",
      " - 1s - loss: 3.0002 - mean_absolute_error: 1.2746 - mean_absolute_percentage_error: 6.4917 - val_loss: 3.9143 - val_mean_absolute_error: 1.5686 - val_mean_absolute_percentage_error: 8.1938\n",
      "Epoch 13/1000\n",
      " - 1s - loss: 2.6400 - mean_absolute_error: 1.1799 - mean_absolute_percentage_error: 6.0620 - val_loss: 2.5250 - val_mean_absolute_error: 1.1212 - val_mean_absolute_percentage_error: 5.8464\n",
      "Epoch 14/1000\n",
      " - 1s - loss: 2.4727 - mean_absolute_error: 1.1350 - mean_absolute_percentage_error: 5.7836 - val_loss: 3.2239 - val_mean_absolute_error: 1.4272 - val_mean_absolute_percentage_error: 6.9981\n",
      "Epoch 15/1000\n",
      " - 1s - loss: 2.6269 - mean_absolute_error: 1.1849 - mean_absolute_percentage_error: 6.0038 - val_loss: 3.1139 - val_mean_absolute_error: 1.3959 - val_mean_absolute_percentage_error: 6.8513\n",
      "Epoch 16/1000\n",
      " - 1s - loss: 2.6224 - mean_absolute_error: 1.1699 - mean_absolute_percentage_error: 5.9564 - val_loss: 3.2639 - val_mean_absolute_error: 1.3858 - val_mean_absolute_percentage_error: 7.2430\n",
      "Epoch 17/1000\n",
      " - 1s - loss: 2.4450 - mean_absolute_error: 1.1260 - mean_absolute_percentage_error: 5.7693 - val_loss: 2.5530 - val_mean_absolute_error: 1.1976 - val_mean_absolute_percentage_error: 5.9148\n",
      "Epoch 18/1000\n",
      " - 1s - loss: 2.3458 - mean_absolute_error: 1.1040 - mean_absolute_percentage_error: 5.6423 - val_loss: 2.2761 - val_mean_absolute_error: 1.0433 - val_mean_absolute_percentage_error: 5.4228\n",
      "Epoch 19/1000\n",
      " - 1s - loss: 2.3935 - mean_absolute_error: 1.1186 - mean_absolute_percentage_error: 5.7026 - val_loss: 2.6935 - val_mean_absolute_error: 1.2546 - val_mean_absolute_percentage_error: 6.1815\n",
      "Epoch 20/1000\n",
      " - 1s - loss: 2.4042 - mean_absolute_error: 1.1175 - mean_absolute_percentage_error: 5.7078 - val_loss: 2.4054 - val_mean_absolute_error: 1.1437 - val_mean_absolute_percentage_error: 5.6656\n",
      "Epoch 21/1000\n",
      " - 1s - loss: 2.3386 - mean_absolute_error: 1.1110 - mean_absolute_percentage_error: 5.6864 - val_loss: 2.3338 - val_mean_absolute_error: 1.0730 - val_mean_absolute_percentage_error: 5.5836\n",
      "Epoch 22/1000\n",
      " - 1s - loss: 2.3011 - mean_absolute_error: 1.1010 - mean_absolute_percentage_error: 5.6222 - val_loss: 2.6078 - val_mean_absolute_error: 1.2251 - val_mean_absolute_percentage_error: 6.0412\n",
      "Epoch 23/1000\n",
      " - 1s - loss: 2.3264 - mean_absolute_error: 1.1068 - mean_absolute_percentage_error: 5.6411 - val_loss: 2.1887 - val_mean_absolute_error: 1.0161 - val_mean_absolute_percentage_error: 5.2708\n",
      "Epoch 24/1000\n",
      " - 1s - loss: 2.4158 - mean_absolute_error: 1.1245 - mean_absolute_percentage_error: 5.7145 - val_loss: 2.8959 - val_mean_absolute_error: 1.3353 - val_mean_absolute_percentage_error: 6.5614\n",
      "Epoch 25/1000\n",
      " - 1s - loss: 2.3415 - mean_absolute_error: 1.1099 - mean_absolute_percentage_error: 5.6681 - val_loss: 2.3048 - val_mean_absolute_error: 1.1068 - val_mean_absolute_percentage_error: 5.4954\n",
      "Epoch 26/1000\n",
      " - 1s - loss: 2.3068 - mean_absolute_error: 1.0993 - mean_absolute_percentage_error: 5.6502 - val_loss: 2.4233 - val_mean_absolute_error: 1.1124 - val_mean_absolute_percentage_error: 5.7980\n",
      "Epoch 27/1000\n",
      " - 1s - loss: 2.3069 - mean_absolute_error: 1.1013 - mean_absolute_percentage_error: 5.6495 - val_loss: 2.3832 - val_mean_absolute_error: 1.1405 - val_mean_absolute_percentage_error: 5.6484\n",
      "Epoch 28/1000\n",
      " - 1s - loss: 2.3044 - mean_absolute_error: 1.0986 - mean_absolute_percentage_error: 5.6228 - val_loss: 2.2858 - val_mean_absolute_error: 1.0622 - val_mean_absolute_percentage_error: 5.5239\n",
      "Epoch 29/1000\n",
      " - 1s - loss: 2.3323 - mean_absolute_error: 1.1066 - mean_absolute_percentage_error: 5.6576 - val_loss: 2.5027 - val_mean_absolute_error: 1.1911 - val_mean_absolute_percentage_error: 5.8825\n",
      "Epoch 30/1000\n",
      " - 1s - loss: 2.3158 - mean_absolute_error: 1.1003 - mean_absolute_percentage_error: 5.6218 - val_loss: 2.1989 - val_mean_absolute_error: 1.0281 - val_mean_absolute_percentage_error: 5.3369\n",
      "Epoch 00030: early stopping\n",
      "Finished training the pipeline!\n",
      "Total training time:\n",
      "0:00:21\n",
      "Calculating feature responses, for advanced analytics.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<auto_ml.predictor.Predictor at 0x1a3e630e10>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_predictor.train(df3_train, model_names='DeepLearningRegressor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scores - 1.003524996129843"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "\n",
      "\n",
      "***********************************************\n",
      "Advanced scoring metrics for the trained regression model on this particular dataset:\n",
      "\n",
      "Here is the overall RMSE for these predictions:\n",
      "1.4667452351339743\n",
      "\n",
      "Here is the average of the predictions:\n",
      "20.25749700344526\n",
      "\n",
      "Here is the average actual value on this validation set:\n",
      "20.12760610576924\n",
      "\n",
      "Here is the median prediction:\n",
      "20.263115882873535\n",
      "\n",
      "Here is the median actual value:\n",
      "20.099995\n",
      "\n",
      "Here is the mean absolute error:\n",
      "1.003524996129843\n",
      "\n",
      "Here is the median absolute error (robust to outliers):\n",
      "0.6711329269409188\n",
      "\n",
      "Here is the explained variance:\n",
      "-0.04271737578792334\n",
      "\n",
      "Here is the R-squared value:\n",
      "-0.050959402244242114\n",
      "Count of positive differences (prediction > actual):\n",
      "684\n",
      "Count of negative differences:\n",
      "564\n",
      "Average positive difference:\n",
      "1.0339934469807948\n",
      "Average negative difference:\n",
      "-0.9665738961616678\n",
      "\n",
      "\n",
      "***********************************************\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Score the model on test data\n",
    "test_score = ml_predictor.score(df3_test, df3_test.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing with XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to auto_ml! We're about to go through and make sense of your data using machine learning, and give you a production-ready pipeline to get predictions with.\n",
      "\n",
      "If you have any issues, or new feature ideas, let us know at http://auto.ml\n",
      "You are running on version 2.9.9\n",
      "Now using the model training_params that you passed in:\n",
      "{}\n",
      "After overwriting our defaults with your values, here are the final params that will be used to initialize the model:\n",
      "{}\n",
      "Running basic data cleaning\n",
      "Fitting DataFrameVectorizer\n",
      "Now using the model training_params that you passed in:\n",
      "{}\n",
      "After overwriting our defaults with your values, here are the final params that will be used to initialize the model:\n",
      "{}\n",
      "\n",
      "\n",
      "********************************************************************************************\n",
      "About to fit the pipeline for the model XGBRegressor to predict target\n",
      "Started at:\n",
      "2018-11-14 21:27:45\n",
      "Finished training the pipeline!\n",
      "Total training time:\n",
      "0:00:03\n",
      "\n",
      "\n",
      "Here are the results from our XGBRegressor\n",
      "predicting target\n",
      "Calculating feature responses, for advanced analytics.\n",
      "The printed list will only contain at most the top 100 features.\n",
      "+----+----------------+--------------+----------+-------------------+-------------------+-----------+-----------+-----------+-----------+\n",
      "|    | Feature Name   |   Importance |    Delta |   FR_Decrementing |   FR_Incrementing |   FRD_abs |   FRI_abs |   FRD_MAD |   FRI_MAD |\n",
      "|----+----------------+--------------+----------+-------------------+-------------------+-----------+-----------+-----------+-----------|\n",
      "| 49 | num50          |       0.0000 |  72.1760 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 70 | cat1_B         |       0.0000 |   0.2019 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 68 | cat19          |       0.0000 |   0.0897 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 67 | cat18          |       0.0000 |   0.0527 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 66 | cat17          |       0.0000 |   0.0775 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 65 | cat16          |       0.0000 |   0.0339 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 64 | cat15          |       0.0000 |   0.0423 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 63 | cat14          |       0.0000 |   0.0851 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 62 | cat13          |       0.0000 |   0.0985 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 59 | cat10          |       0.0000 |   0.0411 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 56 | num57          |       0.0000 |  72.1760 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 55 | num56          |       0.0000 |   2.1232 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 54 | num55          |       0.0000 |  70.0416 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 53 | num54          |       0.0000 |  72.0199 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 52 | num53          |       0.0000 |   1.9206 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 51 | num52          |       0.0000 |   2.1118 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 72 | cat1_D         |       0.0000 |   0.2006 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 50 | num51          |       0.0000 |   4.1740 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 73 | cat1_E         |       0.0000 |   0.1995 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 77 | cat23          |       0.0000 |   0.0435 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 96 | cat6           |       0.0000 |   0.0878 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 95 | cat5           |       0.0000 |   0.0974 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 94 | cat4           |       0.0000 |   0.1048 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 93 | cat3           |       0.0000 |   0.0904 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 91 | cat2_K         |       0.0000 |   0.1362 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 90 | cat2_J         |       0.0000 |   0.1394 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 89 | cat2_I         |       0.0000 |   0.1354 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 88 | cat2_H         |       0.0000 |   0.1407 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 87 | cat2_G         |       0.0000 |   0.1403 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 86 | cat2_F         |       0.0000 |   0.1425 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 83 | cat2_C         |       0.0000 |   0.1400 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 82 | cat2_B         |       0.0000 |   0.1324 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 81 | cat2_A         |       0.0000 |   0.1356 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 80 | cat26          |       0.0000 |   0.0508 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 78 | cat24          |       0.0000 |   0.0331 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 74 | cat20          |       0.0000 |   0.1026 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 98 | cat8           |       0.0000 |   0.0498 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 99 | cat9           |       0.0000 |   0.0331 |            0.0000 |            0.0000 |    0.0000 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 60 | cat11          |       0.0017 |   0.0907 |            0.0111 |            0.0000 |    0.0112 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 75 | cat21          |       0.0017 |   0.0959 |            0.0068 |            0.0000 |    0.0068 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 76 | cat22          |       0.0017 |   0.0843 |            0.0051 |            0.0000 |    0.0051 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 85 | cat2_E         |       0.0017 |   0.1394 |           -0.0001 |            0.0000 |    0.0001 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 97 | cat7           |       0.0017 |   0.0799 |            0.0024 |            0.0000 |    0.0036 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 79 | cat25          |       0.0017 |   0.0784 |            0.0112 |            0.0000 |    0.0112 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 84 | cat2_D         |       0.0017 |   0.1379 |           -0.0010 |            0.0000 |    0.0010 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 61 | cat12          |       0.0017 |   0.1024 |            0.0011 |            0.0000 |    0.0011 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 92 | cat2_L         |       0.0017 |   0.1379 |           -0.0046 |            0.0000 |    0.0046 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 14 | num15          |       0.0034 |   0.6835 |            0.0013 |           -0.0048 |    0.0013 |    0.0048 |    0.0000 |    0.0000 |\n",
      "| 71 | cat1_C         |       0.0034 |   0.2024 |           -0.0007 |            0.0000 |    0.0007 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 36 | num37          |       0.0034 |  16.8259 |            0.0002 |           -0.0026 |    0.0002 |    0.0026 |    0.0000 |    0.0000 |\n",
      "| 69 | cat1_A         |       0.0034 |   0.1955 |           -0.0084 |            0.0000 |    0.0084 |    0.0000 |    0.0000 |    0.0000 |\n",
      "| 29 | num30          |       0.0034 |  18.9531 |            0.0001 |           -0.0002 |    0.0003 |    0.0004 |    0.0000 |    0.0000 |\n",
      "| 44 | num45          |       0.0051 |   2.1118 |            0.0035 |           -0.0106 |    0.0040 |    0.0115 |    0.0000 |    0.0000 |\n",
      "| 35 | num36          |       0.0051 |   0.4661 |           -0.0010 |            0.0055 |    0.0022 |    0.0071 |    0.0000 |    0.0000 |\n",
      "| 41 | num42          |       0.0068 |  16.8698 |           -0.0030 |            0.0029 |    0.0035 |    0.0032 |    0.0000 |    0.0000 |\n",
      "| 43 | num44          |       0.0068 |   4.1740 |            0.0006 |           -0.0014 |    0.0006 |    0.0014 |    0.0000 |    0.0000 |\n",
      "| 25 | num26          |       0.0086 |   8.6283 |            0.0019 |           -0.0014 |    0.0052 |    0.0028 |    0.0000 |    0.0000 |\n",
      "| 13 | num14          |       0.0086 |   0.0526 |           -0.0006 |           -0.0000 |    0.0012 |    0.0006 |    0.0000 |    0.0000 |\n",
      "|  7 | num8           |       0.0086 |   3.5661 |           -0.0004 |            0.0010 |    0.0011 |    0.0015 |    0.0000 |    0.0000 |\n",
      "| 31 | num32          |       0.0103 |   0.2318 |            0.0005 |           -0.0005 |    0.0009 |    0.0012 |    0.0000 |    0.0000 |\n",
      "| 28 | num29          |       0.0103 |   6.6059 |            0.0018 |           -0.0014 |    0.0149 |    0.0088 |    0.0000 |    0.0000 |\n",
      "| 42 | num43          |       0.0103 |   0.5407 |            0.0011 |           -0.0019 |    0.0011 |    0.0019 |    0.0000 |    0.0000 |\n",
      "| 40 | num41          |       0.0103 |   0.4940 |           -0.0002 |           -0.0007 |    0.0026 |    0.0029 |    0.0000 |    0.0000 |\n",
      "| 16 | num17          |       0.0103 |   0.6893 |           -0.0023 |            0.0014 |    0.0041 |    0.0025 |    0.0000 |    0.0000 |\n",
      "| 23 | num24          |       0.0120 |   4.1026 |           -0.0043 |            0.0016 |    0.0043 |    0.0016 |    0.0000 |    0.0000 |\n",
      "| 11 | num12          |       0.0120 |   0.0327 |           -0.0054 |            0.0065 |    0.0064 |    0.0068 |    0.0000 |    0.0000 |\n",
      "| 58 | num59          |       0.0120 | 920.7016 |           -0.0097 |            0.0146 |    0.0112 |    0.0169 |    0.0000 |    0.0000 |\n",
      "| 48 | num49          |       0.0120 |   2.1232 |            0.0050 |           -0.0052 |    0.0058 |    0.0054 |    0.0000 |    0.0000 |\n",
      "| 34 | num35          |       0.0120 |   0.4916 |            0.0028 |           -0.0066 |    0.0062 |    0.0106 |    0.0000 |    0.0000 |\n",
      "| 33 | num34          |       0.0120 |  26.5791 |            0.0014 |           -0.0112 |    0.0023 |    0.0143 |    0.0000 |    0.0000 |\n",
      "| 24 | num25          |       0.0120 |   7.7709 |           -0.0029 |            0.0003 |    0.0036 |    0.0015 |    0.0000 |    0.0000 |\n",
      "| 38 | num39          |       0.0137 |   5.5311 |            0.0002 |            0.0039 |    0.0143 |    0.0070 |    0.0000 |    0.0000 |\n",
      "| 45 | num46          |       0.0137 |   1.9206 |           -0.0012 |            0.0019 |    0.0033 |    0.0054 |    0.0000 |    0.0000 |\n",
      "| 46 | num47          |       0.0137 |  72.0199 |           -0.0011 |            0.0011 |    0.0024 |    0.0038 |    0.0000 |    0.0000 |\n",
      "|  6 | num7           |       0.0137 |   0.0168 |           -0.0055 |            0.0023 |    0.0098 |    0.0050 |    0.0000 |    0.0000 |\n",
      "| 32 | num33          |       0.0137 |  26.6865 |            0.0065 |           -0.0235 |    0.0079 |    0.0254 |    0.0000 |    0.0000 |\n",
      "|  9 | num10          |       0.0154 |   3.6297 |            0.0157 |           -0.0040 |    0.0212 |    0.0096 |    0.0000 |    0.0000 |\n",
      "| 27 | num28          |       0.0171 |   5.4675 |           -0.0136 |            0.0040 |    0.0206 |    0.0085 |    0.0000 |    0.0000 |\n",
      "|  8 | num9           |       0.0171 |   3.6043 |           -0.0043 |            0.0069 |    0.0068 |    0.0120 |    0.0000 |    0.0000 |\n",
      "| 30 | num31          |       0.0188 |   0.4923 |            0.0008 |           -0.0012 |    0.0027 |    0.0039 |    0.0000 |    0.0000 |\n",
      "| 37 | num38          |       0.0188 |  16.5590 |            0.0042 |           -0.0082 |    0.0048 |    0.0090 |    0.0000 |    0.0000 |\n",
      "| 57 | num58          |       0.0223 |   0.6603 |           -0.0013 |           -0.0010 |    0.0127 |    0.0084 |    0.0000 |    0.0000 |\n",
      "| 39 | num40          |       0.0223 |   6.6077 |            0.0186 |           -0.0066 |    0.0208 |    0.0099 |    0.0000 |    0.0000 |\n",
      "|  3 | num4           |       0.0223 |   0.0053 |            0.0016 |           -0.0011 |    0.0021 |    0.0013 |    0.0000 |    0.0000 |\n",
      "| 21 | num22          |       0.0240 |   0.6744 |           -0.0051 |            0.0006 |    0.0124 |    0.0117 |    0.0000 |    0.0000 |\n",
      "| 15 | num16          |       0.0257 |   0.6983 |           -0.0026 |           -0.0004 |    0.0076 |    0.0070 |    0.0000 |    0.0000 |\n",
      "| 22 | num23          |       0.0257 |   2.1255 |            0.0032 |           -0.0026 |    0.0077 |    0.0051 |    0.0000 |    0.0000 |\n",
      "| 26 | num27          |       0.0257 |   5.5298 |           -0.0811 |            0.0039 |    0.0811 |    0.0039 |    0.0000 |    0.0000 |\n",
      "| 47 | num48          |       0.0257 |  70.0416 |           -0.0038 |            0.0002 |    0.0100 |    0.0049 |    0.0000 |    0.0000 |\n",
      "|  4 | num5           |       0.0274 |   0.0073 |            0.0017 |           -0.0015 |    0.0031 |    0.0021 |    0.0000 |    0.0000 |\n",
      "|  1 | num2           |       0.0274 |   3.5423 |           -0.0002 |            0.0011 |    0.0025 |    0.0040 |    0.0000 |    0.0000 |\n",
      "| 10 | num11          |       0.0308 |   3.6390 |            0.0007 |            0.0031 |    0.0045 |    0.0104 |    0.0000 |    0.0000 |\n",
      "| 19 | num20          |       0.0325 |   0.6712 |            0.0019 |            0.0012 |    0.0087 |    0.0092 |    0.0000 |    0.0000 |\n",
      "| 12 | num13          |       0.0342 |   0.0439 |           -0.0080 |            0.0093 |    0.0090 |    0.0102 |    0.0000 |    0.0000 |\n",
      "| 17 | num18          |       0.0394 |   0.6695 |            0.0094 |           -0.0021 |    0.0179 |    0.0120 |    0.0000 |    0.0000 |\n",
      "|  5 | num6           |       0.0411 |   0.0126 |            0.0126 |           -0.0098 |    0.0175 |    0.0123 |    0.0000 |    0.0000 |\n",
      "|  2 | num3           |       0.0411 |   0.6409 |           -0.0016 |           -0.0001 |    0.0157 |    0.0084 |    0.0000 |    0.0000 |\n",
      "| 18 | num19          |       0.0462 |   0.6683 |           -0.0065 |            0.0027 |    0.0130 |    0.0090 |    0.0000 |    0.0000 |\n",
      "| 20 | num21          |       0.0565 |   0.6725 |            0.0043 |           -0.0008 |    0.0122 |    0.0112 |    0.0000 |    0.0000 |\n",
      "|  0 | num1           |       0.0565 | 416.3527 |            0.0002 |           -0.0012 |    0.0186 |    0.0257 |    0.0000 |    0.0000 |\n",
      "+----+----------------+--------------+----------+-------------------+-------------------+-----------+-----------+-----------+-----------+\n",
      "\n",
      "\n",
      "*******\n",
      "Legend:\n",
      "Importance = Feature Importance\n",
      "     Explanation: A weighted measure of how much of the variance the model is able to explain is due to this column\n",
      "FR_delta = Feature Response Delta Amount\n",
      "     Explanation: Amount this column was incremented or decremented by to calculate the feature reponses\n",
      "FR_Decrementing = Feature Response From Decrementing Values In This Column By One FR_delta\n",
      "     Explanation: Represents how much the predicted output values respond to subtracting one FR_delta amount from every value in this column\n",
      "FR_Incrementing = Feature Response From Incrementing Values In This Column By One FR_delta\n",
      "     Explanation: Represents how much the predicted output values respond to adding one FR_delta amount to every value in this column\n",
      "FRD_MAD = Feature Response From Decrementing- Median Absolute Delta\n",
      "     Explanation: Takes the absolute value of all changes in predictions, then takes the median of those. Useful for seeing if decrementing this feature provokes strong changes that are both positive and negative\n",
      "FRI_MAD = Feature Response From Incrementing- Median Absolute Delta\n",
      "     Explanation: Takes the absolute value of all changes in predictions, then takes the median of those. Useful for seeing if incrementing this feature provokes strong changes that are both positive and negative\n",
      "FRD_abs = Feature Response From Decrementing Avg Absolute Change\n",
      "     Explanation: What is the average absolute change in predicted output values to subtracting one FR_delta amount to every value in this column. Useful for seeing if output is sensitive to a feature, but not in a uniformly positive or negative way\n",
      "FRI_abs = Feature Response From Incrementing Avg Absolute Change\n",
      "     Explanation: What is the average absolute change in predicted output values to adding one FR_delta amount to every value in this column. Useful for seeing if output is sensitive to a feature, but not in a uniformly positive or negative way\n",
      "*******\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<auto_ml.predictor.Predictor at 0x1a3e630e10>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_predictor.train(df3_train, model_names='XGBRegressor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scores - 0.98397089196435\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "\n",
      "\n",
      "***********************************************\n",
      "Advanced scoring metrics for the trained regression model on this particular dataset:\n",
      "\n",
      "Here is the overall RMSE for these predictions:\n",
      "1.4755559612209703\n",
      "\n",
      "Here is the average of the predictions:\n",
      "20.00156227289102\n",
      "\n",
      "Here is the average actual value on this validation set:\n",
      "20.12760610576924\n",
      "\n",
      "Here is the median prediction:\n",
      "20.003246307373047\n",
      "\n",
      "Here is the median actual value:\n",
      "20.099995\n",
      "\n",
      "Here is the mean absolute error:\n",
      "0.98397089196435\n",
      "\n",
      "Here is the median absolute error (robust to outliers):\n",
      "0.6624272440795895\n",
      "\n",
      "Here is the explained variance:\n",
      "-0.055862496002833106\n",
      "\n",
      "Here is the R-squared value:\n",
      "-0.06362353329432646\n",
      "Count of positive differences (prediction > actual):\n",
      "562\n",
      "Count of negative differences:\n",
      "686\n",
      "Average positive difference:\n",
      "0.9525738164942184\n",
      "Average negative difference:\n",
      "-1.0096926943174311\n",
      "\n",
      "\n",
      "***********************************************\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Score the model on test data\n",
    "test_score = ml_predictor.score(df3_test, df3_test.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variable Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import FeatureAgglomeration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "agglo = FeatureAgglomeration(n_clusters=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfx = df2.drop('target', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FeatureAgglomeration(affinity='euclidean', compute_full_tree='auto',\n",
       "           connectivity=None, linkage='ward', memory=None, n_clusters=40,\n",
       "           pooling_func=<function mean at 0x110c46620>)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agglo.fit(dfx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reduced = agglo.transform(dfx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reduced.shape\n",
    "df_red = pd.DataFrame(df_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_red['target'] = df2['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training and testing data\n",
    "dfr_train, dfr_test = train_test_split(df_red, test_size=0.2, random_state=303)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow and Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to auto_ml! We're about to go through and make sense of your data using machine learning, and give you a production-ready pipeline to get predictions with.\n",
      "\n",
      "If you have any issues, or new feature ideas, let us know at http://auto.ml\n",
      "You are running on version 2.9.9\n",
      "Now using the model training_params that you passed in:\n",
      "{}\n",
      "After overwriting our defaults with your values, here are the final params that will be used to initialize the model:\n",
      "{}\n",
      "Running basic data cleaning\n",
      "Fitting DataFrameVectorizer\n",
      "Now using the model training_params that you passed in:\n",
      "{}\n",
      "After overwriting our defaults with your values, here are the final params that will be used to initialize the model:\n",
      "{}\n",
      "\n",
      "\n",
      "********************************************************************************************\n",
      "About to fit the pipeline for the model XGBRegressor to predict target\n",
      "Started at:\n",
      "2018-11-14 21:41:20\n",
      "Finished training the pipeline!\n",
      "Total training time:\n",
      "0:00:02\n",
      "\n",
      "\n",
      "Here are the results from our XGBRegressor\n",
      "predicting target\n",
      "Calculating feature responses, for advanced analytics.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<auto_ml.predictor.Predictor at 0x1a3e630e10>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_predictor.train(dfr_train, model_names='XGBRegressor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scores - 0.9765918739430824\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "\n",
      "\n",
      "***********************************************\n",
      "Advanced scoring metrics for the trained regression model on this particular dataset:\n",
      "\n",
      "Here is the overall RMSE for these predictions:\n",
      "1.4310283275441742\n",
      "\n",
      "Here is the average of the predictions:\n",
      "20.038243751826247\n",
      "\n",
      "Here is the average actual value on this validation set:\n",
      "20.024082777165344\n",
      "\n",
      "Here is the median prediction:\n",
      "20.012911796569824\n",
      "\n",
      "Here is the median actual value:\n",
      "20.040008\n",
      "\n",
      "Here is the mean absolute error:\n",
      "0.9765918739430824\n",
      "\n",
      "Here is the median absolute error (robust to outliers):\n",
      "0.6296473571777348\n",
      "\n",
      "Here is the explained variance:\n",
      "-0.02551276816994985\n",
      "\n",
      "Here is the R-squared value:\n",
      "-0.0256132004752887\n",
      "Count of positive differences (prediction > actual):\n",
      "626\n",
      "Count of negative differences:\n",
      "644\n",
      "Average positive difference:\n",
      "1.004996899143012\n",
      "Average negative difference:\n",
      "-0.9489807780189268\n",
      "\n",
      "\n",
      "***********************************************\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Score the model on test data\n",
    "test_score = ml_predictor.score(dfr_test, dfr_test.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
